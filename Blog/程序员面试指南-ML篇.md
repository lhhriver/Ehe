# 基础知识

**问题**：谈谈判别式模型和生成式模型？

**答案**：

**判别方法：**由数据直接学习决策函数 Y = f（X），或者由条件分布概率 P（Y|X）作为预测模型，即判别模型。 

**生成方法：**由数据学习联合概率密度分布函数 P（X,Y）,然后求出条件概率分布P(Y|X)作为预测的模型，即生成模型。

由生成模型可以得到判别模型，但由判别模型得不到生成模型。 
  `常见的判别模型有`：K近邻、SVM、决策树、感知机、线性判别分析（LDA）、线性回归、传统的神经网络、逻辑斯蒂回归、boosting、条件随机场。
  `常见的生成模型有`：朴素贝叶斯、隐马尔可夫模型、高斯混合模型、文档主题生成模型（LDA）、限制玻尔兹曼机。

---

**问题**：协方差和相关性有什么区别？

**答案**：相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

***

**问题**： 线性分类器与非线性分类器的区别以及优劣

**答案**：

1. 如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。 
2. 常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归。
3. 常见的非线性分类器：决策树、RF、GBDT、多层感知机。
4. SVM两种都有(看线性核还是高斯核) 。
5. 线性分类器速度快、编程方便，但是可能拟合效果不会很好。
6. 非线性分类器编程复杂，但是效果拟合能力强 。

---

**问题**：防止过拟合的方法

**答案**：过拟合的原因是算法的学习能力过强；一些假设条件（如样本独立同分布）可能是不成立的；训练样本过少不能对整个空间进行分布估计。 

**处理方法有：  **

1. 早停止：如在训练中多次迭代后发现模型性能没有显著提高就停止训练 
2. 数据集扩增：原有数据增加、原有数据加随机噪声、重采样  
3. 正则化  
4. 交叉验证  
5. 特征选择/特征降维  

***

**问题**：哪些机器学习算法不需要做归一化处理？

**答案**：

概率模型不需要归一化，因为它们不关心变量的值，而是关心变量的分布和变量之间的条件概率，如决策树、rf。而像adaboost、svm、lr、KNN、KMeans之类的最优化问题就需要归一化。 
　　@管博士：我理解归一化和标准化主要是为了使计算更方便 比如两个变量的量纲不同 可能一个的数值远大于另一个那么他们同时作为变量的时候 可能会造成数值计算的问题，比如说求矩阵的逆可能很不精确 或者梯度下降法的收敛比较困难，还有如果需要计算欧式距离的话可能 量纲也需要调整 所以我估计lr 和 knn 保准话一下应该有好处。至于其他的算法 我也觉得如果变量量纲差距很大的话 先标准化一下会有好处。 
　　@寒小阳：一般我习惯说树形模型，这里说的概率模型可能是差不多的意思。

***

**问题**：对于树形结构为什么不需要归一化？

**答案**：

数值缩放，不影响分裂点位置。因为第一步都是按照特征值进行排序的，排序的顺序不变，那么所属的分支以及分裂点就不会有不同。对于线性模型，比如说LR，我有两个特征，一个是(0,1)的，一个是(0,10000)的，这样运用梯度下降时候，损失等高线是一个椭圆的形状，这样我想迭代到最优点，就需要很多次迭代，但是如果进行了归一化，那么等高线就是圆形的，那么SGD就会往原点迭代，需要的迭代次数较少。 
　　另外，注意树模型是不能进行梯度下降的，因为树模型是阶跃的，阶跃点是不可导的，并且求导没意义，所以树模型（回归树）寻找最优点事通过寻找最优分裂点完成的。

---

**问题**：数据归一化（或者标准化，注意归一化和标准化不同）的原因

**答案**：

要强调：能不归一化最好不归一化，之所以进行数据归一化是因为各维度的量纲不相同。而且需要看情况进行归一化。  

有些模型在各维度进行了不均匀的伸缩后，最优解与原来不等价（如SVM）需要归一化。  

有些模型伸缩有与原来等价，如：LR则不用归一化，但是实际中往往通过迭代求解模型参数，如果目标函数太扁（想象一下很扁的高斯模型）迭代算法会发生不收敛的情况，所以最坏进行数据归一化。  

补充：其实本质是由于loss函数不同造成的，SVM用了欧拉距离，如果一个特征很大就会把其他的维度dominated。而LR可以通过权重调整使得损失函数不变。

***

**问题**：了解正则化么

**答案**：

正则化是针对过拟合而提出的，以为在求解模型最优的是一般优化最小的经验风险，现在在该经验风险上加入模型复杂度这一项（正则化项是模型参数向量的范数），并使用一个rate比率来权衡模型复杂度与以往经验风险的权重，如果模型复杂度越高，结构化的经验风险会越大，现在的目标就变为了结构经验风险的最优化，可以防止模型训练过度复杂，有效的降低过拟合的风险。 

奥卡姆剃刀原理，能够很好的解释已知数据并且十分简单才是最好的模型。

---

# LR

**问题**：Logistic回归损失函数是什么？

**答案**：

---

**问题**：逻辑斯特回归为什么要对特征进行离散化

**答案**：在工业界，很少直接将连续值作为逻辑回归模型的特征输入，而是将连续特征离散化为一系列0、1特征交给逻辑回归模型，这样做的优势有以下几点：  

1. 离散特征的增加和减少都很容易，易于模型的快速迭代；  
2. 稀疏向量内积乘法运算速度快，计算结果方便存储，容易扩展；  
3. 离散化后的特征对异常数据有很强的鲁棒性：比如一个特征是年龄>30是1，否则0。如果特征没有离散化，一个异常数据“年龄300岁”会给模型造成很大的干扰；  
4. 逻辑回归属于广义线性模型，表达能力受限；单变量离散化为N个后，每个变量有单独的权重，相当于为模型引入了非线性，能够提升模型表达能力，加大拟合；  
5. 离散化后可以进行特征交叉，由M+N个变量变为M*N个变量，进一步引入非线性，提升表达能力；  
6. 特征离散化后，模型会更稳定，比如如果对用户年龄离散化，20-30作为一个区间，不会因为一个用户年龄长了一岁就变成一个完全不同的人。当然处于区间相邻处的样本会刚好相反，所以怎么划分区间是门学问；  
7. 特征离散化以后，起到了简化了逻辑回归模型的作用，降低了模型过拟合的风险。  

李沐曾经说过：模型是使用离散特征还是连续特征，其实是一个“海量离散特征+简单模型” 同 “少量连续特征+复杂模型”的权衡。既可以离散化用线性模型，也可以用连续特征加深度学习。就看是喜欢折腾特征还是折腾模型了。通常来说，前者容易，而且可以n个人一起并行做，有成功经验；后者目前看很赞，能走多远还须拭目以待。

***

**问题**： LR和SVM的联系与区别

**答案**：

**联系：**  

1. LR和SVM都可以处理分类问题，且一般都用于处理线性二分类问题（在改进的情况下可以处理多分类问题） 
2. 两个方法都可以增加不同的正则化项，如l1、l2等等。所以在很多实验中，两种算法的结果是很接近的。 

**区别：**   

1. LR是参数模型，SVM是非参数模型。  
2. 从目标函数来看，区别在于逻辑回归采用的是logistical loss，SVM采用的是hinge loss.这两个损失函数的目的都是增加对分类影响较大的数据点的权重，减少与分类关系较小的数据点的权重。   
3. SVM的处理方法是只考虑support vectors，也就是和分类最相关的少数点，去学习分类器。而逻辑回归通过非线性映射，大大减小了离分类平面较远的点的权重，相对提升了与分类最相关的数据点的权重。   
4. 逻辑回归相对来说模型更简单，好理解，特别是大规模线性分类时比较方便。而SVM的理解和优化相对来说复杂一些，SVM转化为对偶问题后,分类只需要计算与少数几个支持向量的距离,这个在进行复杂核函数计算时优势很明显,能够大大简化模型和计算。  
5. logic 能做的 svm能做，但可能在准确率上有问题，svm能做的logic有的做不了。

---

**问题**： LR与线性回归的区别与联系

**答案**：逻辑回归和线性回归首先都是广义的线性回归，其次经典线性模型的优化目标函数是最小二乘，而逻辑回归则是似然函数，另外线性回归在整个实数域范围内进行预测，敏感度一致，而分类范围，需要在[0,1]。逻辑回归就是一种减小预测范围，将预测值限定为[0,1]间的一种回归模型，因而对于这类问题来说，逻辑回归的鲁棒性比线性回归的要好。

逻辑回归的模型本质上是一个线性回归模型，逻辑回归都是以线性回归为理论支持的。但线性回归模型无法做到sigmoid的非线性形式，sigmoid可以轻松处理0/1分类问题。

---

# 贝叶斯

***

**问题**： 为什么朴素贝叶斯如此“朴素”？

**答案**：因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。

---



---

# KNN

**问题**：KNN中的K如何选取的？

**答案**：KNN中的K值选取对K近邻算法的结果会产生重大影响。如李航博士的一书「统计学习方法」上所说：  

1. 如果选择较小的K值，就相当于用较小的领域中的训练实例进行预测，“学习”近似误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是“学习”的估计误差会增大，换句话说，K值的减小就意味着整体模型变得复杂，容易发生过拟合；  
2. 如果选择较大的K值，就相当于用较大领域中的训练实例进行预测，其优点是可以减少学习的估计误差，但缺点是学习的近似误差会增大。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且K值的增大就意味着整体的模型变得简单。  
3. K=N，则完全不足取，因为此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的累，模型过于简单，忽略了训练实例中大量有用信息。  

在实际应用中，K值一般取一个比较小的数值，例如采用交叉验证法（简单来说，就是一部分样本做训练集，一部分做测试集）来选择最优的K值。

---



# SVM

**问题**：SVM的损失函数是什么？

**答案**：

***

**问题**：核函数起什么作用，常见的有哪些？

**解答**：

***

**问题**：为什么SVM求解要求解他的对偶问题？

**解答**：

1. 对偶问题可以降低原问题的计算复杂度

***

# 随机森林

**问题**：随机森林的步骤？

**解答**：

1. 对训练样本进行bootstrap自助法采样，即有放回的采样，获得M个采样集合；
2. 在这M个采样集合上训练处M个弱决策树。注意到，在决策树生成中还用到了列采样的技巧，原本决策树中节点分裂时，是选择当前节点中所有属性的最优属性进行划分的，但是列采样的技巧是在所有属性中的子集中选最优属性进行划分。这样做可以进一步降低过拟合的可能性；
3. 对这M个训练出来的弱决策树进行集成。
   

# HMM

**问题：**你知道隐马尔可夫模型吗，大概介绍下？

**解答：**

> 问了HMM的几个要素，即：初始概率，状态转移矩阵，发射矩阵，这三个要素，然后我主要讲了下这三个要素的运算过程，提及了一下维特比算法。



***

**问题**：维特比算法其实是一种动态规划算法，动态规划算法通常用来解决什么问题，在HMM里是怎么使用的？

**解答**：

大致描述了下动态规划的最优解问题，然后结合HMM的迭代过程说了一些。（后来仔细看了下，感觉面试官应该还是想听到HMM的理论，因为HMM推导会用到它里面的假设，然后得到递推关系，就可以分解为子问题，利用维特比算法求解）

***

# XGBoost

**问题**：请问（决策树、Random Forest、Booting、Adaboot）GBDT和XGBoost的区别是什么？

**答案**：xgboost类似于gbdt的优化版，不论是精度还是效率上都有了提升。与gbdt相比，具体的优点有： 

1. 损失函数是用泰勒展式二项逼近，而不是像gbdt里的就是一阶导数。
2. 对树的结构进行了正则化约束，防止模型过度复杂，降低了过拟合的可能性。
3. 节点分裂的方式不同，gbdt是用的gini系数，xgboost是经过优化推导后的 。

---

**问题**： 为什么xgboost要用泰勒展开，优势在哪里？

**答案**：xgboost使用了一阶和二阶偏导, 二阶导数有利于梯度下降的更快更准. 使用泰勒展开取得二阶倒数形式, 可以在不选定损失函数具体形式的情况下用于算法优化分析.本质上也就把损失函数的选取和模型算法优化/参数选择分开了. 这种去耦合增加了xgboost的适用性。

---

**问题**：xgboost如何寻找最优特征？是又放回还是无放回的呢？

**答案**：xgboost在训练的过程中给出各个特征的评分，从而表明每个特征对模型训练的重要性. xgboost利用梯度优化模型算法, 样本是不放回的(想象一个样本连续重复抽出,梯度来回踏步会不会高兴). 但xgboost支持子采样, 也就是每轮计算可以不使用全部样本。

---

# 聚类模型

**问题**：聚类算法有哪些，优缺点是什么？

**解答**：

1. 基于层次的聚类


做法是将每个对象都看做一个类，计算两两之间距离最小的对象归为一类，然后重复这样的操作直至成为一个类，这种方式是采用贪心的方法，一步错步步错，时间复杂度过高，可解释性比较好

2. 基于划分的聚类（k-Means）

 原则是保证簇内的数据距离尽可能小，簇间的距离尽可能大，做法是确定需要划分的k的类别数，然后选择初始点，计算所有点到这些点的距离，将距离最近的点划为一簇，然后计算每一簇的平均值当做新的中心点，重复这样的过程直至最后收敛，优点在于时间空间复杂度都不高，但是对于k比较敏感，容易陷入局部最优解

3. 基于密度的聚类（DBSCAN）

 k-means聚类解决不了不规则形状的聚类，而基于密度的聚类可以解决，并对于噪声点比较有效，能发现任意形状的聚类，但是聚类的结果和参数关系很大

4. 基于网络的聚类

 原理是将数据空间划分成网格，计算每个网格中的数据密度，将相邻的高密度网格划为一簇，优点就是划分速度很快，因为是按照网格划分的，和数据点个数没有关系，所以对数据个数不敏感，但是却是以牺牲精度作为代价来实现的

5. 基于模型的聚类 (SOM)

原理是为每一簇拟定一个概率模型，主要是基于概率模型和神经网络模型的方法，假定随机选择的数据服从某种分布，找到获胜单元，然后调整获胜单元周围的向量向其靠拢，最后形成簇，优点是分成簇没有那么硬，分类比较柔和，是以概率的形式表示的，缺点是执行效率不高，当数据较多较复杂时很慢

6. 基于模糊的聚类（FCM）

原理来自于模糊集合论，使用隶属度来确定每个数据属于哪一类的，不断迭代隶属矩阵直至收敛来确定类别，算法对满足正态分布的数据具有很好的效果，缺点是算法的性能依赖于初始簇心，不能保证收敛于一个最优解

# 正则化

**问题**：机器学习中正则化做什么的？

**解答**：约束模型参数，防止过拟合。

***

**问题**：L1正则为什么产生大量稀疏解？



***

**问题**：L1和L2的区别

**答案**：

L1范数（L1 norm）是指向量中各个元素绝对值之和，也有个美称叫“稀疏规则算子”（Lasso regularization）。 

比如 向量A=[1，-1，3]， 那么A的L1范数为 |1|+|-1|+|3|. 
  

简单总结一下就是： 

1. L1范数: 为x向量各个元素绝对值之和。 
2. L2范数: 为x向量各个元素平方和的1/2次方，L2范数又称Euclidean范数或Frobenius范数   
3. Lp范数: 为x向量各个元素绝对值p次方和的1/p次方. 
4. 在支持向量机学习过程中，L1范数实际是一种对于成本函数求解最优的过程，因此，L1范数正则化通过向成本函数中添加L1范数，使得学习得到的结果满足稀疏化，从而方便人类提取特征。 
5. L1范数可以使权值稀疏，方便特征提取。 
6. L2范数可以防止过拟合，提升模型的泛化能力。  

---

**问题**：L1和L2正则先验分别服从什么分布

**答案**：L1是拉普拉斯分布，L2是高斯分布。

---

# 对比

***

**问题**：GBDT和XGBoost的区别

**解答**：

***

**问题**：HMM和CRF有什么区别？

**解答**：

***

**问题**：正则化有 L1 和 L2 正则化区别是什么

**解答**：

***

**问题**：SVM和LR对于离群点的敏感性

**解答**：

***

**问题**：偏差和方差的区别？

**解答**：

***

# 其它

**问题**：贪心和 DP 区别？

**解答**：

***

**问题**：如何解决过拟合的问题？

**解答**：

***

**问题**：相似度的计算方法了解哪些，各自的优缺点是什么？

**解答**：

1. 皮尔逊相关系数：反映的是两个变量之间的线性相关性，它的一个缺点是针对用户之间只有一个共同的评分项不能进行比较，另外没有考虑重叠的评分项数量对相似度的影响。
2. 欧几里得距离： 描述两个变量之间的直线距离，当两个变量至少有一个相同评分项时可以计算。
3. 余弦相似度： 余弦值代表的是空间向量上的夹角余弦值，更体现的是空间上的差异性，与欧几里得距离相比，欧更注重的是绝对数值之间的差异，而余弦更注重的趋势上的不同。
4. 曼哈顿距离：  表示绝对轴距总和，只有上下和左右的方向。

***

**问题**：优化算法及其优缺点？

**解答**：

1. 随机梯度下降，优点：可以一定程度上解决局部最优解的问题；缺点：收敛速度较慢。
2. 批量梯度下降，优点：容易陷入局部最优解；缺点：收敛速度较快
3. mini_batch梯度下降：综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
4. 牛顿法：牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。
5. 拟牛顿法：拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。

***

**问题**：线性分类器与非线性分类器的区别以及优劣

**解答**：

1. 如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
2. 常见的线性分类器有：LR，贝叶斯分类，单层感知机，线性回归。
3. 常见的非线性分类器：决策树、RF、GBDT、多层感知机。
4. SVM两种都有(看线性核还是高斯核)。
5. 线性分类器速度快、编程方便，但是可能拟合效果不会很好。
6. 非线性分类器编程复杂，但是效果拟合能力强。

***

**问题**：协方差和相关性有什么区别？

**解答**：

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。

***

**问题**：简单说下有监督学习和无监督学习的区别？

**解答**：

1. 有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）
2. 无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)

***

**问题**：请简要说说一个完整机器学习项目的流程

1. 抽象成数学问题：明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
    这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。
2. 获取数据：数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。数据要有代表性，否则必然会过拟合。而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
3. 特征预处理与特征选择：良好的数据要能够提取出良好的特征才能真正发挥效力。特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
4. 训练模型与调优：直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。
5. 模型诊断：如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题。诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
6. 模型融合：一般来说，模型融合后都能使得效果有一定提升。而且效果很好。工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
7. 上线运行：这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。





























