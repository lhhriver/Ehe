# 基础知识

## 基础概念

**问题：**BatchNorm和LayerNorm的区别？

**解答：**

1. BatchNorm：计算每个最小批每层的平均值和方差。
2. LayerNorm：独立计算每层每一个样本的均值和方差。

***

**问题**：Dropout 有什么作用？

**解答**：

***

**问题**：激活函数有什么作用？ 常用的激活函数有哪些?

**解答**：

***

**问题：**文本表示方法有哪些？

**解答：**

1. 基于one-hot、tf-idf、textrank等的bag-of-words
2. 主题模型：LSA（SVD）、pLSA、LDA
3. 基于词向量的固定表征：word2vec、fastText、glove
4. 基于词向量的动态表征：elmo、GPT、bert

***

**问题**：Sigmoid 和 ReLU 区别，ReLU 解决了什么问题？

**解答**：

***

## 文本相似度

**问题：**如何计算文本相似度？

**解答：**

1. 直接使用词向量做平均得到句向量，通过余弦相似度来计算。
2. 直接使用词向量做平均得到句向量，通过向量距离来计算。
3. 使用sentenceBert输出两个句子各自的句向量，通过余弦相似度来计算。
4. 使用sentenceBert输出两个句子各自的句向量，拼接起来，通过全连接层，再做二分类。
5. 使用simCSE输出两个句子各自的句向量，通过余弦相似度来计算。



# HMM

**问题**： 如何对中文分词问题用隐马尔可夫模型进行建模和训练？

**解答**：

***

**问题**：HMM的两个假设是什么？三个基本问题是什么？

**解答**：

***

# tf-idf

**问题：**什么是tf-idf？

**解答：**

1. `tf（Term Frequency）词频`：指的是某一个给定的词语在该文件中出现的次数。这个数字通常会被归一化(一般是词频除以文章总词数), 以防止它偏向长的文件。
2. `idf（Inverse Document Frequency）逆文档频率`：log（语料库的文档总数/包含该词的文档数）
3. 优点：
    - 可用某篇文章中出现次数多但在其他文章中出现次数少的词来作为该篇文章的特征词。
    - 使罕见的单词更加突出并且有效地忽略了常用单词 。
    - 易于理解。
4. 缺点：
    - 因为是词袋模型，所以没有考虑词的位置信息，但词的位置是有一定含义的。
    - 并不能反映单词的重要程度和特征词的分布情况。

***

**问题：**各种词向量的特点

**解答：**

1. One-hot：维度灾难 and 语义鸿沟。
2. 矩阵分解（LSA）：利用全局语料特征，但SVD求解计算复杂度大。
3. 基于NNLM/RNNLM的词向量：词向量为副产物，存在效率不高等问题。
4. word2vec、fastText：优化效率高，但是基于局部语料。
5. glove：基于全局预料，结合了LSA和word2vec的优点。
6. elmo、GPT、bert：动态特征。

***

**问题**：词向量平均法做分类的优劣势是什么？

**解答**：

1. 优势
    - 词向量平均的方法做分类模型，主要的优势是模型简单
    - 有参数模型，无参数模型都可以尝试使用，模型选择大
    - 模型速度极快，训练的参数量少在语句少的场景下，效果好
2. 劣势
    - 在语句长的长的场景下，效果会变的很差
    - 语句长，分出的词多，词越多，信息量越杂，简单的做平均的话，重要的词的信息会在平均的过程中极大的被消弱，从而分类效果差

***

**问题**：词向量的基础上如何做优化？

**解答**：

1. 映入一个新的向量，做attention，此向量专门对重要的，和标签相关的词敏感。从而通过加权平均的方式，得到的句向量只包含重要词的信息，忽略不重要的词的信息，从而加强模型的效果。
2. 使用self-attention, 尝试对语句里词的分布做重新的调整，提高模型的学习能力。
3. 使用Transformer encoder或者bert来做学习。

***

# word2vec

**问题**：什么是 Word2Vec？如何训练？ Word2vec，CBOW 和 Skip-gram 的区别是什么？

**解答**：

***

**问题**：你清楚word2vec吗，大致描述下word2vec的结构以及训练方法。

**解答**：

> 从宏观上描述了DNN的一个结构，从输入（大致带过分词，词表构建，one-hot等过程），到隐层，到输出层。然后详细讲了两种训练结构，即CBoW和Skip-Gram，但是当时这两种方法被我说反了。（当时并无觉察）讲完两种训练方法后，大致介绍了下训练时候词表大小过大，输出层过大的优化方法，即：hierarchical softmax和negative sampling。

***

**问题**：Word2Vec训练词向量时，如何提升共现频率很低但是有明显语义关系的词组之间的相似度？

**解答**：

***

**问题**：word2vec中cbow、skip-gram滑动窗口设定大小有何影响

**解答**：

***

**问题**：word2vec负采样具体怎么做的？

**解答**：

***

# RNN

**问题**：RNN 为什么会发生梯度消失？如何改进？

**解答**：

***

# LSTM

**问题**：LSTM 的模型结构是什么？ 为什么说 LSTM 具有长期记忆功能？ LSTM 为什么能抑制梯度衰减？

**解答**：

***

# Transformer

**问题**：为什么transformer块使用LayerNorm而不是BatchNorm？

**解答**：

***



# Bert

**问题：**Bert模型的输出一般接上一个全连接层做下游的任务，是否可以用xgboost代替全连接层？为什么？

**解答：**

1. 不能使用xgboost代替，不能使用任何非参数模型代替全连接层，比如以树模型为基础的模型，SVM。
2. 因为Bert中的参数的调整是需要通过梯度反向传播来进行梯度下降来更新的，如果梯度都没有，那么如何更新参数。如果需要梯度的话，就必须上可对参数求导的模型，如果参数可求导，那么一定是有参数模型，比如逻辑回归，全链接层。

***

**问题：**Bert模型适合做什么任务，不适合做什么任务？

**解答：**

1. Bert模型是Transformer的encoder，适合做情感分类，文本分类，意图识别，命名实体识别，句子是否相似，句子是否有被包含，半指针半标注的SPO三元组抽取，问答，等任务。
2. Bert只是Transformer的encoder，不能做任何生成式的任务。比如翻译，文本摘要，问题生成，等seq2seq任务。

***

**问题：**你看过哪些以Bert为基础的模型？

**解答：**

1. Roberta、Albert、Bert-wwm、XLnet、DistillationBert、SimBert、SimCSE、UNIgram、PET、BART等等。

***

**问题**：Bert模型中有哪些预训练的任务？

**解答**：

1. 首先是MLM任务，masked language model。随机的mask掉一些词，从而基于上下文，通过attention的方法来训练，来预测mask的词。
2. NSP任务，Next Sentence Prediction任务。通过CLS的来进行二分类，查看当前两个句子是否是上下文。词任务的有效性在各大论文中也是有争议的，有的说这个任务有用，有的说这个任务没用。

***

**问题**：Bert模型的做句向量的缺陷？

**解答**：

1. 直接使用Bert做句向量的输出，会发现所有的句向量两两相似度都很高。
2. 因为对于句子来说，大多数的句子都是使用常见的词组成的。
3. Bert的词向量空间是非凸的，大量的常见的词向量都是在0点附近，从而计算出的句子向量，都很相似。

***

**问题**：如何解决Bert句向量的缺陷？

**解答**：

1. 使用双卡的形式，将两个句子传入两个参数共享的Bert模型，将两个句向量做拼接，进行有监督的学习，从而调整Bert参数。此方法叫sentencebert。
2. 使用无监督或者有监督的对比学习，将同一个句子传入相同的bert(dropout = 0.3)得到标签为正例的一个句子对。通过这种方式来做Bert的微调整，得到SimCSE模型。

**问题**：你认为为什么BERT能达到这么好的效果？

我认为BERT的效果很大程度上取决于Transformer的一个Attention机制，即Self-Attention，正如原文标题《Attention is all you need》，注意力机制很好地找出了文本序列的语义相关度，在语义层面上，能够提取到更关键的特征，比如理解序列中的指示代词。其次是Transformer的结构优势，没有RNN的梯度消失问题，理论上支持任意长度的文本，用了position embedding（区别说明了下Transformer的三角函数和BERT的position embedding的不同）

***

**问题**：bert如何实现微调？

**解答**：



***



# XLNET

**问题**：怎么理解XLNet的，XLNet的输入是什么？

**解答**：

***



# 模型介绍

**问题：**介绍一下NNLM？

**解答：**

1. 神经网络语言模型：利用语言模型产生词向量，详细点说就是利用上下文词预测中心词或利用前n个词预测下一个词，词向量只是副产物。
2. 初始化一个V*D的词向量矩阵，其中V是语料库词汇表的大小，D是词向量的维度，随机初始值。
3. 输入层：利用n个上下文词的one-hot编码与词向量矩阵相乘，找到每个单词的词向量，拼接起来，成为一个更长的向量，即D*n维向量。
4. 隐藏层：将D*n维向量经过线性层转换为M维向量，再经过tanh等激活函数。
5. 输出层：再把M维向量经过线性层转换为大小为V维的向量，经过softmax函数即可得到每一个词的概率。
6. 其训练目标是最大化似然函数。

***



# 模型训练

**问题：**如何减少训练好的神经网络模型的运行的时间？

**解答：**

1. 使用使用GPU，或者TPU来做运算
2. 可以尝试做剪执以减少参数
3. 可以尝试做TeacherForcing做知识蒸馏

***

**问题：**如果训练集的数据量太少了怎么办？

**解答：**

1. 可以尝试做数据增强，模拟出其他的数据。
2. 可以尝试使用Prompt的思路，做小样本学习，甚至零样本学习。
3. 如果是分类问题，那么可以使用PET模型来构建Pattern，通过完形填空的方式来间接的解决分类问题。

***

**问题：**如果业务中，训练好的模型的标签多了一个怎么办？

**解答：**

1. 最笨的办法就是尝试重新标记好数据集，将新的标签标记到原来的数据集中，重新训练模型。
2. 将新的标签重新标记到数据集中，将新标签和非新标签分开。
3. 首先用一个二分类模型判断样本是否属于新标签或者非新标签，如果属于新标签则结束，如果属于非新标签，则尝试用老模型继续判断样本属于哪一个老标签。
4. 直接使用PET的方式来训练，在MASK的位置多增加一个Verbalizer，然后做微调即可，完全不用更加其他的代码。

***

# 知识图谱



# 模型对比

**问题**：Bert模型和Transformer模型之间的关系？

**解答**：

1. Transformer模型有encoder和decoder
2. Bert其实就是Transformer的encoder的部分
3. Transformer只是一个空模型，里面的参数都是随机的，需要在下游任务上做有监督的训练，由于参数量大，直接使用Transformer做训练，模型难收敛，并且速度慢。
4. Bert其实是预训练好的Transformer的encoder部分，也就是已经在海量的数据集上做了Transformer的参数的训练了，其参数可以保存下来，直接拿来在下游任务上使用，做调优。

***

**问题：**Word2vec和nnlm的对比

**解答：**

1. 两者本质都是语言模型。
2. 词向量只是nnlm的一个产物，虽说word2vec本质也是语言模型，但其更专注于词向量本身，所以有一些优化算法来提高计算效率。
3. 具体计算方面，在利用上下文词预测中心词时，nnlm是把上下文词的向量进行拼接，word2vec是进行sum，并舍弃隐藏层（为了减少计算量）。
4. word2vec的两个加速算法：hierarchical softmax 和negative sampling。

***

**问题：**Glove与LSA比较

**解答：**

1. 两者都是基于共现矩阵在操作。
2. LSA（Latent Semantic Analysis）可以基于co-occurance matrix构建词向量，实质上是基于全局语料采用SVD进行矩阵分解，然而SVD计算复杂度高。
3. Glove没有直接利用共现矩阵，而是通过ratio的特性，将词向量和ratio联系起来，建立损失函数，采用Adagrad对最小平方损失进行优化（可看作是对LSA一种优化的高效矩阵分解算法）。

***

**问题：**Glove与Word2vec比较

**解答：**

1. Word2vec是局部语料库训练的，其特征提取是基于滑窗的；而glove的滑窗是为了构建co-occurance matrix（上面详细描述了窗口滑动的过程），统计了全部语料库里在固定窗口内的词共线的频次，是基于全局语料的，可见glove需要事先统计共现概率；因此，word2vec可以进行在线学习，glove则需要统计固定语料信息。
2. Word2vec是无监督学习，同样由于不需要人工标注，glove通常被认为是无监督学习，但实际上glove还是有label的，即共现次数$log(X_i,j)$。
3. Word2vec损失函数实质上是**带权重的交叉熵**，权重固定；glove的损失函数是**最小平方损失函数**，权重可以做映射变换。
4. Glove利用了全局信息，使其在训练时收敛更快，训练周期较word2vec较短且效果更好。

***

**问题：**Fasttext与Word2vec比较

**解答：**

1. 都可以无监督学习词向量， fastText训练词向量时会考虑subword。
2. fastText还可以进行有监督学习进行文本分类。
3. 都利用了hierarchical softmax进行加速。
4. Fasttext引入字符级n-gram可以处理长词，未出现过的词，以及低频词。

***

**问题**：Transformer 模型架构, Transformer 和 BERT 的位置编码有什么区别

**解答**：

***

**问题：**描述下Roberta模型和bert有什么不同？

**解答：**

1. Roberta可以直接看成收敛后的bert模型。
2. 在更加大量的数据集上做了Bert预训练任务。
3. 取消了NSP任务，只关注MLM任务。
4. 使用了动态的MASK方式。

***

**问题：**描述下Albert模型和bert有什么不同？

**解答：**

1. Albert模型相当于乞丐版的bert模型，由于Bert模型效果好，很多业务上都想尝试使用bert来做预测，但是发现Bert重大的缺陷就是模型太大，不好移植，且速度慢，不能达到快速响应。于是尝试想看下是否可以生成一个缩小版的Bert，用于快速响应，且效果差不多。
2. Albert首先减少了词向量的维度。
3. Albert还尝试block中的参数做共享。

以上两种方法下，极大的减少了模型参数的个数，且也达到了和Bert-Base差不多的效果

***

**问题**：GPT和BERT有什么不同？

**解答**：

***

**问题**：ELMO、BERT、GPT模型彼此之间有什么区别？

**解答**：

***

**问题**：LSTM相比RNN改进在哪里？

**解答**：

***

# 其它

**问题：**什么是LSA/pLSA/LDA?

**解答：**

1. 都是主题模型。
2. LDA（Latent Dirichlet Allocation）隐含狄利克雷分布：与词向量无关，文档集中每篇文档的主题以概率分布的形式给出。
3. LSA（Latent Semantic Analysis）潜在语义分析：与词向量有关，是文档与所有词在该文档中出现频次的矩阵（词文档矩阵），利用SVD分解之后，左边的矩阵就是词向量。
4. pLSA（Probability Latent Semantic Analysis）概率潜在语义分析：太复杂了，暂无研究。

***

**问题**：现阶段NLP的研究相对CV发展还是很缓慢，你认为是什么原因？

**解答**：

自然语言存在变化性，和不确定性，即语义的抽取对神经网络来说是很难的，在英文，人脑可以通过词形来建立词与词之间的关系，但是语义不确定性很强，比如歧义，一词多义，词序等等都会影响语义。而CV的特征相对固定，如图像处理，filter提取的特征一般是某种轮廓或边缘特征，这些特征对于特定的物体都是固定的，所以效果会更好。（说了很多废话，不清楚，其实总结就是感知智能和认知智能，感知智能很容易实现，即CV，而认知智能有很多挑战，即NLP）

***

**问题**：OOV问题要怎么解决？

**解答**：

***

`NLP部分需要背诵、理解并默写的内容`

1. RNN的结构，LSTM的推导（图和六个公式），GRU的推导和公式，如果能把LSTM和GRU的推导和公式写出来，相信你也已经可以把他们的联系和区别说明白了。
2. LSTM中为什么有sigmoid和tanh两种激活函数，他们的导数形式是什么样的。和梯度消失有什么关系。
3. 从RNN开始，是怎么一步一步解决梯度消失的问题的（RNN-LSTM-attention / self attention），大家了解一下seq2seq的结构，看看Luong关于attention的论文吧。
4. Word2Vec 部分我建议后面参加集训营的同学看一下XinRong写的那篇论文。在读这个之前，你要确保自己已经非常熟悉BP和网上那篇通俗的博客 illustrated-word2vec。
5. 我还想再提一下self attention，因为真的太重要了，基本上面试必问的就是transformer和self-attention了，也建议大家把havard NLP的那个the annotated transformer的notebook跑通，知道里面的self-attention怎么计算的，原始输入包含哪几部分，两个mask有什么不同，用在什么时候，解码部分除了self-attention，还有一个encoder-decoder attention，到底是怎么回事，encoder传了什么东西给decoder。当你对这些都清楚了之后，transformer论文里面的那张著名的结构图你也可以画出来了，你会发现每个箭头都是有意义的。如果最开始感觉havard-nlp的这个比较难，或者内容比较多，也可以运行七月推荐的那个简易版的。最后顺着简易版，找到代码仓库，把仓库里的代码也都跑一遍吧。
6. 其实搞完了transformer之后，就会发现名动江湖的bert已经不再神秘了，多读几遍论文把里面的调过的参数记一下。bert之后的模型我也虽然自己看的少，但是还是建议大家熟悉一下，特别是XLNET，把相对位置编码和permutation了解一下。提到permutation，你要是不能用DFS把N-Queen和array permutation写出来，会不会感觉特别不好意思。
7. 如果你写了分类相关的任务，你至少可以把textCNN，LSTM， LSTM + attention，fasttext，transformer，bert这些说明白，输入输出是什么样的，在LSTM+attention的模型中score或者weight是如何计算的，transformer模型的输出层是什么样的，bert调了哪些参数，对效果有什么提升。
8. 如果是生成式对话的任务，要可以把seq2seq的结构说清楚，attention和self-attention说哪几点大家可以在模拟面的时候和老师沟通一下。beam-search的代码大家github上搜一搜，或者就直接去transformers库里面找好了。这里我不多说beam-search和BFS的关系了，觉得各位都懂，反正你要是被问到了写不出来level-order-traversal，面试结果你瞬间就知道了。
9. tf-idf （代码必须得会，要不然别人会觉得面你是浪费时间），doc2vec，标注等各类任务或者技术，大家结合着自己的项目准备吧。

***
