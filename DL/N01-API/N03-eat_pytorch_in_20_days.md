# How to eat Pytorch in 20 days ?

**ã€Š20å¤©åƒæ‰é‚£åªPytorchã€‹**

* ğŸš€ [githubé¡¹ç›®åœ°å€](https://github.com/lyhue1991/eat_pytorch_in_20_days)
* ğŸ³ [å’Œé²¸ä¸“æ åœ°å€](https://www.kesci.com/home/column/5f2ac5d8af3980002cb1bc08) 

**ã€Š30å¤©åƒæ‰é‚£åªTensorFlow2ã€‹**

* ğŸš€ [githubé¡¹ç›®åœ°å€](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)
* ğŸ³ [å’Œé²¸ä¸“æ åœ°å€](https://www.kesci.com/home/column/5d8ef3c3037db3002d3aa3a0) 

## Pytorch or TensorFlow2

`å…ˆè¯´ç»“è®º`:

- å¦‚æœæ˜¯å·¥ç¨‹å¸ˆï¼Œåº”è¯¥ä¼˜å…ˆé€‰TensorFlow2ã€‚
- å¦‚æœæ˜¯å­¦ç”Ÿæˆ–è€…ç ”ç©¶äººå‘˜ï¼Œåº”è¯¥ä¼˜å…ˆé€‰æ‹©Pytorchã€‚
- å¦‚æœæ—¶é—´è¶³å¤Ÿï¼Œæœ€å¥½TensorFlow2å’ŒPytorchéƒ½è¦å­¦ä¹ æŒæ¡ã€‚



`ç†ç”±å¦‚ä¸‹`ï¼š

1. **åœ¨å·¥ä¸šç•Œæœ€é‡è¦çš„æ˜¯æ¨¡å‹è½åœ°ï¼Œç›®å‰å›½å†…çš„å¤§éƒ¨åˆ†äº’è”ç½‘ä¼ä¸šåªæ”¯æŒTensorFlowæ¨¡å‹çš„åœ¨çº¿éƒ¨ç½²ï¼Œä¸æ”¯æŒPytorchã€‚** å¹¶ä¸”å·¥ä¸šç•Œæ›´åŠ æ³¨é‡çš„æ˜¯æ¨¡å‹çš„é«˜å¯ç”¨æ€§ï¼Œè®¸å¤šæ—¶å€™ä½¿ç”¨çš„éƒ½æ˜¯æˆç†Ÿçš„æ¨¡å‹æ¶æ„ï¼Œè°ƒè¯•éœ€æ±‚å¹¶ä¸å¤§ã€‚
2. **ç ”ç©¶äººå‘˜æœ€é‡è¦çš„æ˜¯å¿«é€Ÿè¿­ä»£å‘è¡¨æ–‡ç« ï¼Œéœ€è¦å°è¯•ä¸€äº›è¾ƒæ–°çš„æ¨¡å‹æ¶æ„ã€‚è€ŒPytorchåœ¨æ˜“ç”¨æ€§ä¸Šç›¸æ¯”TensorFlow2æœ‰ä¸€äº›ä¼˜åŠ¿ï¼Œæ›´åŠ æ–¹ä¾¿è°ƒè¯•ã€‚** å¹¶ä¸”åœ¨2019å¹´ä»¥æ¥åœ¨å­¦æœ¯ç•Œå é¢†äº†å¤§åŠå£æ±Ÿå±±ï¼Œèƒ½å¤Ÿæ‰¾åˆ°çš„ç›¸åº”æœ€æ–°ç ”ç©¶æˆæœæ›´å¤šã€‚
3. TensorFlow2å’ŒPytorchå®é™…ä¸Šæ•´ä½“é£æ ¼å·²ç»éå¸¸ç›¸ä¼¼äº†ï¼Œå­¦ä¼šäº†å…¶ä¸­ä¸€ä¸ªï¼Œå­¦ä¹ å¦å¤–ä¸€ä¸ªå°†æ¯”è¾ƒå®¹æ˜“ã€‚ä¸¤ç§æ¡†æ¶éƒ½æŒæ¡çš„è¯ï¼Œèƒ½å¤Ÿå‚è€ƒçš„å¼€æºæ¨¡å‹æ¡ˆä¾‹æ›´å¤šï¼Œå¹¶ä¸”å¯ä»¥æ–¹ä¾¿åœ°åœ¨ä¸¤ç§æ¡†æ¶ä¹‹é—´åˆ‡æ¢ã€‚



`æœ¬ä¹¦çš„TensorFlowé•œåƒæ•™ç¨‹`ï¼š

ğŸŠ[ã€Š30å¤©åƒæ‰é‚£åªTensorFlow2ã€‹](https://github.com/lyhue1991/eat_tensorflow2_in_30_days)

## æœ¬ä¹¦é¢å‘è¯»è€…

**æœ¬ä¹¦å‡å®šè¯»è€…æœ‰ä¸€å®šçš„æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€ï¼Œä½¿ç”¨è¿‡Kerasæˆ–TensorFlowæˆ–Pytorchæ­å»ºè®­ç»ƒè¿‡ç®€å•çš„æ¨¡å‹ã€‚**

**å¯¹äºæ²¡æœ‰ä»»ä½•æœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ åŸºç¡€çš„åŒå­¦ï¼Œå»ºè®®åœ¨å­¦ä¹ æœ¬ä¹¦æ—¶åŒæ­¥å‚è€ƒé˜…è¯»ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦çš„ç¬¬ä¸€éƒ¨åˆ†"æ·±åº¦å­¦ä¹ åŸºç¡€"å†…å®¹ã€‚**

ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹è¿™æœ¬ä¹¦æ˜¯Kerasä¹‹çˆ¶Francois Cholletæ‰€è‘—ï¼Œè¯¥ä¹¦å‡å®šè¯»è€…æ— ä»»ä½•æœºå™¨å­¦ä¹ çŸ¥è¯†ï¼Œä»¥Kerasä¸ºå·¥å…·ï¼Œä½¿ç”¨ä¸°å¯Œçš„èŒƒä¾‹ç¤ºèŒƒæ·±åº¦å­¦ä¹ çš„æœ€ä½³å®è·µï¼Œè¯¥ä¹¦é€šä¿—æ˜“æ‡‚ï¼Œ**å…¨ä¹¦æ²¡æœ‰ä¸€ä¸ªæ•°å­¦å…¬å¼ï¼Œæ³¨é‡åŸ¹å…»è¯»è€…çš„æ·±åº¦å­¦ä¹ ç›´è§‰ã€‚**

ã€ŠPythonæ·±åº¦å­¦ä¹ ã€‹ä¸€ä¹¦çš„ç¬¬ä¸€éƒ¨åˆ†çš„4ä¸ªç« èŠ‚å†…å®¹å¦‚ä¸‹ï¼Œé¢„è®¡è¯»è€…å¯ä»¥åœ¨20å°æ—¶ä¹‹å†…å­¦å®Œã€‚

1. ä»€ä¹ˆæ˜¯æ·±åº¦å­¦ä¹ 
2. ç¥ç»ç½‘ç»œçš„æ•°å­¦åŸºç¡€
3. ç¥ç»ç½‘ç»œå…¥é—¨
4. æœºå™¨å­¦ä¹ åŸºç¡€

## æœ¬ä¹¦å†™ä½œé£æ ¼

**æœ¬ä¹¦æ˜¯ä¸€æœ¬å¯¹äººç±»ç”¨æˆ·æå…¶å‹å–„çš„Pytorchå…¥é—¨å·¥å…·ä¹¦ï¼ŒDon't let me thinkæ˜¯æœ¬ä¹¦çš„æœ€é«˜è¿½æ±‚ã€‚**

æœ¬ä¹¦ä¸»è¦æ˜¯åœ¨å‚è€ƒPytorchå®˜æ–¹æ–‡æ¡£å’Œå‡½æ•°docæ–‡æ¡£åŸºç¡€ä¸Šæ•´ç†å†™æˆçš„ã€‚å°½ç®¡Pytorchå®˜æ–¹æ–‡æ¡£å·²ç»ç›¸å½“ç®€æ˜æ¸…æ™°ï¼Œä½†æœ¬ä¹¦åœ¨ç¯‡ç« ç»“æ„å’ŒèŒƒä¾‹é€‰å–ä¸Šåšäº†å¤§é‡çš„ä¼˜åŒ–ï¼Œåœ¨ç”¨æˆ·å‹å¥½åº¦æ–¹é¢æ›´èƒœä¸€ç­¹ã€‚

æœ¬ä¹¦æŒ‰ç…§å†…å®¹éš¾æ˜“ç¨‹åº¦ã€è¯»è€…æ£€ç´¢ä¹ æƒ¯å’ŒPytorchè‡ªèº«çš„å±‚æ¬¡ç»“æ„è®¾è®¡å†…å®¹ï¼Œå¾ªåºæ¸è¿›ï¼Œå±‚æ¬¡æ¸…æ™°ï¼Œæ–¹ä¾¿æŒ‰ç…§åŠŸèƒ½æŸ¥æ‰¾ç›¸åº”èŒƒä¾‹ã€‚

æœ¬ä¹¦åœ¨èŒƒä¾‹è®¾è®¡ä¸Šå°½å¯èƒ½ç®€çº¦åŒ–å’Œç»“æ„åŒ–ï¼Œå¢å¼ºèŒƒä¾‹æ˜“è¯»æ€§å’Œé€šç”¨æ€§ï¼Œå¤§éƒ¨åˆ†ä»£ç ç‰‡æ®µåœ¨å®è·µä¸­å¯å³å–å³ç”¨ã€‚

## æœ¬ä¹¦å­¦ä¹ æ–¹æ¡ˆ 

**1. å­¦ä¹ è®¡åˆ’**

æœ¬ä¹¦æ˜¯ä½œè€…åˆ©ç”¨å·¥ä½œä¹‹ä½™å¤§æ¦‚3ä¸ªæœˆå†™æˆçš„ï¼Œå¤§éƒ¨åˆ†è¯»è€…åº”è¯¥åœ¨20å¤©å¯ä»¥å®Œå…¨å­¦ä¼šã€‚é¢„è®¡æ¯å¤©èŠ±è´¹çš„å­¦ä¹ æ—¶é—´åœ¨30åˆ†é’Ÿåˆ°2ä¸ªå°æ—¶ä¹‹é—´ã€‚å½“ç„¶ï¼Œæœ¬ä¹¦ä¹Ÿéå¸¸é€‚åˆä½œä¸ºPytorchçš„å·¥å…·æ‰‹å†Œåœ¨å·¥ç¨‹è½åœ°æ—¶ä½œä¸ºèŒƒä¾‹åº“å‚è€ƒã€‚

**ç‚¹å‡»å­¦ä¹ å†…å®¹è“è‰²æ ‡é¢˜å³å¯è¿›å…¥è¯¥ç« èŠ‚ã€‚**


|  æ—¥æœŸ  | å­¦ä¹ å†…å®¹                                                     | å†…å®¹éš¾åº¦ | é¢„è®¡å­¦ä¹ æ—¶é—´ | æ›´æ–°çŠ¶æ€ |
| :----: | :----------------------------------------------------------- | :------: | :----------: | :------: |
| &nbsp; | [**ä¸€ã€Pytorchçš„å»ºæ¨¡æµç¨‹**](./ä¸€ã€Pytorchçš„å»ºæ¨¡æµç¨‹.md)      |    â­ï¸     |    0hour     |    âœ…     |
|  day1  | [1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-1,ç»“æ„åŒ–æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md) |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
|  day2  | [1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-2,å›¾ç‰‡æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    |   â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
|  day3  | [1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-3,æ–‡æœ¬æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md)    |  â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
|  day4  | [1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹](./1-4,æ—¶é—´åºåˆ—æ•°æ®å»ºæ¨¡æµç¨‹èŒƒä¾‹.md) |  â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
| &nbsp; | [**äºŒã€Pytorchçš„æ ¸å¿ƒæ¦‚å¿µ**](./äºŒã€Pytorchçš„æ ¸å¿ƒæ¦‚å¿µ.md)      |    â­ï¸     |    0hour     |    âœ…     |
|  day5  | [2-1,å¼ é‡æ•°æ®ç»“æ„](./2-1,å¼ é‡æ•°æ®ç»“æ„.md)                    |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
|  day6  | [2-2,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶](./2-2,è‡ªåŠ¨å¾®åˆ†æœºåˆ¶.md)                    |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
|  day7  | [2-3,åŠ¨æ€è®¡ç®—å›¾](./2-3,åŠ¨æ€è®¡ç®—å›¾.md)                        |  â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
| &nbsp; | [**ä¸‰ã€Pytorchçš„å±‚æ¬¡ç»“æ„**](./ä¸‰ã€Pytorchçš„å±‚æ¬¡ç»“æ„.md)      |    â­ï¸     |    0hour     |    âœ…     |
|  day8  | [3-1,ä½é˜¶APIç¤ºèŒƒ](./3-1,ä½é˜¶APIç¤ºèŒƒ.md)                      |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
|  day9  | [3-2,ä¸­é˜¶APIç¤ºèŒƒ](./3-2,ä¸­é˜¶APIç¤ºèŒƒ.md)                      |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
| day10  | [3-3,é«˜é˜¶APIç¤ºèŒƒ](./3-3,é«˜é˜¶APIç¤ºèŒƒ.md)                      |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
| &nbsp; | [**å››ã€Pytorchçš„ä½é˜¶API**](./å››ã€Pytorchçš„ä½é˜¶API.md)        |    â­ï¸     |    0hour     |    âœ…     |
| day11  | [4-1,å¼ é‡çš„ç»“æ„æ“ä½œ](./4-1,å¼ é‡çš„ç»“æ„æ“ä½œ.md)                |  â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
| day12  | [4-2,å¼ é‡çš„æ•°å­¦è¿ç®—](./4-2,å¼ é‡çš„æ•°å­¦è¿ç®—.md)                |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
| day13  | [4-3,nn.functionalå’Œnn.Module](./4-3,nn.functionalå’Œnn.Module.md) |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
| &nbsp; | [**äº”ã€Pytorchçš„ä¸­é˜¶API**](./äº”ã€Pytorchçš„ä¸­é˜¶API.md)        |    â­ï¸     |    0hour     |    âœ…     |
| day14  | [5-1,Datasetå’ŒDataLoader](./5-1,Datasetå’ŒDataLoader.md)      |  â­ï¸â­ï¸â­ï¸â­ï¸â­ï¸   |    2hour     |    âœ…     |
| day15  | [5-2,æ¨¡å‹å±‚](./5-3,æ¨¡å‹å±‚.md)                                |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
| day16  | [5-3,æŸå¤±å‡½æ•°](./5-4,æŸå¤±å‡½æ•°.md)                            |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
| day17  | [5-4,TensorBoardå¯è§†åŒ–](./5-4,TensorBoardå¯è§†åŒ–.md)          |   â­ï¸â­ï¸â­ï¸    |    1hour     |    âœ…     |
| &nbsp; | [**å…­ã€Pytorchçš„é«˜é˜¶API**](./å…­ã€Pytorchçš„é«˜é˜¶API.md)        |    â­ï¸     |    0hour     |    âœ…     |
| day18  | [6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•](./6-1,æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•.md)          |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
| day19  | [6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•](./6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•.md)          |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |
| day20  | [6-3,ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹](./6-3,ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹.md)              |   â­ï¸â­ï¸â­ï¸â­ï¸   |    1hour     |    âœ…     |



**2. å­¦ä¹ ç¯å¢ƒ**


æœ¬ä¹¦å…¨éƒ¨æºç åœ¨jupyterä¸­ç¼–å†™æµ‹è¯•é€šè¿‡ï¼Œå»ºè®®é€šè¿‡gitå…‹éš†åˆ°æœ¬åœ°ï¼Œå¹¶åœ¨jupyterä¸­äº¤äº’å¼è¿è¡Œå­¦ä¹ ã€‚

ä¸ºäº†ç›´æ¥èƒ½å¤Ÿåœ¨jupyterä¸­æ‰“å¼€markdownæ–‡ä»¶ï¼Œå»ºè®®å®‰è£…jupytextï¼Œå°†markdownè½¬æ¢æˆipynbæ–‡ä»¶ã€‚

```python
#å…‹éš†æœ¬ä¹¦æºç åˆ°æœ¬åœ°,ä½¿ç”¨ç äº‘é•œåƒä»“åº“å›½å†…ä¸‹è½½é€Ÿåº¦æ›´å¿«
#!git clone https://gitee.com/Python_Ai_Road/eat_pytorch_in_20_days

#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…jupytextï¼Œä»¥ä¾¿èƒ½å¤Ÿå°†æœ¬ä¹¦å„ç« èŠ‚markdownæ–‡ä»¶è§†ä½œipynbæ–‡ä»¶è¿è¡Œ
#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple -U jupytext

#å»ºè®®åœ¨jupyter notebook ä¸Šå®‰è£…æœ€æ–°ç‰ˆæœ¬pytorch æµ‹è¯•æœ¬ä¹¦ä¸­çš„ä»£ç 
#!pip install -i https://pypi.tuna.tsinghua.edu.cn/simple  -U torch torchvision torchtext torchkeras 
```

```python
import torch 
from torch import nn

print("torch version:", torch.__version__)

a = torch.tensor([[2,1]])
b = torch.tensor([[-1,2]])
c = a@b.t()
print("[[2,1]]@[[-1],[2]] =", c.item())
```

```
torch version: 1.5.0
[[2,1]]@[[-1],[2]] = 0
```

## é¼“åŠ±å’Œè”ç³»ä½œè€…

**å¦‚æœæœ¬ä¹¦å¯¹ä½ æœ‰æ‰€å¸®åŠ©ï¼Œæƒ³é¼“åŠ±ä¸€ä¸‹ä½œè€…ï¼Œè®°å¾—ç»™æœ¬é¡¹ç›®åŠ ä¸€é¢—æ˜Ÿæ˜Ÿstarâ­ï¸ï¼Œå¹¶åˆ†äº«ç»™ä½ çš„æœ‹å‹ä»¬å–”ğŸ˜Š!** 

å¦‚æœå¯¹æœ¬ä¹¦å†…å®¹ç†è§£ä¸Šæœ‰éœ€è¦è¿›ä¸€æ­¥å’Œä½œè€…äº¤æµçš„åœ°æ–¹ï¼Œæ¬¢è¿åœ¨å…¬ä¼—å·"Pythonä¸ç®—æ³•ä¹‹ç¾"ä¸‹ç•™è¨€ã€‚ä½œè€…æ—¶é—´å’Œç²¾åŠ›æœ‰é™ï¼Œä¼šé…Œæƒ…äºˆä»¥å›å¤ã€‚ä¹Ÿå¯ä»¥åœ¨å…¬ä¼—å·åå°å›å¤å…³é”®å­—ï¼š**åŠ ç¾¤**ï¼ŒåŠ å…¥è¯»è€…äº¤æµç¾¤å’Œå¤§å®¶è®¨è®ºã€‚

# Pytorchçš„å»ºæ¨¡æµç¨‹


ä½¿ç”¨Pytorchå®ç°ç¥ç»ç½‘ç»œæ¨¡å‹çš„ä¸€èˆ¬æµç¨‹åŒ…æ‹¬ï¼š

1. å‡†å¤‡æ•°æ®
2. å®šä¹‰æ¨¡å‹
3. è®­ç»ƒæ¨¡å‹
4. è¯„ä¼°æ¨¡å‹
5. ä½¿ç”¨æ¨¡å‹
6. ä¿å­˜æ¨¡å‹


**å¯¹æ–°æ‰‹æ¥è¯´ï¼Œå…¶ä¸­æœ€å›°éš¾çš„éƒ¨åˆ†å®é™…ä¸Šæ˜¯å‡†å¤‡æ•°æ®è¿‡ç¨‹ã€‚** 

æˆ‘ä»¬åœ¨å®è·µä¸­é€šå¸¸ä¼šé‡åˆ°çš„æ•°æ®ç±»å‹åŒ…æ‹¬ç»“æ„åŒ–æ•°æ®ï¼Œå›¾ç‰‡æ•°æ®ï¼Œæ–‡æœ¬æ•°æ®ï¼Œæ—¶é—´åºåˆ—æ•°æ®ã€‚

æˆ‘ä»¬å°†åˆ†åˆ«ä»¥titanicç”Ÿå­˜é¢„æµ‹é—®é¢˜ï¼Œcifar2å›¾ç‰‡åˆ†ç±»é—®é¢˜ï¼Œimdbç”µå½±è¯„è®ºåˆ†ç±»é—®é¢˜ï¼Œå›½å†…æ–°å† ç–«æƒ…ç»“æŸæ—¶é—´é¢„æµ‹é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºåº”ç”¨Pytorchå¯¹è¿™å››ç±»æ•°æ®çš„å»ºæ¨¡æ–¹æ³•ã€‚

## **ç»“æ„åŒ–æ•°æ®**å»ºæ¨¡æµç¨‹èŒƒä¾‹

```python
import os
import datetime

# æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s" % nowtime)

# macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"] = "TRUE" 
```

### å‡†å¤‡æ•°æ®


Titanicæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ä¹˜å®¢ä¿¡æ¯é¢„æµ‹ä»–ä»¬åœ¨Titanicå·æ’å‡»å†°å±±æ²‰æ²¡åèƒ½å¦ç”Ÿå­˜ã€‚ç»“æ„åŒ–æ•°æ®ä¸€èˆ¬ä¼šä½¿ç”¨Pandasä¸­çš„DataFrameè¿›è¡Œé¢„å¤„ç†ã€‚


```python
import numpy as np 
import pandas as pd
import matplotlib.pyplot as plt
import torch 
from torch import nn 
from torch.utils.data import Dataset, DataLoader, TensorDataset

dftrain_raw = pd.read_csv('./data/titanic/train.csv')
dftest_raw = pd.read_csv('./data/titanic/test.csv')
dftrain_raw.head(10)
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-476734.jpg)


å­—æ®µè¯´æ˜ï¼š

* Survivedï¼š0ä»£è¡¨æ­»äº¡ï¼Œ1ä»£è¡¨å­˜æ´»ã€yæ ‡ç­¾ã€‘
* Pclassï¼šä¹˜å®¢æ‰€æŒç¥¨ç±»ï¼Œæœ‰ä¸‰ç§å€¼(1,2,3) ã€è½¬æ¢æˆonehotç¼–ç ã€‘
* Nameï¼šä¹˜å®¢å§“å ã€èˆå»ã€‘
* Sexï¼šä¹˜å®¢æ€§åˆ« ã€è½¬æ¢æˆboolç‰¹å¾ã€‘
* Ageï¼šä¹˜å®¢å¹´é¾„(æœ‰ç¼ºå¤±) ã€æ•°å€¼ç‰¹å¾ï¼Œæ·»åŠ â€œå¹´é¾„æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘
* SibSpï¼šä¹˜å®¢å…„å¼Ÿå§å¦¹/é…å¶çš„ä¸ªæ•°(æ•´æ•°å€¼) ã€æ•°å€¼ç‰¹å¾ã€‘
* Parchï¼šä¹˜å®¢çˆ¶æ¯/å­©å­çš„ä¸ªæ•°(æ•´æ•°å€¼)ã€æ•°å€¼ç‰¹å¾ã€‘
* Ticketï¼šç¥¨å·(å­—ç¬¦ä¸²)ã€èˆå»ã€‘
* Fareï¼šä¹˜å®¢æ‰€æŒç¥¨çš„ä»·æ ¼(æµ®ç‚¹æ•°ï¼Œ0-500ä¸ç­‰) ã€æ•°å€¼ç‰¹å¾ã€‘
* Cabinï¼šä¹˜å®¢æ‰€åœ¨èˆ¹èˆ±(æœ‰ç¼ºå¤±) ã€æ·»åŠ â€œæ‰€åœ¨èˆ¹èˆ±æ˜¯å¦ç¼ºå¤±â€ä½œä¸ºè¾…åŠ©ç‰¹å¾ã€‘
* Embarkedï¼šä¹˜å®¢ç™»èˆ¹æ¸¯å£ï¼šSã€Cã€Q(æœ‰ç¼ºå¤±)ã€è½¬æ¢æˆonehotç¼–ç ï¼Œå››ç»´åº¦ S,C,Q,nanã€‘



åˆ©ç”¨Pandasçš„æ•°æ®å¯è§†åŒ–åŠŸèƒ½æˆ‘ä»¬å¯ä»¥ç®€å•åœ°è¿›è¡Œæ¢ç´¢æ€§æ•°æ®åˆ†æEDAï¼ˆExploratory Data Analysisï¼‰ã€‚

labelåˆ†å¸ƒæƒ…å†µ

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw['Survived'].value_counts().plot(kind = 'bar',
                                                 figsize = (12,8),
                                                 fontsize=15,
                                                 rot = 0)
ax.set_ylabel('Counts', fontsize = 15)
ax.set_xlabel('Survived', fontsize = 15)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-445755.jpg)


å¹´é¾„åˆ†å¸ƒæƒ…å†µ

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw['Age'].plot(kind = 'hist',
                             bins = 20,
                             color= 'purple',
                             figsize = (12,8),
                             fontsize=15)

ax.set_ylabel('Frequency',fontsize = 15)
ax.set_xlabel('Age',fontsize = 15)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-466212.jpg)


å¹´é¾„å’Œlabelçš„ç›¸å…³æ€§

```python
%matplotlib inline
%config InlineBackend.figure_format = 'png'
ax = dftrain_raw.query('Survived == 0')['Age'].plot(kind = 'density',
                                                    figsize = (12,8),
                                                    fontsize=15)
dftrain_raw.query('Survived == 1')['Age'].plot(kind = 'density',
                                               figsize = (12,8),
                                               fontsize=15)
ax.legend(['Survived==0','Survived==1'],fontsize = 12)
ax.set_ylabel('Density',fontsize = 15)
ax.set_xlabel('Age',fontsize = 15)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-472779.jpg)


ä¸‹é¢ä¸ºæ­£å¼çš„æ•°æ®é¢„å¤„ç†

```python
def preprocessing(dfdata):
    dfresult = pd.DataFrame()

    # Pclass
    dfPclass = pd.get_dummies(dfdata['Pclass'])
    dfPclass.columns = ['Pclass_' + str(x) for x in dfPclass.columns]
    dfresult = pd.concat([dfresult, dfPclass], axis=1)

    # Sex
    dfSex = pd.get_dummies(dfdata['Sex'])
    dfresult = pd.concat([dfresult, dfSex], axis=1)

    # Age
    dfresult['Age'] = dfdata['Age'].fillna(0)
    dfresult['Age_null'] = pd.isna(dfdata['Age']).astype('int32')

    # SibSp,Parch,Fare
    dfresult['SibSp'] = dfdata['SibSp']
    dfresult['Parch'] = dfdata['Parch']
    dfresult['Fare'] = dfdata['Fare']

    # Carbin
    dfresult['Cabin_null'] = pd.isna(dfdata['Cabin']).astype('int32')

    # Embarked
    dfEmbarked = pd.get_dummies(dfdata['Embarked'], dummy_na=True)
    dfEmbarked.columns = ['Embarked_' + str(x) for x in dfEmbarked.columns]
    dfresult = pd.concat([dfresult, dfEmbarked], axis=1)

    return (dfresult)


x_train = preprocessing(dftrain_raw).values
y_train = dftrain_raw[['Survived']].values

x_test = preprocessing(dftest_raw).values
y_test = dftest_raw[['Survived']].values

print("x_train.shape =", x_train.shape)
print("x_test.shape =", x_test.shape)

print("y_train.shape =", y_train.shape)
print("y_test.shape =", y_test.shape)
```

```
x_train.shape = (712, 15)
x_test.shape = (179, 15)
y_train.shape = (712, 1)
y_test.shape = (179, 1)
```


è¿›ä¸€æ­¥ä½¿ç”¨DataLoaderå’ŒTensorDatasetå°è£…æˆå¯ä»¥è¿­ä»£çš„æ•°æ®ç®¡é“ã€‚

```python
dl_train = DataLoader(TensorDataset(torch.tensor(x_train).float(),
                                    torch.tensor(y_train).float()),
                      shuffle = True, 
                      batch_size = 8)
dl_valid = DataLoader(TensorDataset(torch.tensor(x_test).float(),
                                    torch.tensor(y_test).float()),
                      shuffle = False, 
                      batch_size = 8)
```

```python
# æµ‹è¯•æ•°æ®ç®¡é“
for features,labels in dl_train:
    print(features,labels)
    break
```

```
tensor([[  0.0000,   0.0000,   1.0000,   0.0000,   1.0000,   0.0000,   1.0000,
           0.0000,   0.0000,   7.8958,   1.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  1.0000,   0.0000,   0.0000,   0.0000,   1.0000,   0.0000,   1.0000,
           0.0000,   0.0000,  30.5000,   0.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  1.0000,   0.0000,   0.0000,   1.0000,   0.0000,  31.0000,   0.0000,
           1.0000,   0.0000, 113.2750,   0.0000,   1.0000,   0.0000,   0.0000,
           0.0000],
        [  1.0000,   0.0000,   0.0000,   0.0000,   1.0000,  60.0000,   0.0000,
           0.0000,   0.0000,  26.5500,   1.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  0.0000,   0.0000,   1.0000,   0.0000,   1.0000,  28.0000,   0.0000,
           0.0000,   0.0000,  22.5250,   1.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  0.0000,   0.0000,   1.0000,   0.0000,   1.0000,  32.0000,   0.0000,
           0.0000,   0.0000,   8.3625,   1.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  0.0000,   1.0000,   0.0000,   1.0000,   0.0000,  28.0000,   0.0000,
           0.0000,   0.0000,  13.0000,   1.0000,   0.0000,   0.0000,   1.0000,
           0.0000],
        [  1.0000,   0.0000,   0.0000,   0.0000,   1.0000,  36.0000,   0.0000,
           0.0000,   1.0000, 512.3292,   0.0000,   1.0000,   0.0000,   0.0000,
           0.0000]]) tensor([[0.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [1.],
        [1.]])
```



### å®šä¹‰æ¨¡å‹-nn.Sequential

ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼š

1. ä½¿ç”¨**nn.Sequential**æŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹
2. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹
3. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨è¿›è¡Œå°è£…

æ­¤å¤„é€‰æ‹©ä½¿ç”¨æœ€ç®€å•çš„nn.Sequentialï¼ŒæŒ‰å±‚é¡ºåºæ¨¡å‹ã€‚

```python
def create_net():
    net = nn.Sequential()
    net.add_module("linear1",nn.Linear(15,20))
    net.add_module("relu1",nn.ReLU())
    net.add_module("linear2",nn.Linear(20,15))
    net.add_module("relu2",nn.ReLU())
    net.add_module("linear3",nn.Linear(15,1))
    net.add_module("sigmoid",nn.Sigmoid())
    return net
    
net = create_net()
print(net)
```

```
Sequential(
  (linear1): Linear(in_features=15, out_features=20, bias=True)
  (relu1): ReLU()
  (linear2): Linear(in_features=20, out_features=15, bias=True)
  (relu2): ReLU()
  (linear3): Linear(in_features=15, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```



```python
from torchkeras import summary

summary(net,input_shape=(15,))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                   [-1, 20]             320
              ReLU-2                   [-1, 20]               0
            Linear-3                   [-1, 15]             315
              ReLU-4                   [-1, 15]               0
            Linear-5                    [-1, 1]              16
           Sigmoid-6                    [-1, 1]               0
================================================================
Total params: 651
Trainable params: 651
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000057
Forward/backward pass size (MB): 0.000549
Params size (MB): 0.002483
Estimated Total Size (MB): 0.003090
----------------------------------------------------------------
```



### è®­ç»ƒæ¨¡å‹-è„šæœ¬å½¢å¼


Pytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼š

1. è„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯
2. å‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯
3. ç±»å½¢å¼è®­ç»ƒå¾ªç¯

æ­¤å¤„ä»‹ç»ä¸€ç§è¾ƒé€šç”¨çš„è„šæœ¬å½¢å¼ã€‚

```python
from sklearn.metrics import accuracy_score

loss_func = nn.BCELoss()
optimizer = torch.optim.Adam(params=net.parameters(), lr = 0.01)
metric_func = lambda y_pred,y_true: accuracy_score(y_true.data.numpy(),y_pred.data.numpy()>0.5)
metric_name = "accuracy"
```

```python
epochs = 10
log_step_freq = 30

dfhistory = pd.DataFrame(columns = ["epoch", "loss", metric_name, "val_loss", "val_"+metric_name]) 
print("Start Training...")
nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print("=========="*8 + "%s" % nowtime)

for epoch in range(1,epochs+1):  

    # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
    net.train()
    loss_sum = 0.0
    metric_sum = 0.0
    step = 1
    
    for step, (features,labels) in enumerate(dl_train, 1):
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()

        # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
        predictions = net(features)
        loss = loss_func(predictions,labels)
        metric = metric_func(predictions,labels)
        
        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
        loss.backward()
        optimizer.step()

        # æ‰“å°batchçº§åˆ«æ—¥å¿—
        loss_sum += loss.item()
        metric_sum += metric.item()
        
        if step % log_step_freq == 0:
            print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                  (step, loss_sum/step, metric_sum/step))
    
    # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
    net.eval()
    val_loss_sum = 0.0
    val_metric_sum = 0.0
    val_step = 1

    for val_step, (features,labels) in enumerate(dl_valid, 1):
        # å…³é—­æ¢¯åº¦è®¡ç®—
        with torch.no_grad():
            predictions = net(features)
            val_loss = loss_func(predictions,labels)
            val_metric = metric_func(predictions,labels)
        val_loss_sum += val_loss.item()
        val_metric_sum += val_metric.item()

    # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
    info = (epoch, loss_sum/step, metric_sum/step, val_loss_sum/val_step, val_metric_sum/val_step)
    dfhistory.loc[epoch-1] = info
    
    # æ‰“å°epochçº§åˆ«æ—¥å¿—
    print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + \
          "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") 
          %info)
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)
        
print('Finished Training...')
```

```
Start Training...
================================================================================2020-06-17 20:53:49
[step = 30] loss: 0.703, accuracy: 0.583
[step = 60] loss: 0.629, accuracy: 0.675

EPOCH = 1, loss = 0.643,accuracy  = 0.673, val_loss = 0.621, val_accuracy = 0.725

================================================================================2020-06-17 20:53:49
[step = 30] loss: 0.653, accuracy: 0.662
[step = 60] loss: 0.624, accuracy: 0.673

EPOCH = 2, loss = 0.621,accuracy  = 0.669, val_loss = 0.519, val_accuracy = 0.708

================================================================================2020-06-17 20:53:49
[step = 30] loss: 0.582, accuracy: 0.688
[step = 60] loss: 0.555, accuracy: 0.723

EPOCH = 3, loss = 0.543,accuracy  = 0.740, val_loss = 0.516, val_accuracy = 0.741
...
...
...
================================================================================2020-06-17 20:53:50
[step = 30] loss: 0.436, accuracy: 0.796
[step = 60] loss: 0.460, accuracy: 0.794

EPOCH = 10, loss = 0.462,accuracy  = 0.787, val_loss = 0.415, val_accuracy = 0.810

================================================================================2020-06-17 20:53:51
Finished Training...
```



### è¯„ä¼°æ¨¡å‹


æˆ‘ä»¬é¦–å…ˆè¯„ä¼°ä¸€ä¸‹æ¨¡å‹åœ¨è®­ç»ƒé›†å’ŒéªŒè¯é›†ä¸Šçš„æ•ˆæœã€‚

```python
dfhistory 
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084824-945417.png)

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084824-975925.png)



```python
plot_metric(dfhistory,"accuracy")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084824-930226.png)



### ä½¿ç”¨æ¨¡å‹

```python
#é¢„æµ‹æ¦‚ç‡
y_pred_probs = net(torch.tensor(x_test[0:10]).float()).data
y_pred_probs
```

```
tensor([[0.0119],
        [0.6029],
        [0.2970],
        [0.5717],
        [0.5034],
        [0.8655],
        [0.0572],
        [0.9182],
        [0.5038],
        [0.1739]])
```

```python
#é¢„æµ‹ç±»åˆ«
y_pred = torch.where(y_pred_probs>0.5,
                     torch.ones_like(y_pred_probs),
                     torch.zeros_like(y_pred_probs))
y_pred
```

```
tensor([[0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [0.],
        [1.],
        [1.],
        [0.]])
```



### ä¿å­˜æ¨¡å‹-ä¸¤ç§æ–¹å¼

Pytorch æœ‰ä¸¤ç§ä¿å­˜æ¨¡å‹çš„æ–¹å¼ï¼Œéƒ½æ˜¯é€šè¿‡è°ƒç”¨pickleåºåˆ—åŒ–æ–¹æ³•å®ç°çš„ã€‚

1. ç¬¬ä¸€ç§æ–¹æ³•åªä¿å­˜æ¨¡å‹å‚æ•°ã€‚
2. ç¬¬äºŒç§æ–¹æ³•ä¿å­˜å®Œæ•´æ¨¡å‹ã€‚

æ¨èä½¿ç”¨ç¬¬ä¸€ç§ï¼Œç¬¬äºŒç§æ–¹æ³•å¯èƒ½åœ¨åˆ‡æ¢è®¾å¤‡å’Œç›®å½•çš„æ—¶å€™å‡ºç°å„ç§é—®é¢˜ã€‚



**1. ä¿å­˜æ¨¡å‹å‚æ•°(æ¨è)**

```python
print(net.state_dict().keys())
```

```
odict_keys(['linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias', 'linear3.weight', 'linear3.bias'])
```

```python
# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(net.state_dict(), "./data/net_parameter.pkl")

net_clone = create_net()
net_clone.load_state_dict(torch.load("./data/net_parameter.pkl"))

net_clone.forward(torch.tensor(x_test[0:10]).float()).data
```

```
tensor([[0.0119],
        [0.6029],
        [0.2970],
        [0.5717],
        [0.5034],
        [0.8655],
        [0.0572],
        [0.9182],
        [0.5038],
        [0.1739]])
```



**2. ä¿å­˜å®Œæ•´æ¨¡å‹(ä¸æ¨è)**

```python
torch.save(net, './data/net_model.pkl')
net_loaded = torch.load('./data/net_model.pkl')
net_loaded(torch.tensor(x_test[0:10]).float()).data
```

```
tensor([[0.0119],
        [0.6029],
        [0.2970],
        [0.5717],
        [0.5034],
        [0.8655],
        [0.0572],
        [0.9182],
        [0.5038],
        [0.1739]])
```



## **å›¾ç‰‡æ•°æ®**å»ºæ¨¡æµç¨‹èŒƒä¾‹

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```



### å‡†å¤‡æ•°æ®


cifar2æ•°æ®é›†ä¸ºcifar10æ•°æ®é›†çš„å­é›†ï¼ŒåªåŒ…æ‹¬å‰ä¸¤ç§ç±»åˆ«airplaneå’Œautomobileã€‚

è®­ç»ƒé›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„5000å¼ ï¼Œæµ‹è¯•é›†æœ‰airplaneå’Œautomobileå›¾ç‰‡å„1000å¼ ã€‚

cifar2ä»»åŠ¡çš„ç›®æ ‡æ˜¯è®­ç»ƒä¸€ä¸ªæ¨¡å‹æ¥å¯¹é£æœºairplaneå’ŒæœºåŠ¨è½¦automobileä¸¤ç§å›¾ç‰‡è¿›è¡Œåˆ†ç±»ã€‚

æˆ‘ä»¬å‡†å¤‡çš„Cifar2æ•°æ®é›†çš„æ–‡ä»¶ç»“æ„å¦‚ä¸‹æ‰€ç¤ºã€‚

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084825-772787.jpg)



åœ¨Pytorchä¸­æ„å»ºå›¾ç‰‡æ•°æ®ç®¡é“é€šå¸¸æœ‰ä¸‰ç§æ–¹æ³•ã€‚

1. ç¬¬ä¸€ç§æ˜¯ä½¿ç”¨ torchvisionä¸­çš„datasets.ImageFolderæ¥è¯»å–å›¾ç‰‡ç„¶åç”¨ DataLoaderæ¥å¹¶è¡ŒåŠ è½½ã€‚
2. ç¬¬äºŒç§æ˜¯é€šè¿‡ç»§æ‰¿ torch.utils.data.Dataset å®ç°ç”¨æˆ·è‡ªå®šä¹‰è¯»å–é€»è¾‘ç„¶åç”¨ DataLoaderæ¥å¹¶è¡ŒåŠ è½½ã€‚
3. ç¬¬ä¸‰ç§æ–¹æ³•æ˜¯è¯»å–ç”¨æˆ·è‡ªå®šä¹‰æ•°æ®é›†çš„é€šç”¨æ–¹æ³•ï¼Œæ—¢å¯ä»¥è¯»å–å›¾ç‰‡æ•°æ®é›†ï¼Œä¹Ÿå¯ä»¥è¯»å–æ–‡æœ¬æ•°æ®é›†ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»ç¬¬ä¸€ç§æ–¹æ³•ã€‚


```python
import torch 
from torch import nn
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms, datasets
```

```python
transform_train = transforms.Compose([transforms.ToTensor()])
transform_valid = transforms.Compose([transforms.ToTensor()])
```

```python
ds_train = datasets.ImageFolder("./data/cifar2/train/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())
ds_valid = datasets.ImageFolder("./data/cifar2/test/",
                                transform = transform_valid,
                                target_transform= lambda t:torch.tensor([t]).float())

print(ds_train.class_to_idx)
```

```
{'0_airplane': 0, '1_automobile': 1}
```

```python
dl_train = DataLoader(ds_train,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)
dl_valid = DataLoader(ds_valid,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)
```



```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬
from matplotlib import pyplot as plt 

plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_train[i]
    img = img.permute(1,2,0)
    ax = plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d" % label.item())
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-500461.png)

```python
# Pytorchçš„å›¾ç‰‡é»˜è®¤é¡ºåºæ˜¯ Batch,Channel,Width,Height
for x,y in dl_train:
    print(x.shape,y.shape) 
    break
```

```
torch.Size([50, 3, 32, 32]) torch.Size([50, 1])
```



### å®šä¹‰æ¨¡å‹-ç»§æ‰¿nn.Module

ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼š

1. ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹
2. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹
3. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨(nn.Sequential,nn.ModuleList,nn.ModuleDict)è¿›è¡Œå°è£…

æ­¤å¤„é€‰æ‹©é€šè¿‡ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚

```python
#æµ‹è¯•AdaptiveMaxPool2dçš„æ•ˆæœ
pool = nn.AdaptiveMaxPool2d((1,1))
t = torch.randn(10,8,32,32)
pool(t).shape 
```

```
torch.Size([10, 8, 1, 1])
```



```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)
        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)
        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)
        self.dropout = nn.Dropout2d(p = 0.1)
        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(64,32)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(32,1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self,x):
        x = self.conv1(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        y = self.sigmoid(x)
        return y
        
net = Net()
print(net)
```

```
Net(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```

```python
import torchkeras

torchkeras.summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```



### è®­ç»ƒæ¨¡å‹-å‡½æ•°å½¢å¼


Pytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼š

1. è„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯
2. å‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯
3. ç±»å½¢å¼è®­ç»ƒå¾ªç¯

æ­¤å¤„ä»‹ç»ä¸€ç§è¾ƒé€šç”¨çš„**å‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯**ã€‚


```python
import pandas as pd 
from sklearn.metrics import roc_auc_score

model = net
model.optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)
model.loss_func = torch.nn.BCELoss()
model.metric_func = lambda y_pred,y_true: roc_auc_score(y_true.data.numpy(),y_pred.data.numpy())
model.metric_name = "auc"
```

```python
def train_step(model,features,labels):
    
    # è®­ç»ƒæ¨¡å¼ï¼Œdropoutå±‚å‘ç”Ÿä½œç”¨
    model.train()
    
    # æ¢¯åº¦æ¸…é›¶
    model.optimizer.zero_grad()
    
    # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)

    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    model.optimizer.step()
    return loss.item(),metric.item()


def valid_step(model,features,labels):
    
    # é¢„æµ‹æ¨¡å¼ï¼Œdropoutå±‚ä¸å‘ç”Ÿä½œç”¨
    model.eval()
    # å…³é—­æ¢¯åº¦è®¡ç®—
    with torch.no_grad():
        predictions = model(features)
        loss = model.loss_func(predictions,labels)
        metric = model.metric_func(predictions,labels)
    
    return loss.item(), metric.item()


# æµ‹è¯•train_stepæ•ˆæœ
features,labels = next(iter(dl_train))
train_step(model,features,labels)
```

```
(0.6922046542167664, 0.5088566827697262)
```



```python
def train_model(model,epochs,dl_train,dl_valid,log_step_freq):

    metric_name = model.metric_name
    dfhistory = pd.DataFrame(columns = ["epoch","loss",metric_name,"val_loss","val_"+metric_name]) 
    print("Start Training...")
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("=========="*8 + "%s"%nowtime)

    for epoch in range(1, epochs+1):  
        # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
        loss_sum = 0.0
        metric_sum = 0.0
        step = 1

        for step, (features,labels) in enumerate(dl_train, 1):
            loss, metric = train_step(model, features, labels)

            # æ‰“å°batchçº§åˆ«æ—¥å¿—
            loss_sum += loss
            metric_sum += metric
            if step%log_step_freq == 0:   
                print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                      (step, loss_sum/step, metric_sum/step))

        # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
        val_loss_sum = 0.0
        val_metric_sum = 0.0
        val_step = 1

        for val_step, (features,labels) in enumerate(dl_valid, 1):
            val_loss,val_metric = valid_step(model,features,labels)
            val_loss_sum += val_loss
            val_metric_sum += val_metric

        # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
        info = (epoch, loss_sum/step, metric_sum/step, 
                val_loss_sum/val_step, val_metric_sum/val_step)
        dfhistory.loc[epoch-1] = info

        # æ‰“å°epochçº§åˆ«æ—¥å¿—
        print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + \
              "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") 
              %info)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n"+"=========="*8 + "%s"%nowtime)

    print('Finished Training...')
    return dfhistory
```



```python
epochs = 20
dfhistory = train_model(model,epochs,dl_train,dl_valid,log_step_freq = 50)
```

```
Start Training...
================================================================================2020-06-28 20:47:56
[step = 50] loss: 0.691, auc: 0.627
[step = 100] loss: 0.690, auc: 0.673
[step = 150] loss: 0.688, auc: 0.699
[step = 200] loss: 0.686, auc: 0.716

EPOCH = 1, loss = 0.686,auc  = 0.716, val_loss = 0.678, val_auc = 0.806

================================================================================2020-06-28 20:48:18
[step = 50] loss: 0.677, auc: 0.780
[step = 100] loss: 0.675, auc: 0.775
[step = 150] loss: 0.672, auc: 0.782
[step = 200] loss: 0.669, auc: 0.779

EPOCH = 2, loss = 0.669,auc  = 0.779, val_loss = 0.651, val_auc = 0.815

......

================================================================================2020-06-28 20:54:24
[step = 50] loss: 0.386, auc: 0.914
[step = 100] loss: 0.392, auc: 0.913
[step = 150] loss: 0.395, auc: 0.911
[step = 200] loss: 0.398, auc: 0.911

EPOCH = 19, loss = 0.398,auc  = 0.911, val_loss = 0.449, val_auc = 0.924

================================================================================2020-06-28 20:54:43
[step = 50] loss: 0.416, auc: 0.917
[step = 100] loss: 0.417, auc: 0.916
[step = 150] loss: 0.404, auc: 0.918
[step = 200] loss: 0.402, auc: 0.918

EPOCH = 20, loss = 0.402,auc  = 0.918, val_loss = 0.535, val_auc = 0.925

================================================================================2020-06-28 20:55:03
Finished Training...
```



### è¯„ä¼°æ¨¡å‹

```python
dfhistory 
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084825-054341.png)

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-496505.png)

```python
plot_metric(dfhistory,"auc")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-482007.png)



### ä½¿ç”¨æ¨¡å‹

```python
def predict(model,dl):
    model.eval()
    with torch.no_grad():
        result = torch.cat([model.forward(t[0]) for t in dl])
    return(result.data)
```

```python
#é¢„æµ‹æ¦‚ç‡
y_pred_probs = predict(model,dl_valid)
y_pred_probs
```

```
tensor([[8.4032e-01],
        [1.0407e-02],
        [5.4146e-04],
        ...,
        [1.4471e-02],
        [1.7673e-02],
        [4.5081e-01]])
```

```python
#é¢„æµ‹ç±»åˆ«
y_pred = torch.where(y_pred_probs>0.5,
                     torch.ones_like(y_pred_probs),
                     torch.zeros_like(y_pred_probs))
y_pred
```

```
tensor([[1.],
        [0.],
        [0.],
        ...,
        [0.],
        [0.],
        [0.]])
```



### ä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨ä¿å­˜å‚æ•°æ–¹å¼ä¿å­˜Pytorchæ¨¡å‹ã€‚

```python
print(model.state_dict().keys())
```

```
odict_keys(['conv1.weight', 'conv1.bias', 'conv2.weight', 'conv2.bias', 'linear1.weight', 'linear1.bias', 'linear2.weight', 'linear2.bias'])
```

```python
# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(model.state_dict(), "./data/model_parameter.pkl")

net_clone = Net()
net_clone.load_state_dict(torch.load("./data/model_parameter.pkl"))

predict(net_clone,dl_valid)
```

```
tensor([[0.0204],
        [0.7692],
        [0.4967],
        ...,
        [0.6078],
        [0.7182],
        [0.8251]])
```



## **æ–‡æœ¬æ•°æ®**å»ºæ¨¡æµç¨‹èŒƒä¾‹

### å‡†å¤‡æ•°æ®


imdbæ•°æ®é›†çš„ç›®æ ‡æ˜¯æ ¹æ®ç”µå½±è¯„è®ºçš„æ–‡æœ¬å†…å®¹é¢„æµ‹è¯„è®ºçš„æƒ…æ„Ÿæ ‡ç­¾ã€‚

è®­ç»ƒé›†æœ‰20000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œæµ‹è¯•é›†æœ‰5000æ¡ç”µå½±è¯„è®ºæ–‡æœ¬ï¼Œå…¶ä¸­æ­£é¢è¯„è®ºå’Œè´Ÿé¢è¯„è®ºéƒ½å„å ä¸€åŠã€‚

æ–‡æœ¬æ•°æ®é¢„å¤„ç†è¾ƒä¸ºç¹çï¼ŒåŒ…æ‹¬ä¸­æ–‡åˆ‡è¯ï¼ˆæœ¬ç¤ºä¾‹ä¸æ¶‰åŠï¼‰ï¼Œæ„å»ºè¯å…¸ï¼Œç¼–ç è½¬æ¢ï¼Œåºåˆ—å¡«å……ï¼Œæ„å»ºæ•°æ®ç®¡é“ç­‰ç­‰ã€‚

åœ¨torchä¸­é¢„å¤„ç†æ–‡æœ¬æ•°æ®ä¸€èˆ¬ä½¿ç”¨torchtextæˆ–è€…è‡ªå®šä¹‰Datasetï¼ŒtorchtextåŠŸèƒ½éå¸¸å¼ºå¤§ï¼Œå¯ä»¥æ„å»ºæ–‡æœ¬åˆ†ç±»ï¼Œåºåˆ—æ ‡æ³¨ï¼Œé—®ç­”æ¨¡å‹ï¼Œæœºå™¨ç¿»è¯‘ç­‰NLPä»»åŠ¡çš„æ•°æ®é›†ã€‚

ä¸‹é¢ä»…æ¼”ç¤ºä½¿ç”¨å®ƒæ¥æ„å»ºæ–‡æœ¬åˆ†ç±»æ•°æ®é›†çš„æ–¹æ³•ã€‚

è¾ƒå®Œæ•´çš„æ•™ç¨‹å¯ä»¥å‚è€ƒä»¥ä¸‹çŸ¥ä¹æ–‡ç« ï¼š[ã€Špytorchå­¦ä¹ ç¬”è®°â€”Torchtextã€‹](https://zhuanlan.zhihu.com/p/65833208)


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125626-053447.jpg)


torchtextå¸¸è§APIä¸€è§ˆï¼š

* torchtext.data.Example : ç”¨æ¥è¡¨ç¤ºä¸€ä¸ªæ ·æœ¬ï¼Œæ•°æ®å’Œæ ‡ç­¾ã€‚
* torchtext.vocab.Vocab: è¯æ±‡è¡¨ï¼Œå¯ä»¥å¯¼å…¥ä¸€äº›é¢„è®­ç»ƒè¯å‘é‡ã€‚
* torchtext.data.Datasets: æ•°æ®é›†ç±»ï¼Œ`__getitem__`è¿”å› Exampleå®ä¾‹, torchtext.data.TabularDatasetæ˜¯å…¶å­ç±»ã€‚
* torchtext.data.Field : ç”¨æ¥å®šä¹‰å­—æ®µçš„å¤„ç†æ–¹æ³•ï¼ˆæ–‡æœ¬å­—æ®µï¼Œæ ‡ç­¾å­—æ®µï¼‰åˆ›å»º Exampleæ—¶çš„ é¢„å¤„ç†ï¼Œbatch æ—¶çš„ä¸€äº›å¤„ç†æ“ä½œã€‚
* torchtext.data.Iterator: è¿­ä»£å™¨ï¼Œç”¨æ¥ç”Ÿæˆ batchã€‚
* torchtext.datasets: åŒ…å«äº†å¸¸è§çš„æ•°æ®é›†ã€‚


```python
import torch
import string, re
import torchtext

MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯
MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦
BATCH_SIZE = 20 

# åˆ†è¯æ–¹æ³•
tokenizer = lambda x:re.sub('[%s]' % string.punctuation, "", x).split(" ")

# è¿‡æ»¤æ‰ä½é¢‘è¯
def filterLowFreqWords(arr,vocab):
    arr = [[x if x<MAX_WORDS else 0 for x in example] for example in arr]
    return arr

# 1,å®šä¹‰å„ä¸ªå­—æ®µçš„é¢„å¤„ç†æ–¹æ³•
TEXT = torchtext.data.Field(sequential=True, 
                            tokenize=tokenizer, 
                            lower=True, 
                            fix_length=MAX_LEN,
                            postprocessing = filterLowFreqWords)

LABEL = torchtext.data.Field(sequential=False, use_vocab=False)

# 2,æ„å»ºè¡¨æ ¼å‹dataset
# torchtext.data.TabularDatasetå¯è¯»å–csv,tsv,jsonç­‰æ ¼å¼
ds_train, ds_test = torchtext.data.TabularDataset.splits(path='./data/imdb', 
                                                         train='train.tsv',
                                                         test='test.tsv', 
                                                         format='tsv',
                                                         fields=[('label', LABEL), 
                                                                 ('text', TEXT)],
                                                         skip_header = False)

# 3,æ„å»ºè¯å…¸
TEXT.build_vocab(ds_train)

# 4,æ„å»ºæ•°æ®ç®¡é“è¿­ä»£å™¨
train_iter, test_iter = torchtext.data.Iterator.splits((ds_train, ds_test), 
                                                       sort_within_batch=True,
                                                       sort_key=lambda x: len(x.text),
                                                       batch_sizes=(BATCH_SIZE,BATCH_SIZE))
```

```python
# æŸ¥çœ‹exampleä¿¡æ¯
print(ds_train[0].text)
print(ds_train[0].label)
```

```
['it', 'really', 'boggles', 'my', 'mind', 'when', 'someone', 'comes', 'across', 'a', 'movie', 'like', 'this', 'and', 'claims', 'it', 'to', 'be', 'one', 'of', 'the', 'worst', 'slasher', 'films', 'out', 'there', 'this', 'is', 'by', 'far', 'not', 'one', 'of', 'the', 'worst', 'out', 'there', 'still', 'not', 'a', 'good', 'movie', 'but', 'not', 'the', 'worst', 'nonetheless', 'go', 'see', 'something', 'like', 'death', 'nurse', 'or', 'blood', 'lake', 'and', 'then', 'come', 'back', 'to', 'me', 'and', 'tell', 'me', 'if', 'you', 'think', 'the', 'night', 'brings', 'charlie', 'is', 'the', 'worst', 'the', 'film', 'has', 'decent', 'camera', 'work', 'and', 'editing', 'which', 'is', 'way', 'more', 'than', 'i', 'can', 'say', 'for', 'many', 'more', 'extremely', 'obscure', 'slasher', 'filmsbr', 'br', 'the', 'film', 'doesnt', 'deliver', 'on', 'the', 'onscreen', 'deaths', 'theres', 'one', 'death', 'where', 'you', 'see', 'his', 'pruning', 'saw', 'rip', 'into', 'a', 'neck', 'but', 'all', 'other', 'deaths', 'are', 'hardly', 'interesting', 'but', 'the', 'lack', 'of', 'onscreen', 'graphic', 'violence', 'doesnt', 'mean', 'this', 'isnt', 'a', 'slasher', 'film', 'just', 'a', 'bad', 'onebr', 'br', 'the', 'film', 'was', 'obviously', 'intended', 'not', 'to', 'be', 'taken', 'too', 'seriously', 'the', 'film', 'came', 'in', 'at', 'the', 'end', 'of', 'the', 'second', 'slasher', 'cycle', 'so', 'it', 'certainly', 'was', 'a', 'reflection', 'on', 'traditional', 'slasher', 'elements', 'done', 'in', 'a', 'tongue', 'in', 'cheek', 'way', 'for', 'example', 'after', 'a', 'kill', 'charlie', 'goes', 'to', 'the', 'towns', 'welcome', 'sign', 'and', 'marks', 'the', 'population', 'down', 'one', 'less', 'this', 'is', 'something', 'that', 'can', 'only', 'get', 'a', 'laughbr', 'br', 'if', 'youre', 'into', 'slasher', 'films', 'definitely', 'give', 'this', 'film', 'a', 'watch', 'it', 'is', 'slightly', 'different', 'than', 'your', 'usual', 'slasher', 'film', 'with', 'possibility', 'of', 'two', 'killers', 'but', 'not', 'by', 'much', 'the', 'comedy', 'of', 'the', 'movie', 'is', 'pretty', 'much', 'telling', 'the', 'audience', 'to', 'relax', 'and', 'not', 'take', 'the', 'movie', 'so', 'god', 'darn', 'serious', 'you', 'may', 'forget', 'the', 'movie', 'you', 'may', 'remember', 'it', 'ill', 'remember', 'it', 'because', 'i', 'love', 'the', 'name']
0
```

```python
# æŸ¥çœ‹è¯å…¸ä¿¡æ¯
print(len(TEXT.vocab))

#itos: index to string
print(TEXT.vocab.itos[0]) 
print(TEXT.vocab.itos[1]) 

#stoi: string to index
print(TEXT.vocab.stoi['<unk>']) #unknown æœªçŸ¥è¯
print(TEXT.vocab.stoi['<pad>']) #padding  å¡«å……


#freqs: è¯é¢‘
print(TEXT.vocab.freqs['<unk>']) 
print(TEXT.vocab.freqs['a']) 
print(TEXT.vocab.freqs['good']) 
```

```
108197
<unk>
<pad>
0
1
0
129453
11457
```



```python
# æŸ¥çœ‹æ•°æ®ç®¡é“ä¿¡æ¯
# æ³¨æ„æœ‰å‘ï¼štextç¬¬0ç»´æ˜¯å¥å­é•¿åº¦
for batch in train_iter:
    features = batch.text
    labels = batch.label
    print(features)
    print(features.shape)
    print(labels)
    break
```

```
tensor([[  17,   31,  148,  ...,   54,   11,  201],
        [   2,    2,  904,  ...,  335,    7,  109],
        [1371, 1737,   44,  ...,  806,    2,   11],
        ...,
        [   6,    5,   62,  ...,    1,    1,    1],
        [ 170,    0,   27,  ...,    1,    1,    1],
        [  15,    0,   45,  ...,    1,    1,    1]])
torch.Size([200, 20])
tensor([0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0])

```

```python
# å°†æ•°æ®ç®¡é“ç»„ç»‡æˆtorch.utils.data.DataLoaderç›¸ä¼¼çš„features,labelè¾“å‡ºå½¢å¼
class DataLoader:
    def __init__(self,data_iter):
        self.data_iter = data_iter
        self.length = len(data_iter)
    
    def __len__(self):
        return self.length
    
    def __iter__(self):
        # æ³¨æ„ï¼šæ­¤å¤„è°ƒæ•´featuresä¸º batch firstï¼Œå¹¶è°ƒæ•´labelçš„shapeå’Œdtype
        for batch in self.data_iter:
            yield(torch.transpose(batch.text, 0, 1),
                  torch.unsqueeze(batch.label.float(), dim = 1))
    
dl_train = DataLoader(train_iter)
dl_test = DataLoader(test_iter)
```



### å®šä¹‰æ¨¡å‹-torchkeras.Model

ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼š

1. ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹
2. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹
3. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨(nn.Sequential,nn.ModuleList,nn.ModuleDict)è¿›è¡Œå°è£…

æ­¤å¤„é€‰æ‹©ä½¿ç”¨ç¬¬ä¸‰ç§æ–¹å¼è¿›è¡Œæ„å»ºã€‚ç”±äºæ¥ä¸‹æ¥ä½¿ç”¨ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬å°†æ¨¡å‹å°è£…æˆtorchkeras.Modelç±»æ¥è·å¾—ç±»ä¼¼Kerasä¸­é«˜é˜¶æ¨¡å‹æ¥å£çš„åŠŸèƒ½ã€‚Modelç±»å®é™…ä¸Šç»§æ‰¿è‡ªnn.Moduleç±»ã€‚

```python
import torch
from torch import nn 
import torchkeras
```

```python
torch.random.seed()
import torch
from torch import nn 

class Net(torchkeras.Model):
    
    def __init__(self):
        super(Net, self).__init__()
        
        #è®¾ç½®padding_idxå‚æ•°åå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å¡«å……çš„tokenå§‹ç»ˆèµ‹å€¼ä¸º0å‘é‡
        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,
                                      embedding_dim = 3,
                                      padding_idx = 1)
        
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1", nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))
        self.conv.add_module("pool_1", nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1", nn.ReLU())
        self.conv.add_module("conv_2", nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))
        self.conv.add_module("pool_2", nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2", nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten", nn.Flatten())
        self.dense.add_module("linear", nn.Linear(6144,1))
        self.dense.add_module("sigmoid", nn.Sigmoid())
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
        

model = Net()
print(model)

model.summary(input_shape = (200,), input_dtype = torch.LongTensor)
```

```
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten()
    (linear): Linear(in_features=6144, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Embedding-1               [-1, 200, 3]          30,000
            Conv1d-2              [-1, 16, 196]             256
         MaxPool1d-3               [-1, 16, 98]               0
              ReLU-4               [-1, 16, 98]               0
            Conv1d-5              [-1, 128, 97]           4,224
         MaxPool1d-6              [-1, 128, 48]               0
              ReLU-7              [-1, 128, 48]               0
           Flatten-8                 [-1, 6144]               0
            Linear-9                    [-1, 1]           6,145
          Sigmoid-10                    [-1, 1]               0
================================================================
Total params: 40,625
Trainable params: 40,625
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287796
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443531
----------------------------------------------------------------
```



### è®­ç»ƒæ¨¡å‹-ç±»å½¢å¼


è®­ç»ƒPytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼š

1. è„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯
2. å‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯
3. ç±»å½¢å¼è®­ç»ƒå¾ªç¯

æ­¤å¤„ä»‹ç»ä¸€ç§ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ã€‚

æˆ‘ä»¬ä»¿ç…§Keraså®šä¹‰äº†ä¸€ä¸ªé«˜é˜¶çš„æ¨¡å‹æ¥å£Model,å®ç° fit, validateï¼Œpredict, summary æ–¹æ³•ï¼Œç›¸å½“äºç”¨æˆ·è‡ªå®šä¹‰é«˜é˜¶APIã€‚


```python
# å‡†ç¡®ç‡
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,
                         torch.ones_like(y_pred,dtype = torch.float32),
                         torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

model.compile(loss_func = nn.BCELoss(),
              optimizer= torch.optim.Adagrad(model.parameters(),lr = 0.02),
              metrics_dict={"accuracy":accuracy})
```

```python
# æœ‰æ—¶å€™æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­ä¸æ”¶æ•›ï¼Œéœ€è¦å¤šè¯•å‡ æ¬¡
dfhistory = model.fit(20,
                      dl_train,
                      dl_val=dl_test,
                      log_step_freq= 200)
```

```
Start Training ...

================================================================================2020-05-09 17:53:56
{'step': 200, 'loss': 1.127, 'accuracy': 0.504}
{'step': 400, 'loss': 0.908, 'accuracy': 0.517}
{'step': 600, 'loss': 0.833, 'accuracy': 0.531}
{'step': 800, 'loss': 0.793, 'accuracy': 0.545}
{'step': 1000, 'loss': 0.765, 'accuracy': 0.56}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.765 |   0.56   |   0.64   |     0.64     |
+-------+-------+----------+----------+--------------+

================================================================================2020-05-09 17:54:23
{'step': 200, 'loss': 0.626, 'accuracy': 0.659}
{'step': 400, 'loss': 0.621, 'accuracy': 0.662}
{'step': 600, 'loss': 0.616, 'accuracy': 0.664}
{'step': 800, 'loss': 0.61, 'accuracy': 0.671}
{'step': 1000, 'loss': 0.603, 'accuracy': 0.677}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.603 |  0.677   |  0.577   |    0.705     |
+-------+-------+----------+----------+--------------+
...
...

================================================================================2020-05-09 18:03:22
{'step': 200, 'loss': 0.19, 'accuracy': 0.934}
{'step': 400, 'loss': 0.192, 'accuracy': 0.931}
{'step': 600, 'loss': 0.195, 'accuracy': 0.929}
{'step': 800, 'loss': 0.194, 'accuracy': 0.93}
{'step': 1000, 'loss': 0.191, 'accuracy': 0.931}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   20  | 0.191 |  0.931   |  0.506   |    0.795     |
+-------+-------+----------+----------+--------------+

================================================================================2020-05-09 18:03:58
Finished Training...
```



### è¯„ä¼°æ¨¡å‹

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-508370.png)

```python
plot_metric(dfhistory,"accuracy")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-504415.png)

```python
# è¯„ä¼°
model.evaluate(dl_test)
```

```
{'val_loss': 0.5056138457655907, 'val_accuracy': 0.7948000040054322}
```



### ä½¿ç”¨æ¨¡å‹

```python
model.predict(dl_test)
```

```
tensor([[3.9803e-02],
        [9.9295e-01],
        [6.0493e-01],
        ...,
        [1.2023e-01],
        [9.3701e-01],
        [2.5752e-04]])
```



### ä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨ä¿å­˜å‚æ•°æ–¹å¼ä¿å­˜Pytorchæ¨¡å‹ã€‚

```python
print(model.state_dict().keys())
```

```
odict_keys(['embedding.weight', 'conv.conv_1.weight', 'conv.conv_1.bias', 'conv.conv_2.weight', 'conv.conv_2.bias', 'dense.linear.weight', 'dense.linear.bias'])
```

```python
# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(model.state_dict(), "./data/model_parameter.pkl")

model_clone = Net()
model_clone.load_state_dict(torch.load("./data/model_parameter.pkl"))

model_clone.compile(loss_func = nn.BCELoss(),
                    optimizer= torch.optim.Adagrad(model.parameters(),lr = 0.02),
                    metrics_dict={"accuracy":accuracy})

# è¯„ä¼°æ¨¡å‹
model_clone.evaluate(dl_test)
```

```
{'val_loss': 0.5056138457655907, 'val_accuracy': 0.7948000040054322}
```



## **æ—¶é—´åºåˆ—æ•°æ®**å»ºæ¨¡æµç¨‹èŒƒä¾‹

2020å¹´å‘ç”Ÿçš„æ–°å† è‚ºç‚ç–«æƒ…ç¾éš¾ç»™å„å›½äººæ°‘çš„ç”Ÿæ´»é€ æˆäº†è¯¸å¤šæ–¹é¢çš„å½±å“ã€‚

æœ‰çš„åŒå­¦æ˜¯æ”¶å…¥ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯æ„Ÿæƒ…ä¸Šçš„ï¼Œæœ‰çš„åŒå­¦æ˜¯å¿ƒç†ä¸Šçš„ï¼Œè¿˜æœ‰çš„åŒå­¦æ˜¯ä½“é‡ä¸Šçš„ã€‚

æœ¬æ–‡åŸºäºä¸­å›½2020å¹´3æœˆä¹‹å‰çš„ç–«æƒ…æ•°æ®ï¼Œå»ºç«‹æ—¶é—´åºåˆ—RNNæ¨¡å‹ï¼Œå¯¹ä¸­å›½çš„æ–°å† è‚ºç‚ç–«æƒ…ç»“æŸæ—¶é—´è¿›è¡Œé¢„æµ‹ã€‚


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125626-058720.png)

```python
import os
import datetime
import importlib 
import torchkeras

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```



### å‡†å¤‡æ•°æ®


æœ¬æ–‡çš„æ•°æ®é›†å–è‡ªtushareï¼Œè·å–è¯¥æ•°æ®é›†çš„æ–¹æ³•å‚è€ƒäº†ä»¥ä¸‹æ–‡ç« ã€‚ã€Šhttps://zhuanlan.zhihu.com/p/109556102ã€‹

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-541325.png)


```python
import numpy as np
import pandas as pd 
import matplotlib.pyplot as plt
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

df = pd.read_csv("./data/covid-19.csv",sep = "\t")
df.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60)
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-554508.png)

```python
dfdata = df.set_index("date")
dfdiff = dfdata.diff(periods=1).dropna()
dfdiff = dfdiff.reset_index("date")

dfdiff.plot(x = "date",y = ["confirmed_num","cured_num","dead_num"],figsize=(10,6))
plt.xticks(rotation=60)
dfdiff = dfdiff.drop("date",axis = 1).astype("float32")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-545281.png)

```python
dfdiff.head()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084825-133561.png)


ä¸‹é¢æˆ‘ä»¬é€šè¿‡ç»§æ‰¿torch.utils.data.Datasetå®ç°è‡ªå®šä¹‰æ—¶é—´åºåˆ—æ•°æ®é›†ã€‚

torch.utils.data.Datasetæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œç”¨æˆ·æƒ³è¦åŠ è½½è‡ªå®šä¹‰çš„æ•°æ®åªéœ€è¦ç»§æ‰¿è¿™ä¸ªç±»ï¼Œå¹¶ä¸”è¦†å†™å…¶ä¸­çš„ä¸¤ä¸ªæ–¹æ³•å³å¯ï¼š

* `__len__`:å®ç°len(dataset)è¿”å›æ•´ä¸ªæ•°æ®é›†çš„å¤§å°ã€‚
* `__getitem__`:ç”¨æ¥è·å–ä¸€äº›ç´¢å¼•çš„æ•°æ®ï¼Œä½¿`dataset[i]`è¿”å›æ•°æ®é›†ä¸­ç¬¬iä¸ªæ ·æœ¬ã€‚

ä¸è¦†å†™è¿™ä¸¤ä¸ªæ–¹æ³•ä¼šç›´æ¥è¿”å›é”™è¯¯ã€‚


```python
import torch 
from torch import nn 
from torch.utils.data import Dataset,DataLoader,TensorDataset


#ç”¨æŸæ—¥å‰8å¤©çª—å£æ•°æ®ä½œä¸ºè¾“å…¥é¢„æµ‹è¯¥æ—¥æ•°æ®
WINDOW_SIZE = 8

class Covid19Dataset(Dataset):
        
    def __len__(self):
        return len(dfdiff) - WINDOW_SIZE
    
    def __getitem__(self,i):
        x = dfdiff.loc[i:i+WINDOW_SIZE-1,:]
        feature = torch.tensor(x.values)
        y = dfdiff.loc[i+WINDOW_SIZE,:]
        label = torch.tensor(y.values)
        return (feature,label)
    
ds_train = Covid19Dataset()

#æ•°æ®è¾ƒå°ï¼Œå¯ä»¥å°†å…¨éƒ¨è®­ç»ƒæ•°æ®æ”¾å…¥åˆ°ä¸€ä¸ªbatchä¸­ï¼Œæå‡æ€§èƒ½
dl_train = DataLoader(ds_train,batch_size = 38)
```



### å®šä¹‰æ¨¡å‹

ä½¿ç”¨Pytorché€šå¸¸æœ‰ä¸‰ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼š

1. ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹
2. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹
3. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨è¿›è¡Œå°è£…

æ­¤å¤„é€‰æ‹©ç¬¬äºŒç§æ–¹å¼æ„å»ºæ¨¡å‹ã€‚

ç”±äºæ¥ä¸‹æ¥ä½¿ç”¨ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ï¼Œæˆ‘ä»¬è¿›ä¸€æ­¥å°†æ¨¡å‹å°è£…æˆtorchkerasä¸­çš„Modelç±»æ¥è·å¾—ç±»ä¼¼Kerasä¸­é«˜é˜¶æ¨¡å‹æ¥å£çš„åŠŸèƒ½ã€‚

Modelç±»å®é™…ä¸Šç»§æ‰¿è‡ªnn.Moduleç±»ã€‚


```python
import torch
from torch import nn 
import importlib 
import torchkeras 

torch.random.seed()

class Block(nn.Module):
    def __init__(self):
        super(Block, self).__init__()
    
    def forward(self,x,x_input):
        x_out = torch.max((1+x)*x_input[:,-1,:], torch.tensor(0.0))
        return x_out
    
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        # 3å±‚lstm
        self.lstm = nn.LSTM(input_size = 3,
                            hidden_size = 3,
                            num_layers = 5,
                            batch_first = True)
        self.linear = nn.Linear(3,3)
        self.block = Block()
        
    def forward(self,x_input):
        x = self.lstm(x_input)[0][:,-1,:]
        x = self.linear(x)
        y = self.block(x,x_input)
        return y
        
net = Net()
model = torchkeras.Model(net)
print(model)

model.summary(input_shape=(8,3),input_dtype = torch.FloatTensor)
```

```
Net(
  (lstm): LSTM(3, 3, num_layers=5, batch_first=True)
  (linear): Linear(in_features=3, out_features=3, bias=True)
  (block): Block()
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
              LSTM-1                 [-1, 8, 3]             480
            Linear-2                    [-1, 3]              12
             Block-3                    [-1, 3]               0
================================================================
Total params: 492
Trainable params: 492
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000092
Forward/backward pass size (MB): 0.000229
Params size (MB): 0.001877
Estimated Total Size (MB): 0.002197
----------------------------------------------------------------
```



### è®­ç»ƒæ¨¡å‹


è®­ç»ƒPytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼šè„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œå‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯ï¼Œç±»å½¢å¼è®­ç»ƒå¾ªç¯ã€‚

æ­¤å¤„ä»‹ç»ä¸€ç§ç±»å½¢å¼çš„è®­ç»ƒå¾ªç¯ã€‚

æˆ‘ä»¬ä»¿ç…§Keraså®šä¹‰äº†ä¸€ä¸ªé«˜é˜¶çš„æ¨¡å‹æ¥å£Model,å®ç° fit, validateï¼Œpredict, summary æ–¹æ³•ï¼Œç›¸å½“äºç”¨æˆ·è‡ªå®šä¹‰é«˜é˜¶APIã€‚

æ³¨ï¼šå¾ªç¯ç¥ç»ç½‘ç»œè°ƒè¯•è¾ƒä¸ºå›°éš¾ï¼Œéœ€è¦è®¾ç½®å¤šä¸ªä¸åŒçš„å­¦ä¹ ç‡å¤šæ¬¡å°è¯•ï¼Œä»¥å–å¾—è¾ƒå¥½çš„æ•ˆæœã€‚

```python
def mspe(y_pred,y_true):
    err_percent = (y_true - y_pred)**2/(torch.max(y_true**2,torch.tensor(1e-7)))
    return torch.mean(err_percent)

model.compile(loss_func = mspe,optimizer = torch.optim.Adagrad(model.parameters(),lr = 0.1))
```

```python
dfhistory = model.fit(100,dl_train,log_step_freq=10)
```



### è¯„ä¼°æ¨¡å‹


è¯„ä¼°æ¨¡å‹ä¸€èˆ¬è¦è®¾ç½®éªŒè¯é›†æˆ–è€…æµ‹è¯•é›†ï¼Œç”±äºæ­¤ä¾‹æ•°æ®è¾ƒå°‘ï¼Œæˆ‘ä»¬ä»…ä»…å¯è§†åŒ–æŸå¤±å‡½æ•°åœ¨è®­ç»ƒé›†ä¸Šçš„è¿­ä»£æƒ…å†µã€‚

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.title('Training '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric])
    plt.show()

```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-526165.png)

### ä½¿ç”¨æ¨¡å‹


æ­¤å¤„æˆ‘ä»¬ä½¿ç”¨æ¨¡å‹é¢„æµ‹ç–«æƒ…ç»“æŸæ—¶é—´ï¼Œå³ æ–°å¢ç¡®è¯Šç—…ä¾‹ä¸º0 çš„æ—¶é—´ã€‚

```python
#ä½¿ç”¨dfresultè®°å½•ç°æœ‰æ•°æ®ä»¥åŠæ­¤åé¢„æµ‹çš„ç–«æƒ…æ•°æ®
dfresult = dfdiff[["confirmed_num","cured_num","dead_num"]].copy()
dfresult.tail()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-550553.png)

```python
#é¢„æµ‹æ­¤å200å¤©çš„æ–°å¢èµ°åŠ¿,å°†å…¶ç»“æœæ·»åŠ åˆ°dfresultä¸­
for i in range(200):
    arr_input = torch.unsqueeze(torch.from_numpy(dfresult.values[-38:,:]),axis=0)
    arr_predict = model.forward(arr_input)

    dfpredict = pd.DataFrame(torch.floor(arr_predict).data.numpy(),
                             columns = dfresult.columns)
    dfresult = dfresult.append(dfpredict,ignore_index=True)
```

```python
dfresult.query("confirmed_num==0").head()

# ç¬¬50å¤©å¼€å§‹æ–°å¢ç¡®è¯Šé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯5å¤©åï¼Œå³é¢„è®¡3æœˆ15æ—¥æ–°å¢ç¡®è¯Šé™ä¸º0
# æ³¨ï¼šè¯¥é¢„æµ‹åä¹è§‚
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-537373.png)



```python
dfresult.query("cured_num==0").head()

# ç¬¬132å¤©å¼€å§‹æ–°å¢æ²»æ„ˆé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯å¤§æ¦‚3ä¸ªæœˆåï¼Œå³6æœˆ10æ—¥å·¦å³å…¨éƒ¨æ²»æ„ˆã€‚
# æ³¨: è¯¥é¢„æµ‹åæ‚²è§‚ï¼Œå¹¶ä¸”å­˜åœ¨é—®é¢˜ï¼Œå¦‚æœå°†æ¯å¤©æ–°å¢æ²»æ„ˆäººæ•°åŠ èµ·æ¥ï¼Œå°†è¶…è¿‡ç´¯è®¡ç¡®è¯Šäººæ•°ã€‚
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-533415.png)

```python
dfresult.query("dead_num==0").head()

# ç¬¬50å¤©å¼€å§‹æ–°å¢ç¡®è¯Šé™ä¸º0ï¼Œç¬¬45å¤©å¯¹åº”3æœˆ10æ—¥ï¼Œä¹Ÿå°±æ˜¯5å¤©åï¼Œå³é¢„è®¡3æœˆ15æ—¥æ–°å¢ç¡®è¯Šé™ä¸º0
# æ³¨ï¼šè¯¥é¢„æµ‹åä¹è§‚
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-530121.png)



### ä¿å­˜æ¨¡å‹


æ¨èä½¿ç”¨ä¿å­˜å‚æ•°æ–¹å¼ä¿å­˜Pytorchæ¨¡å‹ã€‚

```python
print(model.net.state_dict().keys())
```

```python
# ä¿å­˜æ¨¡å‹å‚æ•°
torch.save(model.net.state_dict(), "./data/model_parameter.pkl")

net_clone = Net()
net_clone.load_state_dict(torch.load("./data/model_parameter.pkl"))
model_clone = torchkeras.Model(net_clone)
model_clone.compile(loss_func = mspe)

# è¯„ä¼°æ¨¡å‹
model_clone.evaluate(dl_train)
```

```
{'val_loss': 4.254558563232422}
```



# Pytorchçš„æ ¸å¿ƒæ¦‚å¿µ

Pytorchæ˜¯ä¸€ä¸ªåŸºäºPythonçš„æœºå™¨å­¦ä¹ åº“ã€‚å®ƒå¹¿æ³›åº”ç”¨äºè®¡ç®—æœºè§†è§‰ï¼Œè‡ªç„¶è¯­è¨€å¤„ç†ç­‰æ·±åº¦å­¦ä¹ é¢†åŸŸã€‚æ˜¯ç›®å‰å’ŒTensorFlowåˆ†åº­æŠ—ç¤¼çš„æ·±åº¦å­¦ä¹ æ¡†æ¶ï¼Œåœ¨å­¦æœ¯åœˆé¢‡å—æ¬¢è¿ã€‚

å®ƒä¸»è¦æä¾›äº†ä»¥ä¸‹ä¸¤ç§æ ¸å¿ƒåŠŸèƒ½ï¼š

1. æ”¯æŒGPUåŠ é€Ÿçš„**å¼ é‡è®¡ç®—**ã€‚
2. æ–¹ä¾¿ä¼˜åŒ–æ¨¡å‹çš„**è‡ªåŠ¨å¾®åˆ†æœºåˆ¶**ã€‚



Pytorchçš„ä¸»è¦ä¼˜ç‚¹ï¼š

1. **ç®€æ´æ˜“æ‡‚**ï¼šPytorchçš„APIè®¾è®¡çš„ç›¸å½“ç®€æ´ä¸€è‡´ã€‚åŸºæœ¬ä¸Šå°±æ˜¯tensor, autograd, nnä¸‰çº§å°è£…ã€‚å­¦ä¹ èµ·æ¥éå¸¸å®¹æ˜“ã€‚æœ‰ä¸€ä¸ªè¿™æ ·çš„æ®µå­ï¼Œè¯´TensorFlowçš„è®¾è®¡å“²å­¦æ˜¯ Make it complicated, Keras çš„è®¾è®¡å“²å­¦æ˜¯ Make it complicated and hide it, è€ŒPytorchçš„è®¾è®¡å“²å­¦æ˜¯ Keep it simple and stupidã€‚
2. **ä¾¿äºè°ƒè¯•**ï¼šPytorché‡‡ç”¨åŠ¨æ€å›¾ï¼Œå¯ä»¥åƒæ™®é€šPythonä»£ç ä¸€æ ·è¿›è¡Œè°ƒè¯•ã€‚ä¸åŒäºTensorFlow, Pytorchçš„æŠ¥é”™è¯´æ˜é€šå¸¸å¾ˆå®¹æ˜“çœ‹æ‡‚ã€‚æœ‰ä¸€ä¸ªè¿™æ ·çš„æ®µå­ï¼Œè¯´ä½ æ°¸è¿œä¸å¯èƒ½ä»TensorFlowçš„æŠ¥é”™è¯´æ˜ä¸­æ‰¾åˆ°å®ƒå‡ºé”™çš„åŸå› ã€‚
3. **å¼ºå¤§é«˜æ•ˆ**ï¼šPytorchæä¾›äº†éå¸¸ä¸°å¯Œçš„æ¨¡å‹ç»„ä»¶ï¼Œå¯ä»¥å¿«é€Ÿå®ç°æƒ³æ³•ã€‚å¹¶ä¸”è¿è¡Œé€Ÿåº¦å¾ˆå¿«ã€‚ç›®å‰å¤§éƒ¨åˆ†æ·±åº¦å­¦ä¹ ç›¸å…³çš„Paperéƒ½æ˜¯ç”¨Pytorchå®ç°çš„ã€‚æœ‰äº›ç ”ç©¶äººå‘˜è¡¨ç¤ºï¼Œä»ä½¿ç”¨TensorFlowè½¬æ¢ä¸ºä½¿ç”¨Pytorchä¹‹åï¼Œä»–ä»¬çš„ç¡çœ å¥½å¤šäº†ï¼Œå¤´å‘æ¯”ä»¥å‰æµ“å¯†äº†ï¼Œçš®è‚¤ä¹Ÿæ¯”ä»¥å‰å…‰æ»‘äº†ã€‚

ä¿—è¯è¯´ï¼Œä¸‡ä¸ˆé«˜æ¥¼å¹³åœ°èµ·ï¼ŒPytorchè¿™åº§å¤§å¦ä¹Ÿæœ‰å®ƒçš„åœ°åŸºã€‚Pytorchåº•å±‚æœ€æ ¸å¿ƒçš„æ¦‚å¿µæ˜¯**å¼ é‡**ï¼Œ**åŠ¨æ€è®¡ç®—å›¾**ä»¥åŠ**è‡ªåŠ¨å¾®åˆ†**ã€‚



## å¼ é‡æ•°æ®ç»“æ„

Pytorchçš„åŸºæœ¬æ•°æ®ç»“æ„æ˜¯å¼ é‡Tensorã€‚å¼ é‡å³å¤šç»´æ•°ç»„ã€‚Pytorchçš„å¼ é‡å’Œnumpyä¸­çš„arrayå¾ˆç±»ä¼¼ã€‚

æœ¬èŠ‚æˆ‘ä»¬ä¸»è¦ä»‹ç»å¼ é‡çš„æ•°æ®ç±»å‹ã€å¼ é‡çš„ç»´åº¦ã€å¼ é‡çš„å°ºå¯¸ã€å¼ é‡å’Œnumpyæ•°ç»„ç­‰åŸºæœ¬æ¦‚å¿µã€‚

### å¼ é‡çš„æ•°æ®ç±»å‹


å¼ é‡çš„æ•°æ®ç±»å‹å’Œnumpy.arrayåŸºæœ¬ä¸€ä¸€å¯¹åº”ï¼Œä½†æ˜¯ä¸æ”¯æŒstrç±»å‹ã€‚

åŒ…æ‹¬:

1. torch.float64(torch.double)
2. torch.float32(torch.float)
3. torch.float16
4. torch.int64(torch.long)
5. torch.int32(torch.int)
6. torch.int16
7. torch.int8
8. torch.uint8
9. torch.bool

ä¸€èˆ¬ç¥ç»ç½‘ç»œå»ºæ¨¡ä½¿ç”¨çš„éƒ½æ˜¯torch.float32ç±»å‹ã€‚ 

```python
import numpy as np
import torch 

# è‡ªåŠ¨æ¨æ–­æ•°æ®ç±»å‹
i = torch.tensor(1)
print(i, i.dtype)

x = torch.tensor(2.0)
print(x, x.dtype)

b = torch.tensor(True)
print(b, b.dtype)
```

```
tensor(1) torch.int64
tensor(2.) torch.float32
tensor(True) torch.bool
```



```python
# æŒ‡å®šæ•°æ®ç±»å‹
i = torch.tensor(1, dtype = torch.int32)
print(i, i.dtype)

x = torch.tensor(2.0, dtype = torch.double)
print(x, x.dtype)
```

```
tensor(1, dtype=torch.int32) torch.int32
tensor(2., dtype=torch.float64) torch.float64
```



```python
# ä½¿ç”¨ç‰¹å®šç±»å‹æ„é€ å‡½æ•°
i = torch.IntTensor(1)
print(i, i.dtype)

x = torch.Tensor(np.array(2.0))
print(x, x.dtype) #ç­‰ä»·äºtorch.FloatTensor

b = torch.BoolTensor(np.array([1,0,2,0]))
print(b, b.dtype)
```

```
tensor([5], dtype=torch.int32) torch.int32
tensor(2.) torch.float32
tensor([ True, False,  True, False]) torch.bool
```

```python
# ä¸åŒç±»å‹è¿›è¡Œè½¬æ¢
i = torch.tensor(1)
print(i, i.dtype)

# è°ƒç”¨ floatæ–¹æ³•è½¬æ¢æˆæµ®ç‚¹ç±»å‹
x = i.float()
print(x, x.dtype) 

# ä½¿ç”¨typeå‡½æ•°è½¬æ¢æˆæµ®ç‚¹ç±»å‹
y = i.type(torch.float)
print(y, y.dtype) 

#ä½¿ç”¨type_asæ–¹æ³•è½¬æ¢æˆæŸä¸ªTensorç›¸åŒç±»å‹
z = i.type_as(x)
print(z, z.dtype) 
```

```
tensor(1) torch.int64
tensor(1.) torch.float32
tensor(1.) torch.float32
tensor(1.) torch.float32
```



### å¼ é‡çš„ç»´åº¦


ä¸åŒç±»å‹çš„æ•°æ®å¯ä»¥ç”¨ä¸åŒç»´åº¦(dimension)çš„å¼ é‡æ¥è¡¨ç¤ºã€‚

1. æ ‡é‡ä¸º0ç»´å¼ é‡ï¼Œå‘é‡ä¸º1ç»´å¼ é‡ï¼ŒçŸ©é˜µä¸º2ç»´å¼ é‡ã€‚
2. å½©è‰²å›¾åƒæœ‰rgbä¸‰ä¸ªé€šé“ï¼Œå¯ä»¥è¡¨ç¤ºä¸º3ç»´å¼ é‡ã€‚
3. è§†é¢‘è¿˜æœ‰æ—¶é—´ç»´ï¼Œå¯ä»¥è¡¨ç¤ºä¸º4ç»´å¼ é‡ã€‚

å¯ä»¥ç®€å•åœ°æ€»ç»“ä¸ºï¼šæœ‰å‡ å±‚ä¸­æ‹¬å·ï¼Œå°±æ˜¯å¤šå°‘ç»´çš„å¼ é‡ã€‚

```python
scalar = torch.tensor(True)
print(scalar)
print(scalar.dim())  # æ ‡é‡ï¼Œ0ç»´å¼ é‡
```

```
tensor(True)
0
```



```python
vector = torch.tensor([1.0,2.0,3.0,4.0]) #å‘é‡ï¼Œ1ç»´å¼ é‡
print(vector)
print(vector.dim())
```

```
tensor([1., 2., 3., 4.])
1
```



```python
matrix = torch.tensor([[1.0,2.0],
                       [3.0,4.0]]) #çŸ©é˜µ, 2ç»´å¼ é‡
print(matrix)
print(matrix.dim())
```



```python
matrix = torch.tensor([[1.0,2.0],
                       [3.0,4.0]]) #çŸ©é˜µ, 2ç»´å¼ é‡
print(matrix)
print(matrix.dim())
```



```python
tensor3 = torch.tensor([[[1.0,2.0],
                         [3.0,4.0]],
                        [[5.0,6.0],
                         [7.0,8.0]]])  # 3ç»´å¼ é‡
print(tensor3)
print(tensor3.dim())
```

```
tensor([[[1., 2.],
         [3., 4.]],

        [[5., 6.],
         [7., 8.]]])
3
```



```python
tensor4 = torch.tensor([[[[1.0,1.0],[2.0,2.0]],
                         [[3.0,3.0],[4.0,4.0]]],
                        [[[5.0,5.0],[6.0,6.0]],
                         [[7.0,7.0],[8.0,8.0]]]])  # 4ç»´å¼ é‡
print(tensor4)
print(tensor4.dim())
```

```
tensor([[[[1., 1.],
          [2., 2.]],

         [[3., 3.],
          [4., 4.]]],


        [[[5., 5.],
          [6., 6.]],

         [[7., 7.],
          [8., 8.]]]])
4
```



### å¼ é‡çš„å°ºå¯¸


å¯ä»¥ä½¿ç”¨ shapeå±æ€§æˆ–è€… size()æ–¹æ³•æŸ¥çœ‹å¼ é‡åœ¨æ¯ä¸€ç»´çš„é•¿åº¦.

å¯ä»¥ä½¿ç”¨viewæ–¹æ³•æ”¹å˜å¼ é‡çš„å°ºå¯¸ã€‚

å¦‚æœviewæ–¹æ³•æ”¹å˜å°ºå¯¸å¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨reshapeæ–¹æ³•.

```python
scalar = torch.tensor(True)
print(scalar.size())
print(scalar.shape)
```

```
torch.Size([])
torch.Size([])
```



```python
vector = torch.tensor([1.0,2.0,3.0,4.0])
print(vector.size())
print(vector.shape)
```

```
torch.Size([4])
torch.Size([4])
```



```python
matrix = torch.tensor([[1.0,2.0],
                       [3.0,4.0],
                       [5.0,6.0]])
print(matrix.size())
```

```
torch.Size([3, 2])
```



```python
# ä½¿ç”¨viewå¯ä»¥æ”¹å˜å¼ é‡å°ºå¯¸
vector = torch.arange(0,12)
print(vector)
print(vector.shape)

matrix34 = vector.view(3,4)
print(matrix34)
print(matrix34.shape)

matrix43 = vector.view(4,-1) #-1è¡¨ç¤ºè¯¥ä½ç½®é•¿åº¦ç”±ç¨‹åºè‡ªåŠ¨æ¨æ–­
print(matrix43)
print(matrix43.shape)
```

```
tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])
torch.Size([12])

tensor([[ 0,  1,  2,  3],
        [ 4,  5,  6,  7],
        [ 8,  9, 10, 11]])
torch.Size([3, 4])

tensor([[ 0,  1,  2],
        [ 3,  4,  5],
        [ 6,  7,  8],
        [ 9, 10, 11]])
torch.Size([4, 3])
```

```python
# æœ‰äº›æ“ä½œä¼šè®©å¼ é‡å­˜å‚¨ç»“æ„æ‰­æ›²ï¼Œç›´æ¥ä½¿ç”¨viewä¼šå¤±è´¥ï¼Œå¯ä»¥ç”¨reshapeæ–¹æ³•

matrix26 = torch.arange(0,12).view(2,6)
print(matrix26)
print(matrix26.shape)

# è½¬ç½®æ“ä½œè®©å¼ é‡å­˜å‚¨ç»“æ„æ‰­æ›²
matrix62 = matrix26.t()
print(matrix62.is_contiguous())

# ç›´æ¥ä½¿ç”¨viewæ–¹æ³•ä¼šå¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨reshapeæ–¹æ³•
#matrix34 = matrix62.view(3,4) #error!
matrix34 = matrix62.reshape(3,4) #ç­‰ä»·äºmatrix34 = matrix62.contiguous().view(3,4)
print(matrix34)
```

```
tensor([[ 0,  1,  2,  3,  4,  5],
        [ 6,  7,  8,  9, 10, 11]])
torch.Size([2, 6])
False

tensor([[ 0,  6,  1,  7],
        [ 2,  8,  3,  9],
        [ 4, 10,  5, 11]])
```



### å¼ é‡å’Œnumpyæ•°ç»„


å¯ä»¥ç”¨numpyæ–¹æ³•ä»Tensorå¾—åˆ°numpyæ•°ç»„ï¼Œä¹Ÿå¯ä»¥ç”¨torch.from_numpyä»numpyæ•°ç»„å¾—åˆ°Tensorã€‚

è¿™ä¸¤ç§æ–¹æ³•å…³è”çš„Tensorå’Œnumpyæ•°ç»„æ˜¯å…±äº«æ•°æ®å†…å­˜çš„ã€‚

å¦‚æœæ”¹å˜å…¶ä¸­ä¸€ä¸ªï¼Œå¦å¤–ä¸€ä¸ªçš„å€¼ä¹Ÿä¼šå‘ç”Ÿæ”¹å˜ã€‚

å¦‚æœæœ‰éœ€è¦ï¼Œå¯ä»¥ç”¨å¼ é‡çš„cloneæ–¹æ³•æ‹·è´å¼ é‡ï¼Œä¸­æ–­è¿™ç§å…³è”ã€‚

æ­¤å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨itemæ–¹æ³•ä»**æ ‡é‡å¼ é‡**å¾—åˆ°å¯¹åº”çš„Pythonæ•°å€¼ã€‚ä½¿ç”¨tolistæ–¹æ³•ä»å¼ é‡å¾—åˆ°å¯¹åº”çš„Pythonæ•°å€¼åˆ—è¡¨ã€‚


```python
import numpy as np
import torch 
```

```python
#torch.from_numpyå‡½æ•°ä»numpyæ•°ç»„å¾—åˆ°Tensor
arr = np.zeros(3)
tensor = torch.from_numpy(arr)
print("before add 1:")
print(arr)
print(tensor)

print("\nafter add 1:")
np.add(arr,1, out = arr) #ç»™ arrå¢åŠ 1ï¼Œtensorä¹Ÿéšä¹‹æ”¹å˜
print(arr)
print(tensor)
```

```
before add 1:
[0. 0. 0.]
tensor([0., 0., 0.], dtype=torch.float64)

after add 1:
[1. 1. 1.]
tensor([1., 1., 1.], dtype=torch.float64)
```



```python
# numpyæ–¹æ³•ä»Tensorå¾—åˆ°numpyæ•°ç»„

tensor = torch.zeros(3)
arr = tensor.numpy()
print("before add 1:")
print(tensor)
print(arr)

print("\nafter add 1:")

#ä½¿ç”¨å¸¦ä¸‹åˆ’çº¿çš„æ–¹æ³•è¡¨ç¤ºè®¡ç®—ç»“æœä¼šè¿”å›ç»™è°ƒç”¨ å¼ é‡
tensor.add_(1) #ç»™ tensorå¢åŠ 1ï¼Œarrä¹Ÿéšä¹‹æ”¹å˜ 
#æˆ–ï¼š torch.add(tensor,1,out = tensor)
print(tensor)
print(arr)
```

```
before add 1:
tensor([0., 0., 0.])
[0. 0. 0.]

after add 1:
tensor([1., 1., 1.])
[1. 1. 1.]
```



```python
# å¯ä»¥ç”¨clone() æ–¹æ³•æ‹·è´å¼ é‡ï¼Œä¸­æ–­è¿™ç§å…³è”
tensor = torch.zeros(3)

#ä½¿ç”¨cloneæ–¹æ³•æ‹·è´å¼ é‡, æ‹·è´åçš„å¼ é‡å’ŒåŸå§‹å¼ é‡å†…å­˜ç‹¬ç«‹
arr = tensor.clone().numpy() # ä¹Ÿå¯ä»¥ä½¿ç”¨tensor.data.numpy()
print("before add 1:")
print(tensor)
print(arr)

print("\nafter add 1:")

#ä½¿ç”¨ å¸¦ä¸‹åˆ’çº¿çš„æ–¹æ³•è¡¨ç¤ºè®¡ç®—ç»“æœä¼šè¿”å›ç»™è°ƒç”¨ å¼ é‡
tensor.add_(1) #ç»™ tensorå¢åŠ 1ï¼Œarrä¸å†éšä¹‹æ”¹å˜
print(tensor)
print(arr)
```

```
before add 1:
tensor([0., 0., 0.])
[0. 0. 0.]

after add 1:
tensor([1., 1., 1.])
[0. 0. 0.]
```

```python
# itemæ–¹æ³•å’Œtolistæ–¹æ³•å¯ä»¥å°†å¼ é‡è½¬æ¢æˆPythonæ•°å€¼å’Œæ•°å€¼åˆ—è¡¨
scalar = torch.tensor(1.0)
s = scalar.item()
print(s)
print(type(s))

tensor = torch.rand(2,2)
t = tensor.tolist()
print(t)
print(type(t))
```

```
1.0
<class 'float'>

[[0.8211846351623535, 0.20020723342895508], [0.011571824550628662, 0.2906131148338318]]
<class 'list'>
```



## è‡ªåŠ¨å¾®åˆ†æœºåˆ¶


ç¥ç»ç½‘ç»œé€šå¸¸ä¾èµ–åå‘ä¼ æ’­æ±‚æ¢¯åº¦æ¥æ›´æ–°ç½‘ç»œå‚æ•°ï¼Œæ±‚æ¢¯åº¦è¿‡ç¨‹é€šå¸¸æ˜¯ä¸€ä»¶éå¸¸å¤æ‚è€Œå®¹æ˜“å‡ºé”™çš„äº‹æƒ…ã€‚è€Œæ·±åº¦å­¦ä¹ æ¡†æ¶å¯ä»¥å¸®åŠ©æˆ‘ä»¬è‡ªåŠ¨åœ°å®Œæˆè¿™ç§æ±‚æ¢¯åº¦è¿ç®—ã€‚

Pytorchä¸€èˆ¬é€šè¿‡åå‘ä¼ æ’­ **backward æ–¹æ³•** å®ç°è¿™ç§æ±‚æ¢¯åº¦è®¡ç®—ã€‚è¯¥æ–¹æ³•æ±‚å¾—çš„æ¢¯åº¦å°†å­˜åœ¨å¯¹åº”è‡ªå˜é‡å¼ é‡çš„gradå±æ€§ä¸‹ã€‚

é™¤æ­¤ä¹‹å¤–ï¼Œä¹Ÿèƒ½å¤Ÿ **è°ƒç”¨torch.autograd.grad** å‡½æ•°æ¥å®ç°æ±‚æ¢¯åº¦è®¡ç®—ã€‚

è¿™å°±æ˜¯Pytorchçš„è‡ªåŠ¨å¾®åˆ†æœºåˆ¶ã€‚

### åˆ©ç”¨backwardæ–¹æ³•æ±‚å¯¼æ•°


backward æ–¹æ³•é€šå¸¸åœ¨ä¸€ä¸ªæ ‡é‡å¼ é‡ä¸Šè°ƒç”¨ï¼Œè¯¥æ–¹æ³•æ±‚å¾—çš„æ¢¯åº¦å°†å­˜åœ¨å¯¹åº”è‡ªå˜é‡å¼ é‡çš„gradå±æ€§ä¸‹ã€‚

å¦‚æœè°ƒç”¨çš„å¼ é‡éæ ‡é‡ï¼Œåˆ™è¦ä¼ å…¥ä¸€ä¸ªå’Œå®ƒåŒå½¢çŠ¶ çš„gradientå‚æ•°å¼ é‡ã€‚ç›¸å½“äºç”¨è¯¥gradientå‚æ•°å¼ é‡ä¸è°ƒç”¨å¼ é‡ä½œå‘é‡ç‚¹ä¹˜ï¼Œå¾—åˆ°çš„æ ‡é‡ç»“æœå†åå‘ä¼ æ’­ã€‚

**1. æ ‡é‡çš„åå‘ä¼ æ’­**

```python
import numpy as np 
import torch 

# f(x) = a*x**2 + b*x + cçš„å¯¼æ•°

x = torch.tensor(0.0, requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c 

y.backward()
dy_dx = x.grad
print(dy_dx)
```

```
tensor(-2.)
```



**2. éæ ‡é‡çš„åå‘ä¼ æ’­**

```python
import numpy as np 
import torch 

# f(x) = a*x**2 + b*x + c

x = torch.tensor([[0.0,0.0],
                  [1.0,2.0]],
                 requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c 

gradient = torch.tensor([[1.0,1.0],
                         [1.0,1.0]])

print("x:\n",x)
print("y:\n",y)
y.backward(gradient = gradient)
x_grad = x.grad
print("x_grad:\n",x_grad)
```

```
x:
 tensor([[0., 0.],
        [1., 2.]], requires_grad=True)
y:
 tensor([[1., 1.],
        [0., 1.]], grad_fn=<AddBackward0>)
x_grad:
 tensor([[-2., -2.],
        [ 0.,  2.]])
```



**3. éæ ‡é‡çš„åå‘ä¼ æ’­å¯ä»¥ç”¨æ ‡é‡çš„åå‘ä¼ æ’­å®ç°**

```python
import numpy as np 
import torch 

# f(x) = a*x**2 + b*x + c

x = torch.tensor([[0.0,0.0],
                  [1.0,2.0]],
                 requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c 

gradient = torch.tensor([[1.0,1.0],
                         [1.0,1.0]])
z = torch.sum(y*gradient)

print("x:",x)
print("y:",y)
z.backward()
x_grad = x.grad
print("x_grad:\n",x_grad)
```

```
x: tensor([[0., 0.],
        [1., 2.]], requires_grad=True)
y: tensor([[1., 1.],
        [0., 1.]], grad_fn=<AddBackward0>)
x_grad:
 tensor([[-2., -2.],
        [ 0.,  2.]])
```



### åˆ©ç”¨autograd.gradæ–¹æ³•æ±‚å¯¼æ•°

```python
import numpy as np 
import torch 

# f(x) = a*x**2 + b*x + cçš„å¯¼æ•°

x = torch.tensor(0.0, requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)
y = a*torch.pow(x,2) + b*x + c


# create_graph è®¾ç½®ä¸º True å°†å…è®¸åˆ›å»ºæ›´é«˜é˜¶çš„å¯¼æ•° 
dy_dx = torch.autograd.grad(y, x, create_graph=True)[0]
print(dy_dx.data)

# æ±‚äºŒé˜¶å¯¼æ•°
dy2_dx2 = torch.autograd.grad(dy_dx, x)[0] 
print(dy2_dx2.data)
```

```
tensor(-2.)
tensor(2.)
```



```python
import numpy as np 
import torch 

x1 = torch.tensor(1.0, requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
x2 = torch.tensor(2.0, requires_grad = True)

y1 = x1*x2
y2 = x1+x2

# å…è®¸åŒæ—¶å¯¹å¤šä¸ªè‡ªå˜é‡æ±‚å¯¼æ•°
(dy1_dx1, dy1_dx2) = torch.autograd.grad(outputs=y1,inputs = [x1,x2],retain_graph = True)
print(dy1_dx1, dy1_dx2)

# å¦‚æœæœ‰å¤šä¸ªå› å˜é‡ï¼Œç›¸å½“äºæŠŠå¤šä¸ªå› å˜é‡çš„æ¢¯åº¦ç»“æœæ±‚å’Œ
(dy12_dx1, dy12_dx2) = torch.autograd.grad(outputs=[y1,y2],inputs = [x1,x2])
print(dy12_dx1, dy12_dx2)
```

```
tensor(2.) tensor(1.)
tensor(3.) tensor(2.)
```



### åˆ©ç”¨è‡ªåŠ¨å¾®åˆ†å’Œä¼˜åŒ–å™¨æ±‚æœ€å°å€¼

```python
import numpy as np 
import torch 

# f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼

x = torch.tensor(0.0, requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)

optimizer = torch.optim.SGD(params=[x], lr = 0.01)


def f(x):
    result = a*torch.pow(x,2) + b*x + c 
    return (result)

for i in range(500):
    optimizer.zero_grad()
    y = f(x)
    y.backward()
    optimizer.step()

print("y=",f(x).data,";","x=",x.data)
```

```
y= tensor(0.) ; x= tensor(1.0000)
```



## åŠ¨æ€è®¡ç®—å›¾


æœ¬èŠ‚æˆ‘ä»¬å°†ä»‹ç» Pytorchçš„åŠ¨æ€è®¡ç®—å›¾ã€‚åŒ…æ‹¬ï¼š 

1. åŠ¨æ€è®¡ç®—å›¾ç®€ä»‹
2. è®¡ç®—å›¾ä¸­çš„Function
3. è®¡ç®—å›¾å’Œåå‘ä¼ æ’­
4. å¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹
5. è®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–



### åŠ¨æ€è®¡ç®—å›¾ç®€ä»‹


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125626-049492.gif)


Pytorchçš„è®¡ç®—å›¾ç”±**èŠ‚ç‚¹**å’Œ**è¾¹**ç»„æˆï¼ŒèŠ‚ç‚¹è¡¨ç¤ºå¼ é‡æˆ–è€…Functionï¼Œè¾¹è¡¨ç¤ºå¼ é‡å’ŒFunctionä¹‹é—´çš„ä¾èµ–å…³ç³»ã€‚

Pytorchä¸­çš„è®¡ç®—å›¾æ˜¯åŠ¨æ€å›¾ã€‚è¿™é‡Œçš„åŠ¨æ€ä¸»è¦æœ‰ä¸¤é‡å«ä¹‰ã€‚

1. ç¬¬ä¸€å±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯**ç«‹å³æ‰§è¡Œ**çš„ã€‚æ— éœ€ç­‰å¾…å®Œæ•´çš„è®¡ç®—å›¾åˆ›å»ºå®Œæ¯•ï¼Œæ¯æ¡è¯­å¥éƒ½ä¼šåœ¨è®¡ç®—å›¾ä¸­åŠ¨æ€æ·»åŠ èŠ‚ç‚¹å’Œè¾¹ï¼Œå¹¶ç«‹å³æ‰§è¡Œæ­£å‘ä¼ æ’­å¾—åˆ°è®¡ç®—ç»“æœã€‚
2. ç¬¬äºŒå±‚å«ä¹‰æ˜¯ï¼šè®¡ç®—å›¾åœ¨åå‘ä¼ æ’­å**ç«‹å³é”€æ¯**ã€‚ä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°æ„å»ºè®¡ç®—å›¾ã€‚å¦‚æœåœ¨ç¨‹åºä¸­ä½¿ç”¨äº†backwardæ–¹æ³•æ‰§è¡Œäº†åå‘ä¼ æ’­ï¼Œæˆ–è€…åˆ©ç”¨torch.autograd.gradæ–¹æ³•è®¡ç®—äº†æ¢¯åº¦ï¼Œé‚£ä¹ˆåˆ›å»ºçš„è®¡ç®—å›¾ä¼šè¢«ç«‹å³é”€æ¯ï¼Œé‡Šæ”¾å­˜å‚¨ç©ºé—´ï¼Œä¸‹æ¬¡è°ƒç”¨éœ€è¦é‡æ–°åˆ›å»ºã€‚



**1. è®¡ç®—å›¾çš„æ­£å‘ä¼ æ’­æ˜¯ç«‹å³æ‰§è¡Œçš„ã€‚**

```python
import torch 
w = torch.tensor([[3.0,
                   1.0]],
                 requires_grad=True)
b = torch.tensor([[3.0]],
                 requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y, 2))

print(loss.data)
print(Y_hat.data)
```

```
tensor(17.8969)
tensor([[3.2613],
        [4.7322],
        [4.5037],
        [7.5899],
        [7.0973],
        [1.3287],
        [6.1473],
        [1.3492],
        [1.3911],
        [1.2150]])
```



**2. è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ã€‚**

```python
import torch 
w = torch.tensor([[3.0,1.0]],
                 requires_grad=True)
b = torch.tensor([[3.0]],
                 requires_grad=True)
X = torch.randn(10,2)
Y = torch.randn(10,1)
Y_hat = X@w.t() + b  # Y_hatå®šä¹‰åå…¶æ­£å‘ä¼ æ’­è¢«ç«‹å³æ‰§è¡Œï¼Œä¸å…¶åé¢çš„lossåˆ›å»ºè¯­å¥æ— å…³
loss = torch.mean(torch.pow(Y_hat-Y,2))

#è®¡ç®—å›¾åœ¨åå‘ä¼ æ’­åç«‹å³é”€æ¯ï¼Œå¦‚æœéœ€è¦ä¿ç•™è®¡ç®—å›¾, éœ€è¦è®¾ç½®retain_graph = True
loss.backward()  #loss.backward(retain_graph = True) 

#loss.backward() #å¦‚æœå†æ¬¡æ‰§è¡Œåå‘ä¼ æ’­å°†æŠ¥é”™
```



### è®¡ç®—å›¾ä¸­çš„Function


è®¡ç®—å›¾ä¸­çš„ å¼ é‡æˆ‘ä»¬å·²ç»æ¯”è¾ƒç†Ÿæ‚‰äº†, è®¡ç®—å›¾ä¸­çš„å¦å¤–ä¸€ç§èŠ‚ç‚¹æ˜¯Function, å®é™…ä¸Šå°±æ˜¯ Pytorchä¸­å„ç§å¯¹å¼ é‡æ“ä½œçš„å‡½æ•°ã€‚

è¿™äº›Functionå’Œæˆ‘ä»¬Pythonä¸­çš„å‡½æ•°æœ‰ä¸€ä¸ªè¾ƒå¤§çš„åŒºåˆ«ï¼Œé‚£å°±æ˜¯å®ƒ**åŒæ—¶åŒ…æ‹¬æ­£å‘è®¡ç®—é€»è¾‘å’Œåå‘ä¼ æ’­çš„é€»è¾‘**ã€‚

æˆ‘ä»¬å¯ä»¥é€šè¿‡ç»§æ‰¿torch.autograd.Functionæ¥åˆ›å»ºè¿™ç§æ”¯æŒåå‘ä¼ æ’­çš„Functionã€‚


```python
class MyReLU(torch.autograd.Function):

    #æ­£å‘ä¼ æ’­é€»è¾‘ï¼Œå¯ä»¥ç”¨ctxå­˜å‚¨ä¸€äº›å€¼ï¼Œä¾›åå‘ä¼ æ’­ä½¿ç”¨ã€‚
    @staticmethod
    def forward(ctx, input):
        ctx.save_for_backward(input)
        return input.clamp(min=0)

    #åå‘ä¼ æ’­é€»è¾‘
    @staticmethod
    def backward(ctx, grad_output):
        input, = ctx.saved_tensors
        grad_input = grad_output.clone()
        grad_input[input < 0] = 0
        return grad_input
```

```python
import torch 
w = torch.tensor([[3.0,1.0]],requires_grad=True)
b = torch.tensor([[3.0]],requires_grad=True)
X = torch.tensor([[-1.0,-1.0],[1.0,1.0]])
Y = torch.tensor([[2.0,3.0]])

relu = MyReLU.apply # reluç°åœ¨ä¹Ÿå¯ä»¥å…·æœ‰æ­£å‘ä¼ æ’­å’Œåå‘ä¼ æ’­åŠŸèƒ½
Y_hat = relu(X@w.t() + b)
loss = torch.mean(torch.pow(Y_hat-Y,2))

loss.backward()

print(w.grad)
print(b.grad)
```

```
tensor([[4.5000, 4.5000]])
tensor([[4.5000]])
```



```python
# Y_hatçš„æ¢¯åº¦å‡½æ•°å³æ˜¯æˆ‘ä»¬è‡ªå·±æ‰€å®šä¹‰çš„ MyReLU.backward
print(Y_hat.grad_fn)
```

```
<torch.autograd.function.MyReLUBackward object at 0x1205a46c8>
```



### è®¡ç®—å›¾ä¸åå‘ä¼ æ’­


äº†è§£äº†Functionçš„åŠŸèƒ½ï¼Œæˆ‘ä»¬å¯ä»¥ç®€å•åœ°ç†è§£ä¸€ä¸‹åå‘ä¼ æ’­çš„åŸç†å’Œè¿‡ç¨‹ã€‚ç†è§£è¯¥éƒ¨åˆ†åŸç†éœ€è¦ä¸€äº›é«˜ç­‰æ•°å­¦ä¸­æ±‚å¯¼é“¾å¼æ³•åˆ™çš„åŸºç¡€çŸ¥è¯†ã€‚


```python
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()
```

loss.backward()è¯­å¥è°ƒç”¨åï¼Œä¾æ¬¡å‘ç”Ÿä»¥ä¸‹è®¡ç®—è¿‡ç¨‹ã€‚

1. lossè‡ªå·±çš„gradæ¢¯åº¦èµ‹å€¼ä¸º1ï¼Œå³å¯¹è‡ªèº«çš„æ¢¯åº¦ä¸º1ã€‚
2. lossæ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•ï¼Œè®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡å³y1å’Œy2çš„æ¢¯åº¦ï¼Œå°†è¯¥å€¼èµ‹å€¼åˆ°y1.gradå’Œy2.gradã€‚
3. y2å’Œy1æ ¹æ®å…¶è‡ªèº«æ¢¯åº¦ä»¥åŠå…³è”çš„backwardæ–¹æ³•, åˆ†åˆ«è®¡ç®—å‡ºå…¶å¯¹åº”çš„è‡ªå˜é‡xçš„æ¢¯åº¦ï¼Œx.gradå°†å…¶æ”¶åˆ°çš„å¤šä¸ªæ¢¯åº¦å€¼ç´¯åŠ ã€‚

ï¼ˆæ³¨æ„ï¼Œ1,2,3æ­¥éª¤çš„æ±‚æ¢¯åº¦é¡ºåºå’Œå¯¹å¤šä¸ªæ¢¯åº¦å€¼çš„ç´¯åŠ è§„åˆ™æ°å¥½æ˜¯æ±‚å¯¼é“¾å¼æ³•åˆ™çš„ç¨‹åºè¡¨è¿°ï¼‰

æ­£å› ä¸ºæ±‚å¯¼é“¾å¼æ³•åˆ™è¡ç”Ÿçš„æ¢¯åº¦ç´¯åŠ è§„åˆ™ï¼Œ**å¼ é‡çš„gradæ¢¯åº¦ä¸ä¼šè‡ªåŠ¨æ¸…é›¶ï¼Œåœ¨éœ€è¦çš„æ—¶å€™éœ€è¦æ‰‹åŠ¨ç½®é›¶**ã€‚



### å¶å­èŠ‚ç‚¹å’Œéå¶å­èŠ‚ç‚¹


æ‰§è¡Œä¸‹é¢ä»£ç ï¼Œæˆ‘ä»¬ä¼šå‘ç° loss.gradå¹¶ä¸æ˜¯æˆ‘ä»¬æœŸæœ›çš„1, è€Œæ˜¯ Noneã€‚

ç±»ä¼¼åœ° y1.grad ä»¥åŠ y2.gradä¹Ÿæ˜¯ Noneã€‚

è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢ï¼Ÿè¿™æ˜¯ç”±äºå®ƒä»¬ä¸æ˜¯å¶å­èŠ‚ç‚¹å¼ é‡ã€‚

åœ¨åå‘ä¼ æ’­è¿‡ç¨‹ä¸­ï¼Œåªæœ‰ is_leaf=True çš„å¶å­èŠ‚ç‚¹ï¼Œéœ€è¦æ±‚å¯¼çš„å¼ é‡çš„å¯¼æ•°ç»“æœæ‰ä¼šè¢«æœ€åä¿ç•™ä¸‹æ¥ã€‚

é‚£ä¹ˆä»€ä¹ˆæ˜¯å¶å­èŠ‚ç‚¹å¼ é‡å‘¢ï¼Ÿå¶å­èŠ‚ç‚¹å¼ é‡éœ€è¦æ»¡è¶³ä¸¤ä¸ªæ¡ä»¶ã€‚

1. å¶å­èŠ‚ç‚¹å¼ é‡æ˜¯ç”±ç”¨æˆ·ç›´æ¥åˆ›å»ºçš„å¼ é‡ï¼Œè€Œéç”±æŸä¸ªFunctioné€šè¿‡è®¡ç®—å¾—åˆ°çš„å¼ é‡ã€‚
2. å¶å­èŠ‚ç‚¹å¼ é‡çš„requires_gradå±æ€§å¿…é¡»ä¸ºTrueã€‚

Pytorchè®¾è®¡è¿™æ ·çš„è§„åˆ™ä¸»è¦æ˜¯ä¸ºäº†èŠ‚çº¦å†…å­˜æˆ–è€…æ˜¾å­˜ç©ºé—´ï¼Œå› ä¸ºå‡ ä¹æ‰€æœ‰çš„æ—¶å€™ï¼Œç”¨æˆ·åªä¼šå…³å¿ƒä»–è‡ªå·±ç›´æ¥åˆ›å»ºçš„å¼ é‡çš„æ¢¯åº¦ã€‚

æ‰€æœ‰ä¾èµ–äºå¶å­èŠ‚ç‚¹å¼ é‡çš„å¼ é‡, å…¶requires_grad å±æ€§å¿…å®šæ˜¯Trueçš„ï¼Œä½†å…¶æ¢¯åº¦å€¼åªåœ¨è®¡ç®—è¿‡ç¨‹ä¸­è¢«ç”¨åˆ°ï¼Œä¸ä¼šæœ€ç»ˆå­˜å‚¨åˆ°gradå±æ€§ä¸­ã€‚

å¦‚æœéœ€è¦ä¿ç•™ä¸­é—´è®¡ç®—ç»“æœçš„æ¢¯åº¦åˆ°gradå±æ€§ä¸­ï¼Œå¯ä»¥ä½¿ç”¨ retain_gradæ–¹æ³•ã€‚

å¦‚æœä»…ä»…æ˜¯ä¸ºäº†è°ƒè¯•ä»£ç æŸ¥çœ‹æ¢¯åº¦å€¼ï¼Œå¯ä»¥åˆ©ç”¨register_hookæ‰“å°æ—¥å¿—ã€‚


```python
import torch 

x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

loss.backward()
print("loss.grad:", loss.grad)
print("y1.grad:", y1.grad)
print("y2.grad:", y2.grad)
print(x.grad)
```

```
loss.grad: None
y1.grad: None
y2.grad: None
tensor(4.)
```

```python
print(x.is_leaf)
print(y1.is_leaf)
print(y2.is_leaf)
print(loss.is_leaf)
```

```
True
False
False
False
```



åˆ©ç”¨retain_gradå¯ä»¥ä¿ç•™éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ï¼Œåˆ©ç”¨register_hookå¯ä»¥æŸ¥çœ‹éå¶å­èŠ‚ç‚¹çš„æ¢¯åº¦å€¼ã€‚

```python
import torch 

#æ­£å‘ä¼ æ’­
x = torch.tensor(3.0,requires_grad=True)
y1 = x + 1
y2 = 2*x
loss = (y1-y2)**2

#éå¶å­èŠ‚ç‚¹æ¢¯åº¦æ˜¾ç¤ºæ§åˆ¶
y1.register_hook(lambda grad: print('y1 grad: ', grad))
y2.register_hook(lambda grad: print('y2 grad: ', grad))
loss.retain_grad()

#åå‘ä¼ æ’­
loss.backward()
print("loss.grad:", loss.grad)
print("x.grad:", x.grad)
```

```
y2 grad:  tensor(4.)
y1 grad:  tensor(-4.)

loss.grad: tensor(1.)
x.grad: tensor(4.)
```



### è®¡ç®—å›¾åœ¨TensorBoardä¸­çš„å¯è§†åŒ–


å¯ä»¥åˆ©ç”¨ torch.utils.tensorboard å°†è®¡ç®—å›¾å¯¼å‡ºåˆ° TensorBoardè¿›è¡Œå¯è§†åŒ–ã€‚

```python
from torch import nn 

class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.w = nn.Parameter(torch.randn(2,1))
        self.b = nn.Parameter(torch.zeros(1,1))

    def forward(self, x):
        y = x@self.w + self.b
        return y

net = Net()
```

```python
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('./data/tensorboard')
writer.add_graph(net, input_to_model = torch.rand(10,2))
writer.close()
```

```python
%load_ext tensorboard
#%tensorboard --logdir ./data/tensorboard
```

```python
from tensorboard import notebook

notebook.list() 
```

```python
#åœ¨tensorboardä¸­æŸ¥çœ‹æ¨¡å‹
notebook.start("--logdir ./data/tensorboard")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-557804.png)



# Pytorchçš„å±‚æ¬¡ç»“æ„

æœ¬ç« æˆ‘ä»¬ä»‹ç»Pytorchä¸­5ä¸ªä¸åŒçš„å±‚æ¬¡ç»“æ„ï¼š**å³ç¡¬ä»¶å±‚ï¼Œå†…æ ¸å±‚ï¼Œä½é˜¶APIï¼Œä¸­é˜¶APIï¼Œé«˜é˜¶APIã€torchkerasã€‘**ã€‚

Pytorchçš„å±‚æ¬¡ç»“æ„ä»ä½åˆ°é«˜å¯ä»¥åˆ†æˆå¦‚ä¸‹äº”å±‚ã€‚

1. æœ€åº•å±‚ä¸ºç¡¬ä»¶å±‚ï¼ŒPytorchæ”¯æŒCPUã€GPUåŠ å…¥è®¡ç®—èµ„æºæ± ã€‚
2. ç¬¬äºŒå±‚ä¸ºC++å®ç°çš„å†…æ ¸ã€‚
3. ç¬¬ä¸‰å±‚ä¸ºPythonå®ç°çš„æ“ä½œç¬¦ï¼Œæä¾›äº†å°è£…C++å†…æ ¸çš„ä½çº§APIæŒ‡ä»¤ï¼Œä¸»è¦åŒ…æ‹¬å„ç§å¼ é‡æ“ä½œç®—å­ã€è‡ªåŠ¨å¾®åˆ†ã€å˜é‡ç®¡ç†ã€‚å¦‚torch.tensor, torch.cat, torch.autograd.grad, nn.Moduleã€‚å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬ä¸‰å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚
4. ç¬¬å››å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹ç»„ä»¶ï¼Œå¯¹ä½çº§APIè¿›è¡Œäº†å‡½æ•°å°è£…ï¼Œä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ç­‰ç­‰ã€‚å¦‚torch.nn.Linear,torch.nn.BCE,torch.optim.Adam,torch.utils.data.DataLoaderã€‚å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬å››å±‚APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚
5. ç¬¬äº”å±‚ä¸ºPythonå®ç°çš„æ¨¡å‹æ¥å£ã€‚Pytorchæ²¡æœ‰å®˜æ–¹çš„é«˜é˜¶APIã€‚ä¸ºäº†ä¾¿äºè®­ç»ƒæ¨¡å‹ï¼Œä½œè€…ä»¿ç…§kerasä¸­çš„æ¨¡å‹æ¥å£ï¼Œä½¿ç”¨äº†ä¸åˆ°300è¡Œä»£ç ï¼Œå°è£…äº†Pytorchçš„é«˜é˜¶æ¨¡å‹æ¥å£torchkeras.Modelã€‚å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆç¬¬äº”å±‚APIå°±æ˜¯æ¨¡å‹æœ¬èº«ï¼Œå³ã€æ¨¡å‹ä¹‹å±‹ã€‘ã€‚

## ä½é˜¶APIç¤ºèŒƒ

ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨Pytorchçš„ä½é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹å’ŒDNNäºŒåˆ†ç±»æ¨¡å‹ã€‚

ä½é˜¶APIä¸»è¦åŒ…æ‹¬å¼ é‡æ“ä½œï¼Œè®¡ç®—å›¾å’Œè‡ªåŠ¨å¾®åˆ†ã€‚

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```

### çº¿æ€§å›å½’æ¨¡å‹

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt 
import torch
from torch import nn


#æ ·æœ¬æ•°é‡
n = 400

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = 10*torch.rand([n,2])-5.0  # torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0],[-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0 + b0 + torch.normal(0.0, 2.0, size=[n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
# æ•°æ®å¯è§†åŒ–
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0].numpy(),Y[:,0].numpy(), c = "b",label = "samples")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)

ax2 = plt.subplot(122)
ax2.scatter(X[:,1].numpy(),Y[:,0].numpy(), c = "g",label = "samples")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-570983.png)

```python
# æ„å»ºæ•°æ®ç®¡é“è¿­ä»£å™¨
def data_iter(features, labels, batch_size=8):
    num_examples = len(features)
    indices = list(range(num_examples))
    np.random.shuffle(indices)  #æ ·æœ¬çš„è¯»å–é¡ºåºæ˜¯éšæœºçš„
    for i in range(0, num_examples, batch_size):
        indexs = torch.LongTensor(indices[i: min(i + batch_size, num_examples)])
        yield features.index_select(0, indexs), labels.index_select(0, indexs)
        
# æµ‹è¯•æ•°æ®ç®¡é“æ•ˆæœ   
batch_size = 8
(features,labels) = next(data_iter(X, Y, batch_size))
print(features)
print(labels)
```

```
tensor([[-4.3880,  1.3655],
        [-0.1082,  3.9533],
        [-2.6286,  2.7058],
        [ 1.0604, -1.8646],
        [-1.5805,  1.5406],
        [-2.6217, -3.2342],
        [ 2.3748, -0.6449],
        [-1.2478, -2.0509]])
        
tensor([[-0.2069],
        [-3.2494],
        [-6.9620],
        [17.0528],
        [ 1.1076],
        [17.2117],
        [16.1081],
        [14.7092]])
```



**2. å®šä¹‰æ¨¡å‹**

```python
# å®šä¹‰æ¨¡å‹
class LinearRegression: 
    
    def __init__(self):
        self.w = torch.randn_like(w0, requires_grad=True)
        self.b = torch.zeros_like(b0, requires_grad=True)
        
    #æ­£å‘ä¼ æ’­
    def forward(self,x): 
        return x@self.w + self.b

    # æŸå¤±å‡½æ•°
    def loss_func(self,y_pred,y_true):  
        return torch.mean((y_pred - y_true)**2/2)

model = LinearRegression()
```



**3. è®­ç»ƒæ¨¡å‹**

```python
def train_step(model, features, labels):
    predictions = model.forward(features)
    loss = model.loss_func(predictions,labels)
        
    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    
    # ä½¿ç”¨torch.no_grad()é¿å…æ¢¯åº¦è®°å½•ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æ“ä½œ model.w.data å®ç°é¿å…æ¢¯åº¦è®°å½• 
    with torch.no_grad():
        # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°
        model.w -= 0.001*model.w.grad
        model.b -= 0.001*model.b.grad

        # æ¢¯åº¦æ¸…é›¶
        model.w.grad.zero_()
        model.b.grad.zero_()
    return loss
```

```python
# æµ‹è¯•train_stepæ•ˆæœ
batch_size = 10
(features,labels) = next(data_iter(X, Y, batch_size))
train_step(model, features, labels)
```

```
tensor(92.8199, grad_fn=<MeanBackward0>)
```

```python
def train_model(model,epochs):
    for epoch in range(1, epochs+1):
        for features, labels in data_iter(X,Y,10):
            loss = train_step(model,features,labels)

        if epoch % 200==0:
            printbar()
            print("epoch =", epoch, "loss = ", loss.item())
            print("model.w =", model.w.data)
            print("model.b =", model.b.data)

train_model(model,epochs = 1000)
```

```
=====================================2020-07-05 08:27:57
epoch = 200 loss =  2.6340413093566895
model.w = tensor([[ 2.0283],
        [-2.9632]])
model.b = tensor([[10.0748]])

======================================2020-07-05 08:28:00
epoch = 400 loss =  2.24908709526062
model.w = tensor([[ 2.0300],
        [-2.9643]])
model.b = tensor([[10.0781]])

=======================================2020-07-05 08:28:04
epoch = 600 loss =  1.510349154472351
model.w = tensor([[ 2.0290],
        [-2.9630]])
model.b = tensor([[10.0781]])

=======================================2020-07-05 08:28:07
epoch = 800 loss =  1.038671851158142
model.w = tensor([[ 2.0314],
        [-2.9649]])
model.b = tensor([[10.0785]])

=======================================2020-07-05 08:28:10
epoch = 1000 loss =  1.9742190837860107
model.w = tensor([[ 2.0313],
        [-2.9648]])
model.b = tensor([[10.0781]])
```



```python
# ç»“æœå¯è§†åŒ–
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0].numpy(),Y[:,0].numpy(), c = "b",label = "samples")
ax1.plot(X[:,0].numpy(),(model.w[0].data*X[:,0]+model.b[0].data).numpy(),"-r",linewidth = 5.0,label = "model")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)


ax2 = plt.subplot(122)
ax2.scatter(X[:,1].numpy(),Y[:,0].numpy(), c = "g",label = "samples")
ax2.plot(X[:,1].numpy(),(model.w[1].data*X[:,1]+model.b[0].data).numpy(),"-r",linewidth = 5.0,label = "model")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)

plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-574939.png)



### DNNäºŒåˆ†ç±»æ¨¡å‹

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn

%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 2000,2000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)

#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0].numpy(),Xp[:,1].numpy(),c = "r")
plt.scatter(Xn[:,0].numpy(),Xn[:,1].numpy(),c = "g")
plt.legend(["positive","negative"]);
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-561757.png)

```python
# æ„å»ºæ•°æ®ç®¡é“è¿­ä»£å™¨
def data_iter(features, labels, batch_size=8):
    num_examples = len(features)
    indices = list(range(num_examples))
    np.random.shuffle(indices)  #æ ·æœ¬çš„è¯»å–é¡ºåºæ˜¯éšæœºçš„
    for i in range(0, num_examples, batch_size):
        indexs = torch.LongTensor(indices[i: min(i + batch_size, num_examples)])
        yield  features.index_select(0, indexs), labels.index_select(0, indexs)
        
# æµ‹è¯•æ•°æ®ç®¡é“æ•ˆæœ   
batch_size = 8
(features,labels) = next(data_iter(X,Y,batch_size))
print(features)
print(labels)
```

```
tensor([[ 6.9914, -1.0820],
        [ 4.8156,  4.0532],
        [-1.0697, -7.4644],
        [ 2.6291,  3.8851],
        [-1.6780, -4.3390],
        [-6.1495,  1.2269],
        [-4.3422,  3.9552],
        [-6.2265,  2.6159]])
tensor([[0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [1.],
        [1.]])
```



**2. å®šä¹‰æ¨¡å‹**


æ­¤å¤„èŒƒä¾‹æˆ‘ä»¬åˆ©ç”¨nn.Moduleæ¥ç»„ç»‡æ¨¡å‹å˜é‡ã€‚

```python
class DNNModel(nn.Module):
    def __init__(self):
        super(DNNModel, self).__init__()
        self.w1 = nn.Parameter(torch.randn(2,4))
        self.b1 = nn.Parameter(torch.zeros(1,4))
        self.w2 = nn.Parameter(torch.randn(4,8))
        self.b2 = nn.Parameter(torch.zeros(1,8))
        self.w3 = nn.Parameter(torch.randn(8,1))
        self.b3 = nn.Parameter(torch.zeros(1,1))

    # æ­£å‘ä¼ æ’­
    def forward(self,x):
        x = torch.relu(x@self.w1 + self.b1)
        x = torch.relu(x@self.w2 + self.b2)
        y = torch.sigmoid(x@self.w3 + self.b3)
        return y
    
    # æŸå¤±å‡½æ•°(äºŒå…ƒäº¤å‰ç†µ)
    def loss_func(self,y_pred,y_true):  
        #å°†é¢„æµ‹å€¼é™åˆ¶åœ¨1e-7ä»¥ä¸Š, 1- (1e-7)ä»¥ä¸‹ï¼Œé¿å…log(0)é”™è¯¯
        eps = 1e-7
        y_pred = torch.clamp(y_pred, eps, 1.0-eps)
        bce = - y_true*torch.log(y_pred) - (1-y_true)*torch.log(1-y_pred)
        return torch.mean(bce)
    
    # è¯„ä¼°æŒ‡æ ‡(å‡†ç¡®ç‡)
    def metric_func(self,y_pred,y_true):
        y_pred = torch.where(y_pred>0.5,
                             torch.ones_like(y_pred, dtype = torch.float32),
                             torch.zeros_like(y_pred, dtype = torch.float32))
        acc = torch.mean(1-torch.abs(y_true-y_pred))
        return acc
    
model = DNNModel()
```

```python
# æµ‹è¯•æ¨¡å‹ç»“æ„
batch_size = 10
(features,labels) = next(data_iter(X, Y, batch_size))

predictions = model(features)
loss = model.loss_func(labels, predictions)
metric = model.metric_func(labels, predictions)

print("init loss:", loss.item())
print("init metric:", metric.item())
```

```
init loss: 7.979694366455078
init metric: 0.50347900390625
```

```python
len(list(model.parameters()))
```

```
6
```



**3. è®­ç»ƒæ¨¡å‹**

```python
def train_step(model, features, labels):   
    
    # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
    predictions = model.forward(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)
        
    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    
    # æ¢¯åº¦ä¸‹é™æ³•æ›´æ–°å‚æ•°
    for param in model.parameters():
        #æ³¨æ„æ˜¯å¯¹param.dataè¿›è¡Œé‡æ–°èµ‹å€¼,é¿å…æ­¤å¤„æ“ä½œå¼•èµ·æ¢¯åº¦è®°å½•
        param.data = (param.data - 0.01*param.grad.data) 
        
    # æ¢¯åº¦æ¸…é›¶
    model.zero_grad()
    return loss.item(), metric.item()
 

def train_model(model,epochs):
    
    for epoch in range(1,epochs+1):
        loss_list, metric_list = [], []
        
        for features, labels in data_iter(X,Y,20):
            lossi,metrici = train_step(model,features,labels)
            loss_list.append(lossi)
            metric_list.append(metrici)
            
        loss = np.mean(loss_list)
        metric = np.mean(metric_list)

        if epoch % 100==0:
            printbar()
            print("epoch =",epoch,"loss = ",loss,"metric = ",metric)
        
train_model(model,epochs = 1000)
```

```
================================================================================2020-07-05 08:32:16
epoch = 100 loss =  0.24841043589636683 metric =  0.8944999960064888

================================================================================2020-07-05 08:32:34
epoch = 200 loss =  0.20398724960163236 metric =  0.920999992787838

================================================================================2020-07-05 08:32:54
epoch = 300 loss =  0.19509393003769218 metric =  0.9239999914169311

================================================================================2020-07-05 08:33:14
epoch = 400 loss =  0.19067603485658766 metric =  0.9272499939799309

================================================================================2020-07-05 08:33:33
epoch = 500 loss =  0.1898010154720396 metric =  0.9237499925494194

================================================================================2020-07-05 08:33:54
epoch = 600 loss =  0.19151576517149807 metric =  0.9254999926686287

================================================================================2020-07-05 08:34:18
epoch = 700 loss =  0.18914461021777243 metric =  0.9274999949336052

================================================================================2020-07-05 08:34:39
epoch = 800 loss =  0.18801998342387377 metric =  0.9264999932050705

================================================================================2020-07-05 08:35:00
epoch = 900 loss =  0.1852504052128643 metric =  0.9249999937415123

================================================================================2020-07-05 08:35:21
epoch = 1000 loss =  0.18695520935580134 metric =  0.9272499927878379
```

```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-566376.png)



## ä¸­é˜¶APIç¤ºèŒƒ

ä¸‹é¢çš„èŒƒä¾‹ä½¿ç”¨Pytorchçš„ä¸­é˜¶APIå®ç°çº¿æ€§å›å½’æ¨¡å‹å’Œå’ŒDNNäºŒåˆ†ç±»æ¨¡å‹ã€‚

Pytorchçš„ä¸­é˜¶APIä¸»è¦åŒ…æ‹¬å„ç§æ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°ï¼Œä¼˜åŒ–å™¨ï¼Œæ•°æ®ç®¡é“ç­‰ç­‰ã€‚

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```



### çº¿æ€§å›å½’æ¨¡å‹

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt 
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader, TensorDataset

# æ ·æœ¬æ•°é‡
n = 400

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = 10*torch.rand([n,2])-5.0  # torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0],[-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0 + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
# æ•°æ®å¯è§†åŒ–

%matplotlib inline
%config InlineBackend.figure_format = 'svg'

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)

ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-590756.png)

```python
#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(X,Y)
dl = DataLoader(ds, 
                batch_size = 10, 
                shuffle=True, 
                num_workers=2)
```



**2. å®šä¹‰æ¨¡å‹**

```python
model = nn.Linear(2,1) #çº¿æ€§å±‚

model.loss_func = nn.MSELoss()
model.optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)
```



**3. è®­ç»ƒæ¨¡å‹**

```python
def train_step(model, features, labels):
    
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    loss.backward()
    model.optimizer.step()
    model.optimizer.zero_grad()
    return loss.item()

# æµ‹è¯•train_stepæ•ˆæœ
features,labels = next(iter(dl))
train_step(model,features,labels)
```

```
269.98016357421875
```



```python
def train_model(model, epochs):
    for epoch in range(1, epochs + 1):
        for features, labels in dl:
            loss = train_step(model,features,labels)
            
        if epoch % 50 == 0:
            printbar()
            w = model.state_dict()["weight"]
            b = model.state_dict()["bias"]
            print("epoch =",epoch,"loss = ",loss)
            print("w =",w)
            print("b =",b)
train_model(model,epochs = 200)
```

```
================================================================================2020-07-05 22:51:53
epoch = 50 loss =  3.0177409648895264
w = tensor([[ 1.9315, -2.9573]])
b = tensor([9.9625])

================================================================================2020-07-05 22:51:57
epoch = 100 loss =  2.1144354343414307
w = tensor([[ 1.9760, -2.9398]])
b = tensor([9.9428])

================================================================================2020-07-05 22:52:01
epoch = 150 loss =  3.290461778640747
w = tensor([[ 2.1075, -2.9509]])
b = tensor([9.9599])

================================================================================2020-07-05 22:52:06
epoch = 200 loss =  3.047853469848633
w = tensor([[ 2.1134, -2.9306]])
b = tensor([9.9722])
```



```python
# ç»“æœå¯è§†åŒ–

%matplotlib inline
%config InlineBackend.figure_format = 'svg'

w, b = model.state_dict()["weight"], model.state_dict()["bias"]

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.plot(X[:,0], w[0,0]*X[:,0]+b[0], "-r", linewidth = 5.0, label = "model")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)

ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.plot(X[:,1],w[0,1]*X[:,1]+b[0],"-r",linewidth = 5.0,label = "model")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)

plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-586802.png)



### DNNäºŒåˆ†ç±»æ¨¡å‹

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 2000,2000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)

#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-578894.png)

```python
# æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(X, Y)
dl = DataLoader(ds, batch_size = 10, shuffle=True, num_workers=2)
```



**2. å®šä¹‰æ¨¡å‹**

```python
class DNNModel(nn.Module):
    def __init__(self):
        super(DNNModel, self).__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)

    # æ­£å‘ä¼ æ’­
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = nn.Sigmoid()(self.fc3(x))
        return y
    
    # æŸå¤±å‡½æ•°
    def loss_func(self,y_pred,y_true):
        return nn.BCELoss()(y_pred,y_true)
    
    # è¯„ä¼°å‡½æ•°(å‡†ç¡®ç‡)
    def metric_func(self,y_pred,y_true):
        y_pred = torch.where(y_pred>0.5,
                             torch.ones_like(y_pred,dtype = torch.float32),
                             torch.zeros_like(y_pred,dtype = torch.float32))
        acc = torch.mean(1-torch.abs(y_true-y_pred))
        return acc
    
    # ä¼˜åŒ–å™¨
    @property
    def optimizer(self):
        return torch.optim.Adam(self.parameters(), lr = 0.001)
    
model = DNNModel()
```

```python
# æµ‹è¯•æ¨¡å‹ç»“æ„
(features,labels) = next(iter(dl))
predictions = model(features)

loss = model.loss_func(predictions,labels)
metric = model.metric_func(predictions,labels)

print("init loss:", loss.item())
print("init metric:", metric.item())
```

```
init loss: 0.7065666913986206
init metric: 0.6000000238418579
```



**3. è®­ç»ƒæ¨¡å‹**

```python
def train_step(model, features, labels):
    
    # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)
    
    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    
    # æ›´æ–°æ¨¡å‹å‚æ•°
    model.optimizer.step()
    model.optimizer.zero_grad()
    
    return loss.item(), metric.item()

# æµ‹è¯•train_stepæ•ˆæœ
features,labels = next(iter(dl))
train_step(model,features,labels)
```

```
(0.6048880815505981, 0.699999988079071)
```



```python
def train_model(model,epochs):
    for epoch in range(1,epochs+1):
        loss_list,metric_list = [],[]
        
        for features, labels in dl:
            lossi,metrici = train_step(model,features,labels)
            loss_list.append(lossi)
            metric_list.append(metrici)
            
        loss = np.mean(loss_list)
        metric = np.mean(metric_list)

        if epoch%100==0:
            printbar()
            print("epoch =",epoch,"loss = ",loss,"metric = ",metric)
        
train_model(model,epochs = 300)
```

```
================================================================================2020-07-05 22:56:38
epoch = 100 loss =  0.23532892110607917 metric =  0.934749992787838

================================================================================2020-07-05 22:58:18
epoch = 200 loss =  0.24743918558603128 metric =  0.934999993443489

================================================================================2020-07-05 22:59:56
epoch = 300 loss =  0.2936080049697884 metric =  0.931499992609024
```



```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-582848.png)



## é«˜é˜¶APIç¤ºèŒƒ

Pytorchæ²¡æœ‰å®˜æ–¹çš„é«˜é˜¶APIï¼Œä¸€èˆ¬éœ€è¦ç”¨æˆ·è‡ªå·±å®ç°è®­ç»ƒå¾ªç¯ã€éªŒè¯å¾ªç¯ã€å’Œé¢„æµ‹å¾ªç¯ã€‚

ä½œè€…é€šè¿‡ä»¿ç…§tf.keras.Modelçš„åŠŸèƒ½å¯¹Pytorchçš„nn.Moduleè¿›è¡Œäº†å°è£…ï¼Œ

å®ç°äº† fit, validateï¼Œpredict, summary æ–¹æ³•ï¼Œç›¸å½“äºç”¨æˆ·è‡ªå®šä¹‰é«˜é˜¶APIã€‚

å¹¶åœ¨å…¶åŸºç¡€ä¸Šå®ç°çº¿æ€§å›å½’æ¨¡å‹å’ŒDNNäºŒåˆ†ç±»æ¨¡å‹ã€‚


```python
import os
import datetime
from torchkeras import Model, summary

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```



### çº¿æ€§å›å½’æ¨¡å‹


æ­¤èŒƒä¾‹æˆ‘ä»¬é€šè¿‡ç»§æ‰¿ä¸Šè¿°ç”¨æˆ·è‡ªå®šä¹‰ Modelæ¨¡å‹æ¥å£ï¼Œå®ç°çº¿æ€§å›å½’æ¨¡å‹ã€‚

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd
from matplotlib import pyplot as plt 
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset

#æ ·æœ¬æ•°é‡
n = 400

# ç”Ÿæˆæµ‹è¯•ç”¨æ•°æ®é›†
X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0],[-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0 + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
# æ•°æ®å¯è§†åŒ–
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)

ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-628986.png)

```python
#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(X,Y)
ds_train,ds_valid = torch.utils.data.random_split(ds,[int(400*0.7),400-int(400*0.7)])
dl_train = DataLoader(ds_train,batch_size = 10,shuffle=True,num_workers=2)
dl_valid = DataLoader(ds_valid,batch_size = 10,num_workers=2)
```



**2. å®šä¹‰æ¨¡å‹**

```python
# ç»§æ‰¿ç”¨æˆ·è‡ªå®šä¹‰æ¨¡å‹
from torchkeras import Model
class LinearRegression(Model):
    def __init__(self):
        super(LinearRegression, self).__init__()
        self.fc = nn.Linear(2,1)
    
    def forward(self,x):
        return self.fc(x)

model = LinearRegression()
```

```python
model.summary(input_shape = (2,))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                    [-1, 1]               3
================================================================
Total params: 3
Trainable params: 3
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000008
Forward/backward pass size (MB): 0.000008
Params size (MB): 0.000011
Estimated Total Size (MB): 0.000027
----------------------------------------------------------------
```



**3. è®­ç»ƒæ¨¡å‹**

```python
### ä½¿ç”¨fitæ–¹æ³•è¿›è¡Œè®­ç»ƒ

def mean_absolute_error(y_pred,y_true):
    return torch.mean(torch.abs(y_pred-y_true))

def mean_absolute_percent_error(y_pred,y_true):
    absolute_percent_error = (torch.abs(y_pred-y_true)+1e-7)/(torch.abs(y_true)+1e-7)
    return torch.mean(absolute_percent_error)

model.compile(loss_func = nn.MSELoss(),
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.01),
              metrics_dict={"mae":mean_absolute_error,
                            "mape":mean_absolute_percent_error})

dfhistory = model.fit(200,dl_train = dl_train, dl_val = dl_valid,log_step_freq = 20)
```

```
Start Training ...

================================================================================2020-07-05 23:07:25
{'step': 20, 'loss': 226.768, 'mae': 12.198, 'mape': 1.212}

 +-------+---------+-------+-------+----------+---------+----------+
| epoch |   loss  |  mae  |  mape | val_loss | val_mae | val_mape |
+-------+---------+-------+-------+----------+---------+----------+
|   1   | 230.773 | 12.41 | 1.394 | 223.262  |  12.582 |  1.095   |
+-------+---------+-------+-------+----------+---------+----------+

================================================================================2020-07-05 23:07:26
{'step': 20, 'loss': 200.964, 'mae': 11.584, 'mape': 1.382}

 +-------+---------+--------+------+----------+---------+----------+
| epoch |   loss  |  mae   | mape | val_loss | val_mae | val_mape |
+-------+---------+--------+------+----------+---------+----------+
|   2   | 206.238 | 11.759 | 1.26 | 199.669  |  11.895 |  1.012   |
+-------+---------+--------+------+----------+---------+----------+

================================================================================2020-07-05 23:07:26
{'step': 20, 'loss': 188.247, 'mae': 11.387, 'mape': 1.172}

 +-------+---------+--------+-------+----------+---------+----------+
| epoch |   loss  |  mae   |  mape | val_loss | val_mae | val_mape |
+-------+---------+--------+-------+----------+---------+----------+
|   3   | 185.185 | 11.177 | 1.189 | 178.112  |  11.24  |  0.952   |
+-------+---------+--------+-------+----------+---------+----------+
================================================================================2020-07-05 23:07:59
{'step': 20, 'loss': 4.14, 'mae': 1.677, 'mape': 1.845}

 +-------+-------+-------+-------+----------+---------+----------+
| epoch |  loss |  mae  |  mape | val_loss | val_mae | val_mape |
+-------+-------+-------+-------+----------+---------+----------+
|  199  | 4.335 | 1.707 | 1.441 |  3.741   |  1.533  |  0.359   |
+-------+-------+-------+-------+----------+---------+----------+

================================================================================2020-07-05 23:07:59
{'step': 20, 'loss': 4.653, 'mae': 1.775, 'mape': 0.679}

 +-------+------+-------+-------+----------+---------+----------+
| epoch | loss |  mae  |  mape | val_loss | val_mae | val_mape |
+-------+------+-------+-------+----------+---------+----------+
|  200  | 4.37 | 1.718 | 1.394 |  3.749   |  1.534  |  0.363   |
+-------+------+-------+-------+----------+---------+----------+

================================================================================2020-07-05 23:07:59
Finished Training...
```

```python
# ç»“æœå¯è§†åŒ–
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

w,b = model.state_dict()["fc.weight"],model.state_dict()["fc.bias"]

plt.figure(figsize = (12,5))
ax1 = plt.subplot(121)
ax1.scatter(X[:,0],Y[:,0], c = "b",label = "samples")
ax1.plot(X[:,0],w[0,0]*X[:,0]+b[0],"-r",linewidth = 5.0,label = "model")
ax1.legend()
plt.xlabel("x1")
plt.ylabel("y",rotation = 0)


ax2 = plt.subplot(122)
ax2.scatter(X[:,1],Y[:,0], c = "g",label = "samples")
ax2.plot(X[:,1],w[0,1]*X[:,1]+b[0],"-r",linewidth = 5.0,label = "model")
ax2.legend()
plt.xlabel("x2")
plt.ylabel("y",rotation = 0)

plt.show()
```



**4. è¯„ä¼°æ¨¡å‹**

```python
dfhistory.tail()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084825-413042.png)

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-605917.png)

```python
plot_metric(dfhistory,"mape")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-609213.png)

```python
# è¯„ä¼°
model.evaluate(dl_valid)
```

```
{'val_loss': 3.749117374420166,
 'val_mae': 1.5336137612660725,
 'val_mape': 0.36319838215907413}
```



**5. ä½¿ç”¨æ¨¡å‹**

```python
# é¢„æµ‹
dl = DataLoader(TensorDataset(X))
model.predict(dl)[0:10]
```

```
tensor([[ 3.9212],
        [ 8.6336],
        [ 6.1982],
        [ 6.1212],
        [-5.0974],
        [-6.3183],
        [ 4.6588],
        [ 5.5349],
        [11.9106],
        [24.6937]])
```

```python
# é¢„æµ‹
model.predict(dl_valid)[0:10]
```

```
tensor([[ 2.8368],
        [16.2797],
        [ 2.3135],
        [ 9.5395],
        [16.4363],
        [10.0742],
        [15.0864],
        [12.9775],
        [21.8568],
        [21.8226]])
```



### DNNäºŒåˆ†ç±»æ¨¡å‹


æ­¤èŒƒä¾‹æˆ‘ä»¬é€šè¿‡ç»§æ‰¿ä¸Šè¿°ç”¨æˆ·è‡ªå®šä¹‰ Modelæ¨¡å‹æ¥å£ï¼Œå®ç°DNNäºŒåˆ†ç±»æ¨¡å‹ã€‚

**1. å‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras 
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 2000,2000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p),r_p*torch.sin(theta_p)],axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-621076.png)

```python
ds = TensorDataset(X,Y)

ds_train, ds_valid = torch.utils.data.random_split(ds, 
                                                   [int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_valid = DataLoader(ds_valid,batch_size = 100,num_workers=2)
```



**2. å®šä¹‰æ¨¡å‹**

```python
class Net(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = nn.Sigmoid()(self.fc3(x))
        return y
        
model = torchkeras.Model(Net())
model.summary(input_shape =(2,))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                    [-1, 4]              12
            Linear-2                    [-1, 8]              40
            Linear-3                    [-1, 1]               9
================================================================
Total params: 61
Trainable params: 61
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000008
Forward/backward pass size (MB): 0.000099
Params size (MB): 0.000233
Estimated Total Size (MB): 0.000340
----------------------------------------------------------------
```



**3. è®­ç»ƒæ¨¡å‹**

```python
# å‡†ç¡®ç‡
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,
                         torch.ones_like(y_pred,dtype = torch.float32),
                         torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

model.compile(loss_func = nn.BCELoss(),
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.01),
              metrics_dict={"accuracy":accuracy})

dfhistory = model.fit(100, dl_train = dl_train, dl_val = dl_valid, log_step_freq = 10)
```

```
Start Training ...

================================================================================2020-07-05 23:12:51
{'step': 10, 'loss': 0.733, 'accuracy': 0.487}
{'step': 20, 'loss': 0.713, 'accuracy': 0.515}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.704 |  0.539   |  0.676   |    0.666     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-05 23:12:51
{'step': 10, 'loss': 0.67, 'accuracy': 0.703}
{'step': 20, 'loss': 0.66, 'accuracy': 0.697}

 +-------+------+----------+----------+--------------+
| epoch | loss | accuracy | val_loss | val_accuracy |
+-------+------+----------+----------+--------------+
|   2   | 0.65 |  0.702   |  0.625   |    0.651     |
+-------+------+----------+----------+--------------+
================================================================================2020-07-05 23:13:10
{'step': 10, 'loss': 0.17, 'accuracy': 0.929}
{'step': 20, 'loss': 0.173, 'accuracy': 0.929}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   98  | 0.175 |  0.929   |  0.169   |    0.933     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-05 23:13:10
{'step': 10, 'loss': 0.165, 'accuracy': 0.942}
{'step': 20, 'loss': 0.171, 'accuracy': 0.932}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   99  | 0.173 |  0.931   |  0.166   |    0.935     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-05 23:13:10
{'step': 10, 'loss': 0.156, 'accuracy': 0.945}
{'step': 20, 'loss': 0.17, 'accuracy': 0.935}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|  100  | 0.168 |  0.937   |  0.173   |    0.926     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-05 23:13:11
Finished Training...
```

```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-625692.png)



**4. è¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-617122.png)

```python
plot_metric(dfhistory,"accuracy")
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-613169.png)

```python
model.evaluate(dl_valid)
```

```
{'val_loss': 0.17309962399303913, 'val_accuracy': 0.9258333394924799}
```



**5. ä½¿ç”¨æ¨¡å‹**

```python
model.predict(dl_valid)[0:10]
```

```
tensor([[0.9998],
        [0.0459],
        [0.0349],
        [0.0147],
        [0.9990],
        [0.9995],
        [0.8535],
        [0.0373],
        [0.2134],
        [0.9356]])
```



# Pytorchçš„ä½é˜¶API

Pytorchçš„ä½é˜¶APIä¸»è¦åŒ…æ‹¬**å¼ é‡æ“ä½œ**ï¼Œ**åŠ¨æ€è®¡ç®—å›¾**å’Œ**è‡ªåŠ¨å¾®åˆ†**ã€‚å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä½é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹ç –ã€‘ã€‚åœ¨ä½é˜¶APIå±‚æ¬¡ä¸Šï¼Œå¯ä»¥æŠŠPytorchå½“åšä¸€ä¸ªå¢å¼ºç‰ˆçš„numpyæ¥ä½¿ç”¨ã€‚Pytorchæä¾›çš„æ–¹æ³•æ¯”numpyæ›´å…¨é¢ï¼Œè¿ç®—é€Ÿåº¦æ›´å¿«ï¼Œå¦‚æœéœ€è¦çš„è¯ï¼Œè¿˜å¯ä»¥ä½¿ç”¨GPUè¿›è¡ŒåŠ é€Ÿã€‚

å‰é¢å‡ ç« æˆ‘ä»¬å¯¹ä½é˜¶APIå·²ç»æœ‰äº†ä¸€ä¸ªæ•´ä½“çš„è®¤è¯†ï¼Œæœ¬ç« æˆ‘ä»¬å°†é‡ç‚¹è¯¦ç»†ä»‹ç»å¼ é‡æ“ä½œå’ŒåŠ¨æ€è®¡ç®—å›¾ã€‚


å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

- å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚
- å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

åŠ¨æ€è®¡ç®—å›¾æˆ‘ä»¬å°†ä¸»è¦ä»‹ç»åŠ¨æ€è®¡ç®—å›¾çš„ç‰¹æ€§ï¼Œè®¡ç®—å›¾ä¸­çš„Functionï¼Œè®¡ç®—å›¾ä¸åå‘ä¼ æ’­ã€‚



## å¼ é‡çš„ç»“æ„æ“ä½œ

å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

1. å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚
2. å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„ç»“æ„æ“ä½œã€‚

### åˆ›å»ºå¼ é‡


å¼ é‡åˆ›å»ºçš„è®¸å¤šæ–¹æ³•å’Œnumpyä¸­åˆ›å»ºarrayçš„æ–¹æ³•å¾ˆåƒã€‚

```python
import numpy as np
import torch 
```

```python
a = torch.tensor([1,2,3], dtype = torch.float)
print(a)
```

```
tensor([1., 2., 3.])
```



```python
b = torch.arange(1,10,step = 2)
print(b)
```

```
tensor([1, 3, 5, 7, 9])
```



```python
c = torch.linspace(0.0, 2*3.14, 10)
print(c)
```

```
tensor([0.0000, 0.6978, 1.3956, 2.0933, 2.7911, 3.4889, 4.1867, 4.8844, 5.5822,
        6.2800])
```



```python
d = torch.zeros((3,3))
print(d)
```

```
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
```



```python
a = torch.ones((3,3), dtype = torch.int)
b = torch.zeros_like(a, dtype = torch.float)
print(a)
print(b)
```

```
tensor([[1, 1, 1],
        [1, 1, 1],
        [1, 1, 1]], dtype=torch.int32)
tensor([[0., 0., 0.],
        [0., 0., 0.],
        [0., 0., 0.]])
```



```python
torch.fill_(b,5)
print(b)
```

```
tensor([[5., 5., 5.],
        [5., 5., 5.],
        [5., 5., 5.]])
```



```python
# å‡åŒ€éšæœºåˆ†å¸ƒ
torch.manual_seed(0)
minval,maxval = 0,10
a = minval + (maxval-minval)*torch.rand([5])
print(a)
```

```
tensor([4.9626, 7.6822, 0.8848, 1.3203, 3.0742])
```



```python
#æ­£æ€åˆ†å¸ƒéšæœº
b = torch.normal(mean = torch.zeros(3,3), 
                 std = torch.ones(3,3))
print(b)
```

```
tensor([[-1.3836,  0.2459, -0.1312],
        [-0.1785, -0.5959,  0.2739],
        [ 0.5679, -0.6731, -1.2095]])
```



```python
#æ­£æ€åˆ†å¸ƒéšæœº
mean,std = 2, 5
c = std*torch.randn((3,3)) + mean
print(c)
```

```
tensor([[  8.7204,  13.9161,  -0.8323],
        [ -3.7681, -10.5115,   6.3778],
        [-11.3628,   1.8433,   4.4939]])
```



```python
#æ•´æ•°éšæœºæ’åˆ—
d = torch.randperm(20)
print(d)
```

```
tensor([ 5, 15, 19, 10,  7, 17,  0,  4, 12, 16, 14, 13,  1,  3,  9,  6, 18,  2,
         8, 11])
```



```python
#ç‰¹æ®ŠçŸ©é˜µ
I = torch.eye(3,3) #å•ä½çŸ©é˜µ
print(I)

t = torch.diag(torch.tensor([1,2,3])) #å¯¹è§’çŸ©é˜µ
print(t)
```

```
tensor([[1., 0., 0.],
        [0., 1., 0.],
        [0., 0., 1.]])
tensor([[1, 0, 0],
        [0, 2, 0],
        [0, 0, 3]])
```



### ç´¢å¼•åˆ‡ç‰‡


å¼ é‡çš„ç´¢å¼•åˆ‡ç‰‡æ–¹å¼å’Œnumpyå‡ ä¹æ˜¯ä¸€æ ·çš„ã€‚åˆ‡ç‰‡æ—¶æ”¯æŒç¼ºçœå‚æ•°å’Œçœç•¥å·ã€‚

å¯ä»¥é€šè¿‡ç´¢å¼•å’Œåˆ‡ç‰‡å¯¹éƒ¨åˆ†å…ƒç´ è¿›è¡Œä¿®æ”¹ã€‚

æ­¤å¤–ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨torch.index_select, torch.masked_select, torch.take

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„æŸäº›å…ƒç´ å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨torch.where, torch.masked_fill, torch.index_fill

```python
#å‡åŒ€éšæœºåˆ†å¸ƒ
torch.manual_seed(0)
minval, maxval = 0, 10
t = torch.floor(minval + (maxval-minval)*torch.rand([5,5])).int()
print(t)
```

```
tensor([[4, 7, 0, 1, 3],
        [6, 4, 8, 4, 6],
        [3, 4, 0, 1, 2],
        [5, 6, 8, 1, 2],
        [6, 9, 3, 8, 4]], dtype=torch.int32)
```



```python
#ç¬¬0è¡Œ
print(t[0])
```

```
tensor([4, 7, 0, 1, 3], dtype=torch.int32)
```



```python
#å€’æ•°ç¬¬ä¸€è¡Œ
print(t[-1])
```

```
tensor([6, 9, 3, 8, 4], dtype=torch.int32)
```



```python
#ç¬¬1è¡Œç¬¬3åˆ—
print(t[1,3])
print(t[1][3])
```

```
tensor(4, dtype=torch.int32)
tensor(4, dtype=torch.int32)
```



```python
#ç¬¬1è¡Œè‡³ç¬¬3è¡Œ
print(t[1:4,:])
```

```
tensor([[6, 4, 8, 4, 6],
        [3, 4, 0, 1, 2],
        [5, 6, 8, 1, 2]], dtype=torch.int32)
```



```python
#ç¬¬1è¡Œè‡³æœ€åä¸€è¡Œï¼Œç¬¬0åˆ—åˆ°æœ€åä¸€åˆ—æ¯éš”ä¸¤åˆ—å–ä¸€åˆ—
print(t[1:4,:4:2])
```

```
tensor([[6, 8],
        [3, 0],
        [5, 8]], dtype=torch.int32)
```



```python
#å¯ä»¥ä½¿ç”¨ç´¢å¼•å’Œåˆ‡ç‰‡ä¿®æ”¹éƒ¨åˆ†å…ƒç´ 
x = torch.tensor([[1,2],
                  [3,4]],
                 dtype = torch.float32,
                 requires_grad=True)
x.data[1,:] = torch.tensor([0.0,0.0])
x
```

```
tensor([[1., 2.],
        [0., 0.]], requires_grad=True)
```



```python
a = torch.arange(27).view(3,3,3)
print(a)
```

```
tensor([[[ 0,  1,  2],
         [ 3,  4,  5],
         [ 6,  7,  8]],

        [[ 9, 10, 11],
         [12, 13, 14],
         [15, 16, 17]],

        [[18, 19, 20],
         [21, 22, 23],
         [24, 25, 26]]])
```



```python
#çœç•¥å·å¯ä»¥è¡¨ç¤ºå¤šä¸ªå†’å·
print(a[...,1])
```

```
tensor([[ 1,  4,  7],
        [10, 13, 16],
        [19, 22, 25]])
```



ä»¥ä¸Šåˆ‡ç‰‡æ–¹å¼ç›¸å¯¹è§„åˆ™ï¼Œå¯¹äºä¸è§„åˆ™çš„åˆ‡ç‰‡æå–,å¯ä»¥ä½¿ç”¨torch.index_select, torch.take, torch.gather, torch.masked_select.

è€ƒè™‘ç­çº§æˆç»©å†Œçš„ä¾‹å­ï¼Œæœ‰4ä¸ªç­çº§ï¼Œæ¯ä¸ªç­çº§10ä¸ªå­¦ç”Ÿï¼Œæ¯ä¸ªå­¦ç”Ÿ7é—¨ç§‘ç›®æˆç»©ã€‚å¯ä»¥ç”¨ä¸€ä¸ª4Ã—10Ã—7çš„å¼ é‡æ¥è¡¨ç¤ºã€‚


```python
minval=0
maxval=100
scores = torch.floor(minval + (maxval-minval)*torch.rand([4,10,7])).int()
print(scores)
```

```
tensor([[[55, 95,  3, 18, 37, 30, 93],
         [17, 26, 15,  3, 20, 92, 72],
         [74, 52, 24, 58,  3, 13, 24],
         [81, 79, 27, 48, 81, 99, 69],
         [56, 83, 20, 59, 11, 15, 24],
         [72, 70, 20, 65, 77, 43, 51],
         [61, 81, 98, 11, 31, 69, 91],
         [93, 94, 59,  6, 54, 18,  3],
         [94, 88,  0, 59, 41, 41, 27],
         [69, 20, 68, 75, 85, 68,  0]],

        [[17, 74, 60, 10, 21, 97, 83],
         [28, 37,  2, 49, 12, 11, 47],
         [57, 29, 79, 19, 95, 84,  7],
         [37, 52, 57, 61, 69, 52, 25],
         [73,  2, 20, 37, 25, 32,  9],
         [39, 60, 17, 47, 85, 44, 51],
         [45, 60, 81, 97, 81, 97, 46],
         [ 5, 26, 84, 49, 25, 11,  3],
         [ 7, 39, 77, 77,  1, 81, 10],
         [39, 29, 40, 40,  5,  6, 42]],

        [[50, 27, 68,  4, 46, 93, 29],
         [95, 68,  4, 81, 44, 27, 89],
         [ 9, 55, 39, 85, 63, 74, 67],
         [37, 39,  8, 77, 89, 84, 14],
         [52, 14, 22, 20, 67, 20, 48],
         [52, 82, 12, 15, 20, 84, 32],
         [92, 68, 56, 49, 40, 56, 38],
         [49, 56, 10, 23, 90,  9, 46],
         [99, 68, 51,  6, 74, 14, 35],
         [33, 42, 50, 91, 56, 94, 80]],

        [[18, 72, 14, 28, 64, 66, 87],
         [33, 50, 75,  1, 86,  8, 50],
         [41, 23, 56, 91, 35, 20, 31],
         [ 0, 72, 25, 16, 21, 78, 76],
         [88, 68, 33, 36, 64, 91, 63],
         [26, 26,  2, 60, 21,  5, 93],
         [17, 44, 64, 51, 16,  9, 89],
         [58, 91, 33, 64, 38, 47, 19],
         [66, 65, 48, 38, 19, 84, 12],
         [70, 33, 25, 58, 24, 61, 59]]], dtype=torch.int32)
```



```python
#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©
torch.index_select(scores,
                   dim = 1,
                   index = torch.tensor([0,5,9]))
```

```
tensor([[[55, 95,  3, 18, 37, 30, 93],
         [72, 70, 20, 65, 77, 43, 51],
         [69, 20, 68, 75, 85, 68,  0]],

        [[17, 74, 60, 10, 21, 97, 83],
         [39, 60, 17, 47, 85, 44, 51],
         [39, 29, 40, 40,  5,  6, 42]],

        [[50, 27, 68,  4, 46, 93, 29],
         [52, 82, 12, 15, 20, 84, 32],
         [33, 42, 50, 91, 56, 94, 80]],

        [[18, 72, 14, 28, 64, 66, 87],
         [26, 26,  2, 60, 21,  5, 93],
         [70, 33, 25, 58, 24, 61, 59]]], dtype=torch.int32)
```



```python
#æŠ½å–æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„ç¬¬1é—¨è¯¾ç¨‹ï¼Œç¬¬3é—¨è¯¾ç¨‹ï¼Œç¬¬6é—¨è¯¾ç¨‹æˆç»©
q = torch.index_select(torch.index_select(scores,
                                          dim = 1,
                                          index = torch.tensor([0,5,9])),
                       dim=2,
                       index = torch.tensor([1,3,6]))
print(q)
```

```
tensor([[[95, 18, 93],
         [70, 65, 51],
         [20, 75,  0]],

        [[74, 10, 83],
         [60, 47, 51],
         [29, 40, 42]],

        [[27,  4, 29],
         [82, 15, 32],
         [42, 91, 80]],

        [[72, 28, 87],
         [26, 60, 93],
         [33, 58, 59]]], dtype=torch.int32)
```



```python
#æŠ½å–ç¬¬0ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿçš„ç¬¬0é—¨è¯¾ç¨‹ï¼Œç¬¬2ä¸ªç­çº§çš„ç¬¬4ä¸ªå­¦ç”Ÿçš„ç¬¬1é—¨è¯¾ç¨‹ï¼Œç¬¬3ä¸ªç­çº§çš„ç¬¬9ä¸ªå­¦ç”Ÿç¬¬6é—¨è¯¾ç¨‹æˆç»©
#takeå°†è¾“å…¥çœ‹æˆä¸€ç»´æ•°ç»„ï¼Œè¾“å‡ºå’ŒindexåŒå½¢çŠ¶
s = torch.take(scores, torch.tensor([0*10*7+0,2*10*7+4*7+1,3*10*7+9*7+6]))
s
```

```
<tf.Tensor: shape=(3, 7), dtype=int32, numpy=
array([[52, 82, 66, 55, 17, 86, 14],
       [99, 94, 46, 70,  1, 63, 41],
       [46, 83, 70, 80, 90, 85, 17]], dtype=int32)>
```



```python
#æŠ½å–åˆ†æ•°å¤§äºç­‰äº80åˆ†çš„åˆ†æ•°ï¼ˆå¸ƒå°”ç´¢å¼•ï¼‰
#ç»“æœæ˜¯1ç»´å¼ é‡
g = torch.masked_select(scores,scores>=80)
print(g)
```

ä»¥ä¸Šè¿™äº›æ–¹æ³•ä»…èƒ½æå–å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼ï¼Œä½†ä¸èƒ½æ›´æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ã€‚

å¦‚æœè¦é€šè¿‡ä¿®æ”¹å¼ é‡çš„éƒ¨åˆ†å…ƒç´ å€¼å¾—åˆ°æ–°çš„å¼ é‡ï¼Œå¯ä»¥ä½¿ç”¨torch.where,torch.index_fill å’Œ torch.masked_fill

torch.whereå¯ä»¥ç†è§£ä¸ºifçš„å¼ é‡ç‰ˆæœ¬ã€‚

torch.index_fillçš„é€‰å–å…ƒç´ é€»è¾‘å’Œtorch.index_selectç›¸åŒã€‚

torch.masked_fillçš„é€‰å–å…ƒç´ é€»è¾‘å’Œtorch.masked_selectç›¸åŒã€‚


```python
#å¦‚æœåˆ†æ•°å¤§äº60åˆ†ï¼Œèµ‹å€¼æˆ1ï¼Œå¦åˆ™èµ‹å€¼æˆ0
ifpass = torch.where(scores>60,
                     torch.tensor(1),
                     torch.tensor(0))
print(ifpass)
```



```python
#å°†æ¯ä¸ªç­çº§ç¬¬0ä¸ªå­¦ç”Ÿï¼Œç¬¬5ä¸ªå­¦ç”Ÿï¼Œç¬¬9ä¸ªå­¦ç”Ÿçš„å…¨éƒ¨æˆç»©èµ‹å€¼æˆæ»¡åˆ†
torch.index_fill(scores,
                 dim = 1,
                 index = torch.tensor([0,5,9]),
                 value = 100)
#ç­‰ä»·äº scores.index_fill(dim = 1,index = torch.tensor([0,5,9]),value = 100)
```

```python
#å°†åˆ†æ•°å°äº60åˆ†çš„åˆ†æ•°èµ‹å€¼æˆ60åˆ†
b = torch.masked_fill(scores,scores<60,60)
#ç­‰ä»·äºb = scores.masked_fill(scores<60,60)
b
```



### ç»´åº¦å˜æ¢


ç»´åº¦å˜æ¢ç›¸å…³å‡½æ•°ä¸»è¦æœ‰ torch.reshape(æˆ–è€…è°ƒç”¨å¼ é‡çš„viewæ–¹æ³•), torch.squeeze, torch.unsqueeze, torch.transpose

- torch.reshape å¯ä»¥**æ”¹å˜å¼ é‡çš„å½¢çŠ¶**ã€‚
- torch.squeeze å¯ä»¥**å‡å°‘ç»´åº¦**ã€‚
- torch.unsqueeze å¯ä»¥**å¢åŠ ç»´åº¦**ã€‚
- torch.transpose å¯ä»¥**äº¤æ¢ç»´åº¦**ã€‚


```python
# å¼ é‡çš„viewæ–¹æ³•æœ‰æ—¶å€™ä¼šè°ƒç”¨å¤±è´¥ï¼Œå¯ä»¥ä½¿ç”¨reshapeæ–¹æ³•ã€‚
torch.manual_seed(0)
minval,maxval = 0,255
a = (minval + (maxval-minval)*torch.rand([1,3,3,2])).int()
print(a.shape)
print(a)
```

```
torch.Size([1, 3, 3, 2])
tensor([[[[126, 195],
          [ 22,  33],
          [ 78, 161]],

         [[124, 228],
          [116, 161],
          [ 88, 102]],

         [[  5,  43],
          [ 74, 132],
          [177, 204]]]], dtype=torch.int32)
```



```python
# æ”¹æˆ ï¼ˆ3,6ï¼‰å½¢çŠ¶çš„å¼ é‡
b = a.view([3,6]) #torch.reshape(a,[3,6])
print(b.shape)
print(b)
```

```
torch.Size([3, 6])
tensor([[126, 195,  22,  33,  78, 161],
        [124, 228, 116, 161,  88, 102],
        [  5,  43,  74, 132, 177, 204]], dtype=torch.int32)
```

```python
# æ”¹å›æˆ [1,3,3,2] å½¢çŠ¶çš„å¼ é‡
c = torch.reshape(b,[1,3,3,2]) # b.view([1,3,3,2]) 
print(c)
```

```
tensor([[[[126, 195],
          [ 22,  33],
          [ 78, 161]],

         [[124, 228],
          [116, 161],
          [ 88, 102]],

         [[  5,  43],
          [ 74, 132],
          [177, 204]]]], dtype=torch.int32)
```



å¦‚æœå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šåªæœ‰ä¸€ä¸ªå…ƒç´ ï¼Œåˆ©ç”¨torch.squeezeå¯ä»¥æ¶ˆé™¤è¿™ä¸ªç»´åº¦ã€‚

torch.unsqueezeçš„ä½œç”¨å’Œtorch.squeezeçš„ä½œç”¨ç›¸åã€‚

```python
a = torch.tensor([[1.0,2.0]])
s = torch.squeeze(a)
print(a)
print(s)
print(a.shape)
print(s.shape)
```

```
tensor([[1., 2.]])
tensor([1., 2.])
torch.Size([1, 2])
torch.Size([2])
```



```python
#åœ¨ç¬¬0ç»´æ’å…¥é•¿åº¦ä¸º1çš„ä¸€ä¸ªç»´åº¦
d = torch.unsqueeze(s,axis=0)  
print(s)
print(d)

print(s.shape)
print(d.shape)
```

```
tensor([1., 2.])
tensor([[1., 2.]])
torch.Size([2])
torch.Size([1, 2])
```


torch.transposeå¯ä»¥äº¤æ¢å¼ é‡çš„ç»´åº¦ï¼Œtorch.transposeå¸¸ç”¨äºå›¾ç‰‡å­˜å‚¨æ ¼å¼çš„å˜æ¢ä¸Šã€‚

å¦‚æœæ˜¯äºŒç»´çš„çŸ©é˜µï¼Œé€šå¸¸ä¼šè°ƒç”¨çŸ©é˜µçš„è½¬ç½®æ–¹æ³• matrix.t()ï¼Œç­‰ä»·äº torch.transpose(matrix,0,1)ã€‚


```python
minval=0
maxval=255
# Batch,Height,Width,Channel
data = torch.floor(minval + (maxval-minval)*torch.rand([100,256,256,4])).int()
print(data.shape)

# è½¬æ¢æˆ Pytorché»˜è®¤çš„å›¾ç‰‡æ ¼å¼ Batch,Channel,Height,Width 
# éœ€è¦äº¤æ¢ä¸¤æ¬¡
data_t = torch.transpose(torch.transpose(data,1,2),1,3)
print(data_t.shape)
```

```
torch.Size([100, 256, 256, 4])
torch.Size([100, 4, 256, 256])
```



```python
matrix = torch.tensor([[1,2,3],[4,5,6]])
print(matrix)
print(matrix.t()) #ç­‰ä»·äºtorch.transpose(matrix,0,1)
```

```
tensor([[1, 2, 3],
        [4, 5, 6]])
tensor([[1, 4],
        [2, 5],
        [3, 6]])
```



### åˆå¹¶åˆ†å‰²


å¯ä»¥ç”¨torch.catæ–¹æ³•å’Œtorch.stackæ–¹æ³•å°†å¤šä¸ªå¼ é‡åˆå¹¶ï¼Œå¯ä»¥ç”¨torch.splitæ–¹æ³•æŠŠä¸€ä¸ªå¼ é‡åˆ†å‰²æˆå¤šä¸ªå¼ é‡ã€‚

torch.catå’Œtorch.stackæœ‰ç•¥å¾®çš„åŒºåˆ«ï¼Œtorch.catæ˜¯è¿æ¥ï¼Œä¸ä¼šå¢åŠ ç»´åº¦ï¼Œè€Œtorch.stackæ˜¯å †å ï¼Œä¼šå¢åŠ ç»´åº¦ã€‚


```python
a = torch.tensor([[1.0,2.0],
                  [3.0,4.0]])
b = torch.tensor([[5.0,6.0],
                  [7.0,8.0]])
c = torch.tensor([[9.0,10.0],
                  [11.0,12.0]])

abc_cat = torch.cat([a,b,c], dim = 0)
print(abc_cat.shape)
print(abc_cat)
```

```
torch.Size([6, 2])
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
```

```python
abc_stack = torch.stack([a,b,c],axis = 0) #torchä¸­dimå’Œaxiså‚æ•°åå¯ä»¥æ··ç”¨
print(abc_stack.shape)
print(abc_stack)
```

```
torch.Size([3, 2, 2])
tensor([[[ 1.,  2.],
         [ 3.,  4.]],
        [[ 5.,  6.],
         [ 7.,  8.]],
        [[ 9., 10.],
         [11., 12.]]])
```



```python
torch.cat([a,b,c], axis = 1)
```

```
tensor([[ 1.,  2.,  5.,  6.,  9., 10.],
        [ 3.,  4.,  7.,  8., 11., 12.]])
```



```python
torch.stack([a,b,c],axis = 1)
```

```
tensor([[[ 1.,  2.],
         [ 5.,  6.],
         [ 9., 10.]],

        [[ 3.,  4.],
         [ 7.,  8.],
         [11., 12.]]])
```


torch.splitæ˜¯torch.catçš„é€†è¿ç®—ï¼Œå¯ä»¥æŒ‡å®šåˆ†å‰²ä»½æ•°å¹³å‡åˆ†å‰²ï¼Œä¹Ÿå¯ä»¥é€šè¿‡æŒ‡å®šæ¯ä»½çš„è®°å½•æ•°é‡è¿›è¡Œåˆ†å‰²ã€‚

```python
print(abc_cat)
a,b,c = torch.split(abc_cat, split_size_or_sections = 2, dim = 0) #æ¯ä»½2ä¸ªè¿›è¡Œåˆ†å‰²
print(a)
print(b)
print(c)
```

```python
print(abc_cat)
p,q,r = torch.split(abc_cat,split_size_or_sections =[4,1,1],dim = 0) #æ¯ä»½åˆ†åˆ«ä¸º[4,1,1]
print(p)
print(q)
print(r)
```

```
tensor([[ 1.,  2.],
        [ 3.,  4.],
        [ 5.,  6.],
        [ 7.,  8.],
        [ 9., 10.],
        [11., 12.]])
tensor([[1., 2.],
        [3., 4.],
        [5., 6.],
        [7., 8.]])
tensor([[ 9., 10.]])
tensor([[11., 12.]])
```



## å¼ é‡çš„æ•°å­¦è¿ç®—

å¼ é‡çš„æ“ä½œä¸»è¦åŒ…æ‹¬å¼ é‡çš„ç»“æ„æ“ä½œå’Œå¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

1. å¼ é‡ç»“æ„æ“ä½œè¯¸å¦‚ï¼šå¼ é‡åˆ›å»ºï¼Œç´¢å¼•åˆ‡ç‰‡ï¼Œç»´åº¦å˜æ¢ï¼Œåˆå¹¶åˆ†å‰²ã€‚
2. å¼ é‡æ•°å­¦è¿ç®—ä¸»è¦æœ‰ï¼šæ ‡é‡è¿ç®—ï¼Œå‘é‡è¿ç®—ï¼ŒçŸ©é˜µè¿ç®—ã€‚å¦å¤–æˆ‘ä»¬ä¼šä»‹ç»å¼ é‡è¿ç®—çš„å¹¿æ’­æœºåˆ¶ã€‚

æœ¬ç¯‡æˆ‘ä»¬ä»‹ç»å¼ é‡çš„æ•°å­¦è¿ç®—ã€‚

æœ¬ç¯‡æ–‡ç« éƒ¨åˆ†å†…å®¹å‚è€ƒå¦‚ä¸‹åšå®¢ï¼š

- https://blog.csdn.net/duan_zhihua/article/details/82526505



### æ ‡é‡è¿ç®—


å¼ é‡çš„æ•°å­¦è¿ç®—ç¬¦å¯ä»¥åˆ†ä¸ºæ ‡é‡è¿ç®—ç¬¦ã€å‘é‡è¿ç®—ç¬¦ã€ä»¥åŠçŸ©é˜µè¿ç®—ç¬¦ã€‚

åŠ å‡ä¹˜é™¤ä¹˜æ–¹ï¼Œä»¥åŠä¸‰è§’å‡½æ•°ï¼ŒæŒ‡æ•°ï¼Œå¯¹æ•°ç­‰å¸¸è§å‡½æ•°ï¼Œé€»è¾‘æ¯”è¾ƒè¿ç®—ç¬¦ç­‰éƒ½æ˜¯æ ‡é‡è¿ç®—ç¬¦ã€‚

æ ‡é‡è¿ç®—ç¬¦çš„ç‰¹ç‚¹æ˜¯å¯¹å¼ é‡å®æ–½é€å…ƒç´ è¿ç®—ã€‚

æœ‰äº›æ ‡é‡è¿ç®—ç¬¦å¯¹å¸¸ç”¨çš„æ•°å­¦è¿ç®—ç¬¦è¿›è¡Œäº†é‡è½½ã€‚å¹¶ä¸”æ”¯æŒç±»ä¼¼numpyçš„å¹¿æ’­ç‰¹æ€§ã€‚

```python
import torch 
import numpy as np 
```

```python
a = torch.tensor([[1.0,2],
                  [-3,4.0]])
b = torch.tensor([[5.0,6],
                  [7.0,8.0]])
a+b  #è¿ç®—ç¬¦é‡è½½
```

```
tensor([[ 6.,  8.],
        [ 4., 12.]])
```



```python
a-b 
```

```
tensor([[ -4.,  -4.],
        [-10.,  -4.]])
```



```python
a*b 
```

```
tensor([[  5.,  12.],
        [-21.,  32.]])
```



```python
a/b
```

```
tensor([[ 0.2000,  0.3333],
        [-0.4286,  0.5000]])
```



```python
a**2
```

```
tensor([[ 1.,  4.],
        [ 9., 16.]])
```



```python
a**(0.5)
```

```
tensor([[1.0000, 1.4142],
        [   nan, 2.0000]])
```



```python
a%3 #æ±‚æ¨¡
```

```
tensor([[1., 2.],
        [0., 1.]])
```

```python
a//3  #åœ°æ¿é™¤æ³•
```

```
tensor([[ 0.,  0.],
        [-1.,  1.]])
```



```python
a>=2 # torch.ge(a,2)  #ge: greater_equalç¼©å†™
```

```
tensor([[False,  True],
        [False,  True]])
```



```python
(a>=2)&(a<=3)
```

```
tensor([[False,  True],
        [False, False]])
```



```python
(a>=2)|(a<=3)
```

```
tensor([[True, True],
        [True, True]])
```



```python
a==5 #torch.eq(a,5)
```

```
tensor([[False, False],
        [False, False]])
```



```python
torch.sqrt(a)
```

```
tensor([[1.0000, 1.4142],
        [   nan, 2.0000]])
```



```python
a = torch.tensor([1.0, 8.0])
b = torch.tensor([5.0, 6.0])
c = torch.tensor([6.0, 7.0])

d = a+b+c
print(d)
```

```
tensor([12., 21.])
```



```python
print(torch.max(a,b))
```

```
tensor([5., 8.])
```



```python
print(torch.min(a,b))
```

```
tensor([1., 6.])
```



```python
x = torch.tensor([2.6,-2.7])

print(torch.round(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå››èˆäº”å…¥
print(torch.floor(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘ä¸‹å½’æ•´
print(torch.ceil(x))  #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘ä¸Šå½’æ•´
print(torch.trunc(x)) #ä¿ç•™æ•´æ•°éƒ¨åˆ†ï¼Œå‘0å½’æ•´
```

```
tensor([ 3., -3.])
tensor([ 2., -3.])
tensor([ 3., -2.])
tensor([ 2., -2.])
```



```python
x = torch.tensor([2.6,-2.7])
print(torch.fmod(x,2)) #ä½œé™¤æ³•å–ä½™æ•° 
print(torch.remainder(x,2)) #ä½œé™¤æ³•å–å‰©ä½™çš„éƒ¨åˆ†ï¼Œç»“æœæ’æ­£
```

```
tensor([ 0.6000, -0.7000])
tensor([0.6000, 1.3000])
```

```python
# å¹…å€¼è£å‰ª
x = torch.tensor([0.9,-0.8,100.0,-20.0,0.7])
y = torch.clamp(x,min=-1,max = 1)
z = torch.clamp(x,max = 1)
print(y)
print(z)
```

```
tensor([ 0.9000, -0.8000,  1.0000, -1.0000,  0.7000])
tensor([  0.9000,  -0.8000,   1.0000, -20.0000,   0.7000])
```



### å‘é‡è¿ç®—


å‘é‡è¿ç®—ç¬¦åªåœ¨ä¸€ä¸ªç‰¹å®šè½´ä¸Šè¿ç®—ï¼Œå°†ä¸€ä¸ªå‘é‡æ˜ å°„åˆ°ä¸€ä¸ªæ ‡é‡æˆ–è€…å¦å¤–ä¸€ä¸ªå‘é‡ã€‚


```python
#ç»Ÿè®¡å€¼
a = torch.arange(1,10).float()
print(torch.sum(a))
print(torch.mean(a))
print(torch.max(a))
print(torch.min(a))
print(torch.prod(a)) #ç´¯ä¹˜
print(torch.std(a))  #æ ‡å‡†å·®
print(torch.var(a))  #æ–¹å·®
print(torch.median(a)) #ä¸­ä½æ•°
```

```
tensor(45.)
tensor(5.)
tensor(9.)
tensor(1.)
tensor(362880.)
tensor(2.7386)
tensor(7.5000)
tensor(5.)
```



```python
#æŒ‡å®šç»´åº¦è®¡ç®—ç»Ÿè®¡å€¼
b = a.view(3,3)
print(b)
print(torch.max(b,dim = 0))
print(torch.max(b,dim = 1))
```



```
tensor([[1., 2., 3.],
        [4., 5., 6.],
        [7., 8., 9.]])
torch.return_types.max(
values=tensor([7., 8., 9.]),
indices=tensor([2, 2, 2]))
torch.return_types.max(
values=tensor([3., 6., 9.]),
indices=tensor([2, 2, 2]))
```



```python
#cumæ‰«æ
a = torch.arange(1,10)

print(torch.cumsum(a,0))
print(torch.cumprod(a,0))
print(torch.cummax(a,0).values)
print(torch.cummax(a,0).indices)
print(torch.cummin(a,0))
```

```
tensor([ 1,  3,  6, 10, 15, 21, 28, 36, 45])
tensor([     1,      2,      6,     24,    120,    720,   5040,  40320, 362880])
tensor([1, 2, 3, 4, 5, 6, 7, 8, 9])
tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])
torch.return_types.cummin(
values=tensor([1, 1, 1, 1, 1, 1, 1, 1, 1]),
indices=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0]))
```



```python
#torch.sortå’Œtorch.topkå¯ä»¥å¯¹å¼ é‡æ’åº
a = torch.tensor([[9,7,8],
                  [1,3,2],
                  [5,6,4]]).float()
print(torch.topk(a,2,dim = 0),"\n")
print(torch.topk(a,2,dim = 1),"\n")
print(torch.sort(a,dim = 1),"\n")

#åˆ©ç”¨torch.topkå¯ä»¥åœ¨Pytorchä¸­å®ç°KNNç®—æ³•
```

```
torch.return_types.topk(
values=tensor([[9., 7., 8.],
        [5., 6., 4.]]),
indices=tensor([[0, 0, 0],
        [2, 2, 2]]))

torch.return_types.topk(
values=tensor([[9., 8.],
        [3., 2.],
        [6., 5.]]),
indices=tensor([[0, 2],
        [1, 2],
        [1, 0]]))

torch.return_types.sort(
values=tensor([[7., 8., 9.],
        [1., 2., 3.],
        [4., 5., 6.]]),
indices=tensor([[1, 2, 0],
        [0, 2, 1],
        [2, 0, 1]]))
```



### çŸ©é˜µè¿ç®—


çŸ©é˜µå¿…é¡»æ˜¯äºŒç»´çš„ã€‚ç±»ä¼¼torch.tensor([1,2,3])è¿™æ ·çš„ä¸æ˜¯çŸ©é˜µã€‚

çŸ©é˜µè¿ç®—åŒ…æ‹¬ï¼š

- çŸ©é˜µä¹˜æ³•
- çŸ©é˜µè½¬ç½®
- çŸ©é˜µé€†
- çŸ©é˜µæ±‚è¿¹
- çŸ©é˜µèŒƒæ•°
- çŸ©é˜µè¡Œåˆ—å¼
- çŸ©é˜µæ±‚ç‰¹å¾å€¼
- çŸ©é˜µåˆ†è§£
- ç­‰è¿ç®—...


```python
#çŸ©é˜µä¹˜æ³•
a = torch.tensor([[1,2],[3,4]])
b = torch.tensor([[2,0],[0,2]])
print(a@b)  #ç­‰ä»·äºtorch.matmul(a,b) æˆ– torch.mm(a,b)
```

```
tensor([[2, 4],
        [6, 8]])
```



```python
#çŸ©é˜µè½¬ç½®
a = torch.tensor([[1.0,2],[3,4]])
print(a.t())
```

```
tensor([[1., 3.],
        [2., 4.]])
```



```python
#çŸ©é˜µé€†ï¼Œå¿…é¡»ä¸ºæµ®ç‚¹ç±»å‹
a = torch.tensor([[1.0,2],[3,4]])
print(torch.inverse(a))
```

```
tensor([[-2.0000,  1.0000],
        [ 1.5000, -0.5000]])
```



```python
#çŸ©é˜µæ±‚trace
a = torch.tensor([[1.0,2],[3,4]])
print(torch.trace(a))
```

```
tensor(5.)
```



```python
#çŸ©é˜µæ±‚èŒƒæ•°
a = torch.tensor([[1.0,2],[3,4]])
print(torch.norm(a))
```

```
tensor(5.4772)
```



```python
#çŸ©é˜µè¡Œåˆ—å¼
a = torch.tensor([[1.0,2],[3,4]])
print(torch.det(a))
```

```
tensor(-2.0000)
```



```python
#çŸ©é˜µç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡
a = torch.tensor([[1.0,2],[-5,4]],dtype = torch.float)
print(torch.eig(a, eigenvectors=True))

#ä¸¤ä¸ªç‰¹å¾å€¼åˆ†åˆ«æ˜¯ -2.5+2.7839j, 2.5-2.7839j 
```

```
torch.return_types.eig(
eigenvalues=tensor([[ 2.5000,  2.7839],
        [ 2.5000, -2.7839]]),
eigenvectors=tensor([[ 0.2535, -0.4706],
        [ 0.8452,  0.0000]]))
```

```python
#çŸ©é˜µQRåˆ†è§£, å°†ä¸€ä¸ªæ–¹é˜µåˆ†è§£ä¸ºä¸€ä¸ªæ­£äº¤çŸ©é˜µqå’Œä¸Šä¸‰è§’çŸ©é˜µr
#QRåˆ†è§£å®é™…ä¸Šæ˜¯å¯¹çŸ©é˜µaå®æ–½Schmidtæ­£äº¤åŒ–å¾—åˆ°q

a  = torch.tensor([[1.0,2.0],[3.0,4.0]])
q,r = torch.qr(a)
print(q,"\n")
print(r,"\n")
print(q@r)
```



```python
#çŸ©é˜µsvdåˆ†è§£
#svdåˆ†è§£å¯ä»¥å°†ä»»æ„ä¸€ä¸ªçŸ©é˜µåˆ†è§£ä¸ºä¸€ä¸ªæ­£äº¤çŸ©é˜µu,ä¸€ä¸ªå¯¹è§’é˜µså’Œä¸€ä¸ªæ­£äº¤çŸ©é˜µv.t()çš„ä¹˜ç§¯
#svdå¸¸ç”¨äºçŸ©é˜µå‹ç¼©å’Œé™ç»´
a=torch.tensor([[1.0,2.0],[3.0,4.0],[5.0,6.0]])

u,s,v = torch.svd(a)

print(u,"\n")
print(s,"\n")
print(v,"\n")

print(u@torch.diag(s)@v.t())

#åˆ©ç”¨svdåˆ†è§£å¯ä»¥åœ¨Pytorchä¸­å®ç°ä¸»æˆåˆ†åˆ†æé™ç»´
```

```
tensor([[-0.2298,  0.8835],
        [-0.5247,  0.2408],
        [-0.8196, -0.4019]]) 

tensor([9.5255, 0.5143]) 

tensor([[-0.6196, -0.7849],
        [-0.7849,  0.6196]]) 

tensor([[1.0000, 2.0000],
        [3.0000, 4.0000],
        [5.0000, 6.0000]])
```



### å¹¿æ’­æœºåˆ¶


Pytorchçš„å¹¿æ’­è§„åˆ™å’Œnumpyæ˜¯ä¸€æ ·çš„:

1. å¦‚æœå¼ é‡çš„ç»´åº¦ä¸åŒï¼Œå°†ç»´åº¦è¾ƒå°çš„å¼ é‡è¿›è¡Œæ‰©å±•ï¼Œç›´åˆ°ä¸¤ä¸ªå¼ é‡çš„ç»´åº¦éƒ½ä¸€æ ·ã€‚
2. å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æŸä¸ªç»´åº¦ä¸Šçš„é•¿åº¦æ˜¯ç›¸åŒçš„ï¼Œæˆ–è€…å…¶ä¸­ä¸€ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šçš„é•¿åº¦ä¸º1ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°±è¯´è¿™ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦ä¸Šæ˜¯ç›¸å®¹çš„ã€‚
3. å¦‚æœä¸¤ä¸ªå¼ é‡åœ¨æ‰€æœ‰ç»´åº¦ä¸Šéƒ½æ˜¯ç›¸å®¹çš„ï¼Œå®ƒä»¬å°±èƒ½ä½¿ç”¨å¹¿æ’­ã€‚
4. å¹¿æ’­ä¹‹åï¼Œæ¯ä¸ªç»´åº¦çš„é•¿åº¦å°†å–ä¸¤ä¸ªå¼ é‡åœ¨è¯¥ç»´åº¦é•¿åº¦çš„è¾ƒå¤§å€¼ã€‚
5. åœ¨ä»»ä½•ä¸€ä¸ªç»´åº¦ä¸Šï¼Œå¦‚æœä¸€ä¸ªå¼ é‡çš„é•¿åº¦ä¸º1ï¼Œå¦ä¸€ä¸ªå¼ é‡é•¿åº¦å¤§äº1ï¼Œé‚£ä¹ˆåœ¨è¯¥ç»´åº¦ä¸Šï¼Œå°±å¥½åƒæ˜¯å¯¹ç¬¬ä¸€ä¸ªå¼ é‡è¿›è¡Œäº†å¤åˆ¶ã€‚

torch.broadcast_tensorså¯ä»¥å°†å¤šä¸ªå¼ é‡æ ¹æ®å¹¿æ’­è§„åˆ™è½¬æ¢æˆç›¸åŒçš„ç»´åº¦ã€‚

```python
a = torch.tensor([1,2,3])
b = torch.tensor([[0,0,0],
                  [1,1,1],
                  [2,2,2]])
print(b + a) 
```

```
tensor([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])
```

```python
a_broad,b_broad = torch.broadcast_tensors(a,b)
print(a_broad,"\n")
print(b_broad,"\n")
print(a_broad + b_broad) 
```

```
tensor([[1, 2, 3],
        [1, 2, 3],
        [1, 2, 3]]) 

tensor([[0, 0, 0],
        [1, 1, 1],
        [2, 2, 2]]) 

tensor([[1, 2, 3],
        [2, 3, 4],
        [3, 4, 5]])
```



## nn.functional å’Œ nn.Module

```python
import os
import datetime

#æ‰“å°æ—¶é—´
def printbar():
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)

#macç³»ç»Ÿä¸Špytorchå’Œmatplotlibåœ¨jupyterä¸­åŒæ—¶è·‘éœ€è¦æ›´æ”¹ç¯å¢ƒå˜é‡
os.environ["KMP_DUPLICATE_LIB_OK"]="TRUE" 
```



### nn.functional å’Œ nn.Module


å‰é¢æˆ‘ä»¬ä»‹ç»äº†Pytorchçš„å¼ é‡çš„ç»“æ„æ“ä½œå’Œæ•°å­¦è¿ç®—ä¸­çš„ä¸€äº›å¸¸ç”¨APIã€‚åˆ©ç”¨è¿™äº›å¼ é‡çš„APIæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºç¥ç»ç½‘ç»œç›¸å…³çš„ç»„ä»¶(å¦‚æ¿€æ´»å‡½æ•°ï¼Œæ¨¡å‹å±‚ï¼ŒæŸå¤±å‡½æ•°)ã€‚

Pytorchå’Œç¥ç»ç½‘ç»œç›¸å…³çš„åŠŸèƒ½ç»„ä»¶å¤§å¤šéƒ½å°è£…åœ¨ torch.nnæ¨¡å—ä¸‹ã€‚è¿™äº›åŠŸèƒ½ç»„ä»¶çš„ç»å¤§éƒ¨åˆ†æ—¢æœ‰å‡½æ•°å½¢å¼å®ç°ï¼Œä¹Ÿæœ‰ç±»å½¢å¼å®ç°ã€‚å…¶ä¸­nn.functional(ä¸€èˆ¬å¼•å…¥åæ”¹åä¸ºF)æœ‰å„ç§åŠŸèƒ½ç»„ä»¶çš„å‡½æ•°å®ç°ã€‚ä¾‹å¦‚ï¼š

1. æ¿€æ´»å‡½æ•°

* F.relu 
* F.sigmoid
* F.tanh
* F.softmax

2. æ¨¡å‹å±‚

* F.linear
* F.conv2d
* F.max_pool2d
* F.dropout2d
* F.embedding

3. æŸå¤±å‡½æ•°

* F.binary_cross_entropy
* F.mse_loss
* F.cross_entropy



ä¸ºäº†ä¾¿äºå¯¹å‚æ•°è¿›è¡Œç®¡ç†ï¼Œä¸€èˆ¬é€šè¿‡ç»§æ‰¿ nn.Module è½¬æ¢æˆä¸ºç±»çš„å®ç°å½¢å¼ï¼Œå¹¶ç›´æ¥å°è£…åœ¨ nn æ¨¡å—ä¸‹ã€‚ä¾‹å¦‚ï¼š

1. æ¿€æ´»å‡½æ•°

* nn.ReLU
* nn.Sigmoid
* nn.Tanh
* nn.Softmax

2. æ¨¡å‹å±‚

* nn.Linear
* nn.Conv2d
* nn.MaxPool2d
* nn.Dropout2d
* nn.Embedding

3. æŸå¤±å‡½æ•°

* nn.BCELoss
* nn.MSELoss
* nn.CrossEntropyLoss



å®é™…ä¸Šnn.Moduleé™¤äº†å¯ä»¥ç®¡ç†å…¶å¼•ç”¨çš„å„ç§å‚æ•°ï¼Œè¿˜å¯ä»¥ç®¡ç†å…¶å¼•ç”¨çš„å­æ¨¡å—ï¼ŒåŠŸèƒ½ååˆ†å¼ºå¤§ã€‚



### ä½¿ç”¨nn.Moduleæ¥ç®¡ç†å‚æ•°


åœ¨Pytorchä¸­ï¼Œæ¨¡å‹çš„å‚æ•°æ˜¯éœ€è¦è¢«ä¼˜åŒ–å™¨è®­ç»ƒçš„ï¼Œå› æ­¤ï¼Œé€šå¸¸è¦è®¾ç½®å‚æ•°ä¸º requires_grad = True çš„å¼ é‡ã€‚

åŒæ—¶ï¼Œåœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ï¼Œå¾€å¾€æœ‰è®¸å¤šçš„å‚æ•°ï¼Œè¦æ‰‹åŠ¨ç®¡ç†è¿™äº›å‚æ•°å¹¶ä¸æ˜¯ä¸€ä»¶å®¹æ˜“çš„äº‹æƒ…ã€‚

Pytorchä¸€èˆ¬å°†å‚æ•°ç”¨nn.Parameteræ¥è¡¨ç¤ºï¼Œå¹¶ä¸”ç”¨nn.Moduleæ¥ç®¡ç†å…¶ç»“æ„ä¸‹çš„æ‰€æœ‰å‚æ•°ã€‚


```python
import torch 
from torch import nn 
import torch.nn.functional as F
from matplotlib import pyplot as plt
```

```python
# nn.Parameter å…·æœ‰ requires_grad = True å±æ€§
w = nn.Parameter(torch.randn(2,2))
print(w)
print(w.requires_grad)
```

```
Parameter containing:
tensor([[ 0.3544, -1.1643],
        [ 1.2302,  1.3952]], requires_grad=True)
True
```



```python
# nn.ParameterList å¯ä»¥å°†å¤šä¸ªnn.Parameterç»„æˆä¸€ä¸ªåˆ—è¡¨
params_list = nn.ParameterList([nn.Parameter(torch.rand(8,i)) for i in range(1,3)])
print(params_list)
print(params_list[0].requires_grad)
```

```
ParameterList(
    (0): Parameter containing: [torch.FloatTensor of size 8x1]
    (1): Parameter containing: [torch.FloatTensor of size 8x2]
)
True
```



```python
# nn.ParameterDict å¯ä»¥å°†å¤šä¸ªnn.Parameterç»„æˆä¸€ä¸ªå­—å…¸
params_dict = nn.ParameterDict({"a": nn.Parameter(torch.rand(2,2)),
                                "b": nn.Parameter(torch.zeros(2))})
print(params_dict)
print(params_dict["a"].requires_grad)
```

```
ParameterDict(
    (a): Parameter containing: [torch.FloatTensor of size 2x2]
    (b): Parameter containing: [torch.FloatTensor of size 2]
)
True
```



```python
# å¯ä»¥ç”¨Moduleå°†å®ƒä»¬ç®¡ç†èµ·æ¥
# module.parameters()è¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬å…¶ç»“æ„ä¸‹çš„æ‰€æœ‰parameters

module = nn.Module()
module.w = w
module.params_list = params_list
module.params_dict = params_dict

num_param = 0
for param in module.parameters():
    print(param,"\n")
    num_param = num_param + 1
print("number of Parameters =", num_param)
```

```
Parameter containing:
tensor([[ 0.3544, -1.1643],
        [ 1.2302,  1.3952]], requires_grad=True) 

Parameter containing:
tensor([[0.9391],
        [0.7590],
        [0.6899],
        [0.4786],
        [0.2392],
        [0.9645],
        [0.1968],
        [0.1353]], requires_grad=True) 

Parameter containing:
tensor([[0.8012, 0.9587],
        [0.0276, 0.5995],
        [0.7338, 0.5559],
        [0.1704, 0.5814],
        [0.7626, 0.1179],
        [0.4945, 0.2408],
        [0.7179, 0.0575],
        [0.3418, 0.7291]], requires_grad=True) 

Parameter containing:
tensor([[0.7729, 0.2383],
        [0.7054, 0.9937]], requires_grad=True) 

Parameter containing:
tensor([0., 0.], requires_grad=True) 

number of Parameters = 5
```

```python
#å®è·µå½“ä¸­ï¼Œä¸€èˆ¬é€šè¿‡ç»§æ‰¿nn.Moduleæ¥æ„å»ºæ¨¡å—ç±»ï¼Œå¹¶å°†æ‰€æœ‰å«æœ‰éœ€è¦å­¦ä¹ çš„å‚æ•°çš„éƒ¨åˆ†æ”¾åœ¨æ„é€ å‡½æ•°ä¸­ã€‚

#ä»¥ä¸‹èŒƒä¾‹ä¸ºPytorchä¸­nn.Linearçš„æºç çš„ç®€åŒ–ç‰ˆæœ¬
#å¯ä»¥çœ‹åˆ°å®ƒå°†éœ€è¦å­¦ä¹ çš„å‚æ•°æ”¾åœ¨äº†__init__æ„é€ å‡½æ•°ä¸­ï¼Œå¹¶åœ¨forwardä¸­è°ƒç”¨F.linearå‡½æ•°æ¥å®ç°è®¡ç®—é€»è¾‘ã€‚

class Linear(nn.Module):
    __constants__ = ['in_features', 'out_features']

    def __init__(self, in_features, out_features, bias=True):
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

    def forward(self, input):
        return F.linear(input, self.weight, self.bias)
```



### ä½¿ç”¨nn.Moduleæ¥ç®¡ç†å­æ¨¡å—


ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬éƒ½å¾ˆå°‘ç›´æ¥ä½¿ç”¨ nn.Parameteræ¥å®šä¹‰å‚æ•°æ„å»ºæ¨¡å‹ï¼Œè€Œæ˜¯é€šè¿‡ä¸€äº›æ‹¼è£…ä¸€äº›å¸¸ç”¨çš„æ¨¡å‹å±‚æ¥æ„é€ æ¨¡å‹ã€‚

è¿™äº›æ¨¡å‹å±‚ä¹Ÿæ˜¯ç»§æ‰¿è‡ªnn.Moduleçš„å¯¹è±¡,æœ¬èº«ä¹ŸåŒ…æ‹¬å‚æ•°ï¼Œå±äºæˆ‘ä»¬è¦å®šä¹‰çš„æ¨¡å—çš„å­æ¨¡å—ã€‚

nn.Moduleæä¾›äº†ä¸€äº›æ–¹æ³•å¯ä»¥ç®¡ç†è¿™äº›å­æ¨¡å—ã€‚

* **children()** æ–¹æ³•: è¿”å›ç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å­æ¨¡å—ã€‚
* **named_children()**æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å­æ¨¡å—ï¼Œä»¥åŠå®ƒä»¬çš„åå­—ã€‚
* **modules()**æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å„ä¸ªå±‚çº§çš„æ¨¡å—ï¼ŒåŒ…æ‹¬æ¨¡å—æœ¬èº«ã€‚
* **named_modules()**æ–¹æ³•ï¼šè¿”å›ä¸€ä¸ªç”Ÿæˆå™¨ï¼ŒåŒ…æ‹¬æ¨¡å—ä¸‹çš„æ‰€æœ‰å„ä¸ªå±‚çº§çš„æ¨¡å—ä»¥åŠå®ƒä»¬çš„åå­—ï¼ŒåŒ…æ‹¬æ¨¡å—æœ¬èº«ã€‚

å…¶ä¸­chidren()æ–¹æ³•å’Œnamed_children()æ–¹æ³•è¾ƒå¤šä½¿ç”¨ã€‚

modules()æ–¹æ³•å’Œnamed_modules()æ–¹æ³•è¾ƒå°‘ä½¿ç”¨ï¼Œå…¶åŠŸèƒ½å¯ä»¥é€šè¿‡å¤šä¸ªnamed_children()çš„åµŒå¥—ä½¿ç”¨å®ç°ã€‚


```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        
        self.embedding = nn.Embedding(num_embeddings = 10000,
                                      embedding_dim = 3,
                                      padding_idx = 1)
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1",nn.Conv1d(in_channels = 3,
                                                out_channels = 16,
                                                kernel_size = 5))
        self.conv.add_module("pool_1",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1",nn.ReLU())
        self.conv.add_module("conv_2",nn.Conv1d(in_channels = 16,
                                                out_channels = 128,
                                                kernel_size = 2))
        self.conv.add_module("pool_2",nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2",nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten",nn.Flatten())
        self.dense.add_module("linear",nn.Linear(6144,1))
        self.dense.add_module("sigmoid",nn.Sigmoid())
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
    
net = Net()
```

```python
i = 0
for child in net.children():
    i+=1
    print(child,"\n")
print("child number",i)
```

```
Embedding(10000, 3, padding_idx=1) 

Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

Sequential(
  (flatten): Flatten()
  (linear): Linear(in_features=6144, out_features=1, bias=True)
  (sigmoid): Sigmoid()
) 

child number 3
```



```python
i = 0
for name, child in net.named_children():
    i+=1
    print(name,":",child,"\n")
print("child number",i)
```

```
embedding : Embedding(10000, 3, padding_idx=1) 

conv : Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
) 

dense : Sequential(
  (flatten): Flatten()
  (linear): Linear(in_features=6144, out_features=1, bias=True)
  (sigmoid): Sigmoid()
) 

child number 3
```



```python
i = 0
for module in net.modules():
    i+=1
    print(module)
print("module number:",i)
```

```
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten()
    (linear): Linear(in_features=6144, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
Embedding(10000, 3, padding_idx=1)
Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
)
Conv1d(3, 16, kernel_size=(5,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Conv1d(16, 128, kernel_size=(2,), stride=(1,))
MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
ReLU()
Sequential(
  (flatten): Flatten()
  (linear): Linear(in_features=6144, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
Flatten()
Linear(in_features=6144, out_features=1, bias=True)
Sigmoid()
module number: 13
```



ä¸‹é¢æˆ‘ä»¬é€šè¿‡named_childrenæ–¹æ³•æ‰¾åˆ°embeddingå±‚ï¼Œå¹¶å°†å…¶å‚æ•°è®¾ç½®ä¸ºä¸å¯è®­ç»ƒ(ç›¸å½“äºå†»ç»“embeddingå±‚)ã€‚

```python
children_dict = {name:module for name,module in net.named_children()}

print(children_dict)
embedding = children_dict["embedding"]
embedding.requires_grad_(False) #å†»ç»“å…¶å‚æ•°
```

```
{'embedding': Embedding(10000, 3, padding_idx=1), 'conv': Sequential(
  (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
  (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_1): ReLU()
  (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
  (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (relu_2): ReLU()
), 'dense': Sequential(
  (flatten): Flatten()
  (linear): Linear(in_features=6144, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)}
```



```python
#å¯ä»¥çœ‹åˆ°å…¶ç¬¬ä¸€å±‚çš„å‚æ•°å·²ç»ä¸å¯ä»¥è¢«è®­ç»ƒäº†ã€‚
for param in embedding.parameters():
    print(param.requires_grad)
    print(param.numel())
```

```
False
30000
```



```python
from torchkeras import summary
summary(net,input_shape = (200,),input_dtype = torch.LongTensor)
# ä¸å¯è®­ç»ƒå‚æ•°æ•°é‡å¢åŠ 
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Embedding-1               [-1, 200, 3]          30,000
            Conv1d-2              [-1, 16, 196]             256
         MaxPool1d-3               [-1, 16, 98]               0
              ReLU-4               [-1, 16, 98]               0
            Conv1d-5              [-1, 128, 97]           4,224
         MaxPool1d-6              [-1, 128, 48]               0
              ReLU-7              [-1, 128, 48]               0
           Flatten-8                 [-1, 6144]               0
            Linear-9                    [-1, 1]           6,145
          Sigmoid-10                    [-1, 1]               0
================================================================
Total params: 40,625
Trainable params: 10,625
Non-trainable params: 30,000
----------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287796
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443531
----------------------------------------------------------------
```



# Pytorchçš„ä¸­é˜¶API

æˆ‘ä»¬å°†ä¸»è¦ä»‹ç»Pytorchçš„å¦‚ä¸‹ä¸­é˜¶API

* æ•°æ®ç®¡é“
* æ¨¡å‹å±‚
* æŸå¤±å‡½æ•°
* TensorBoardå¯è§†åŒ–

å¦‚æœæŠŠæ¨¡å‹æ¯”ä½œä¸€ä¸ªæˆ¿å­ï¼Œé‚£ä¹ˆä¸­é˜¶APIå°±æ˜¯ã€æ¨¡å‹ä¹‹å¢™ã€‘ã€‚



## Datasetå’ŒDataLoader

Pytorché€šå¸¸ä½¿ç”¨Datasetå’ŒDataLoaderè¿™ä¸¤ä¸ªå·¥å…·ç±»æ¥æ„å»ºæ•°æ®ç®¡é“ã€‚

**Datasetå®šä¹‰äº†æ•°æ®é›†çš„å†…å®¹**ï¼Œå®ƒç›¸å½“äºä¸€ä¸ªç±»ä¼¼åˆ—è¡¨çš„æ•°æ®ç»“æ„ï¼Œå…·æœ‰ç¡®å®šçš„é•¿åº¦ï¼Œèƒ½å¤Ÿç”¨ç´¢å¼•è·å–æ•°æ®é›†ä¸­çš„å…ƒç´ ã€‚

**DataLoaderå®šä¹‰äº†æŒ‰batchåŠ è½½æ•°æ®é›†çš„æ–¹æ³•**ï¼Œå®ƒæ˜¯ä¸€ä¸ªå®ç°äº†`__iter__`æ–¹æ³•çš„å¯è¿­ä»£å¯¹è±¡ï¼Œæ¯æ¬¡è¿­ä»£è¾“å‡ºä¸€ä¸ªbatchçš„æ•°æ®ã€‚

DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚

åœ¨ç»å¤§éƒ¨åˆ†æƒ…å†µä¸‹ï¼Œç”¨æˆ·åªéœ€å®ç°Datasetçš„`__len__`æ–¹æ³•å’Œ`__getitem__`æ–¹æ³•ï¼Œå°±å¯ä»¥è½»æ¾æ„å»ºè‡ªå·±çš„æ•°æ®é›†ï¼Œå¹¶ç”¨é»˜è®¤æ•°æ®ç®¡é“è¿›è¡ŒåŠ è½½ã€‚



### Datasetå’ŒDataLoaderæ¦‚è¿°

**1. è·å–ä¸€ä¸ªbatchæ•°æ®çš„æ­¥éª¤**


è®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸‹ä»ä¸€ä¸ªæ•°æ®é›†ä¸­è·å–ä¸€ä¸ªbatchçš„æ•°æ®éœ€è¦å“ªäº›æ­¥éª¤ã€‚å‡å®šæ•°æ®é›†çš„ç‰¹å¾å’Œæ ‡ç­¾åˆ†åˆ«è¡¨ç¤ºä¸ºå¼ é‡Xå’ŒYï¼Œæ•°æ®é›†å¯ä»¥è¡¨ç¤ºä¸º(X,Y), å‡å®šbatchå¤§å°ä¸ºmã€‚

1. é¦–å…ˆæˆ‘ä»¬è¦ç¡®å®šæ•°æ®é›†çš„é•¿åº¦nã€‚ç»“æœç±»ä¼¼ï¼šn = 1000ã€‚
2. ç„¶åæˆ‘ä»¬ä»0åˆ°n-1çš„èŒƒå›´ä¸­æŠ½æ ·å‡ºmä¸ªæ•°(batchå¤§å°)ã€‚å‡å®šm=4, æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªåˆ—è¡¨ï¼Œç±»ä¼¼ï¼šindices = [1,4,8,9]
3. æ¥ç€æˆ‘ä»¬ä»æ•°æ®é›†ä¸­å»å–è¿™mä¸ªæ•°å¯¹åº”ä¸‹æ ‡çš„å…ƒç´ ã€‚æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸€ä¸ªå…ƒç»„åˆ—è¡¨ï¼Œç±»ä¼¼ï¼šsamples = [(X[1],Y[1]), (X[4],Y[4]), (X[8],Y[8]), (X[9],Y[9])]
4. æœ€åæˆ‘ä»¬å°†ç»“æœæ•´ç†æˆä¸¤ä¸ªå¼ é‡ä½œä¸ºè¾“å‡ºã€‚æ‹¿åˆ°çš„ç»“æœæ˜¯ä¸¤ä¸ªå¼ é‡ï¼Œç±»ä¼¼batch = (features,labels) ï¼Œ å…¶ä¸­ features = torch.stack([X[1], X[4], X[8], X[9]])ï¼Œlabels = torch.stack([Y[1], Y[4], Y[8], Y[9]])



**2. Datasetå’ŒDataLoaderçš„åŠŸèƒ½åˆ†å·¥**

1. ä¸Šè¿°ç¬¬1ä¸ªæ­¥éª¤ç¡®å®šæ•°æ®é›†çš„é•¿åº¦æ˜¯ç”± Datasetçš„\_\_len\_\_ æ–¹æ³•å®ç°çš„ã€‚
2. ç¬¬2ä¸ªæ­¥éª¤ä»0åˆ°n-1çš„èŒƒå›´ä¸­æŠ½æ ·å‡ºmä¸ªæ•°çš„æ–¹æ³•æ˜¯ç”± DataLoaderçš„ samplerå’Œ batch_samplerå‚æ•°æŒ‡å®šçš„ã€‚
    - samplerå‚æ•°æŒ‡å®šå•ä¸ªå…ƒç´ æŠ½æ ·æ–¹æ³•ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œç¨‹åºé»˜è®¤åœ¨DataLoaderçš„å‚æ•°shuffle=Trueæ—¶é‡‡ç”¨éšæœºæŠ½æ ·ï¼Œshuffle=Falseæ—¶é‡‡ç”¨é¡ºåºæŠ½æ ·ã€‚
    - batch_samplerå‚æ•°å°†å¤šä¸ªæŠ½æ ·çš„å…ƒç´ æ•´ç†æˆä¸€ä¸ªåˆ—è¡¨ï¼Œä¸€èˆ¬æ— éœ€ç”¨æˆ·è®¾ç½®ï¼Œé»˜è®¤æ–¹æ³•åœ¨DataLoaderçš„å‚æ•°drop_last=Trueæ—¶ä¼šä¸¢å¼ƒæ•°æ®é›†æœ€åä¸€ä¸ªé•¿åº¦ä¸èƒ½è¢«batchå¤§å°æ•´é™¤çš„æ‰¹æ¬¡ï¼Œåœ¨drop_last=Falseæ—¶ä¿ç•™æœ€åä¸€ä¸ªæ‰¹æ¬¡ã€‚

3. ç¬¬3ä¸ªæ­¥éª¤çš„æ ¸å¿ƒé€»è¾‘æ ¹æ®ä¸‹æ ‡å–æ•°æ®é›†ä¸­çš„å…ƒç´  æ˜¯ç”± Datasetçš„ \_\_getitem\_\_æ–¹æ³•å®ç°çš„ã€‚
4. ç¬¬4ä¸ªæ­¥éª¤çš„é€»è¾‘ç”±DataLoaderçš„å‚æ•°collate_fnæŒ‡å®šã€‚ä¸€èˆ¬æƒ…å†µä¸‹ä¹Ÿæ— éœ€ç”¨æˆ·è®¾ç½®ã€‚



**3. Datasetå’ŒDataLoaderçš„ä¸»è¦æ¥å£**


ä»¥ä¸‹æ˜¯ Datasetå’Œ DataLoaderçš„æ ¸å¿ƒæ¥å£é€»è¾‘ä¼ªä»£ç ï¼Œä¸å®Œå…¨å’Œæºç ä¸€è‡´ã€‚

```python
import torch 

class Dataset(object):
    def __init__(self):
        pass
    
    def __len__(self):
        raise NotImplementedError
        
    def __getitem__(self,index):
        raise NotImplementedError
        

class DataLoader(object):
    def __init__(self, dataset, batch_size, collate_fn, shuffle = True, drop_last = False):
        self.dataset = dataset
        self.sampler =torch.utils.data.RandomSampler if shuffle else \
           torch.utils.data.SequentialSampler
        self.batch_sampler = torch.utils.data.BatchSampler
        self.sample_iter = self.batch_sampler(self.sampler(range(len(dataset))),
                                              batch_size = batch_size,
                                              drop_last = drop_last)
        
    def __next__(self):
        indices = next(self.sample_iter)
        batch = self.collate_fn([self.dataset[i] for i in indices])
        return batch
```



### ä½¿ç”¨Datasetåˆ›å»ºæ•°æ®é›†

Datasetåˆ›å»ºæ•°æ®é›†å¸¸ç”¨çš„æ–¹æ³•æœ‰ï¼š

* ä½¿ç”¨ torch.utils.data.TensorDataset æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†(numpyçš„arrayï¼ŒPandasçš„DataFrameéœ€è¦å…ˆè½¬æ¢æˆTensor)ã€‚
* ä½¿ç”¨ torchvision.datasets.ImageFolder æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†ã€‚
* ç»§æ‰¿ torch.utils.data.Dataset åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†ã€‚


æ­¤å¤–ï¼Œè¿˜å¯ä»¥é€šè¿‡ï¼š

* torch.utils.data.random_split å°†ä¸€ä¸ªæ•°æ®é›†åˆ†å‰²æˆå¤šä»½ï¼Œå¸¸ç”¨äºåˆ†å‰²è®­ç»ƒé›†ï¼ŒéªŒè¯é›†å’Œæµ‹è¯•é›†ã€‚
* è°ƒç”¨Datasetçš„åŠ æ³•è¿ç®—ç¬¦(`+`)å°†å¤šä¸ªæ•°æ®é›†åˆå¹¶æˆä¸€ä¸ªæ•°æ®é›†ã€‚



**1. æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†**

```python
import numpy as np 
import torch 
from torch.utils.data import TensorDataset,Dataset,DataLoader,random_split 
```

```python
# æ ¹æ®Tensoråˆ›å»ºæ•°æ®é›†
from sklearn import datasets 

iris = datasets.load_iris()
ds_iris = TensorDataset(torch.tensor(iris.data), torch.tensor(iris.target))

# åˆ†å‰²æˆè®­ç»ƒé›†å’Œé¢„æµ‹é›†
n_train = int(len(ds_iris)*0.8)
n_valid = len(ds_iris) - n_train
ds_train,ds_valid = random_split(ds_iris, [n_train, n_valid])

print(type(ds_iris))
print(type(ds_train))
```



```python
# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†
dl_train,dl_valid = DataLoader(ds_train, batch_size = 8), DataLoader(ds_valid, batch_size = 8)

for features,labels in dl_train:
    print(features,labels)
    break
```

```python
# æ¼”ç¤ºåŠ æ³•è¿ç®—ç¬¦ï¼ˆ+ï¼‰çš„åˆå¹¶ä½œç”¨
ds_data = ds_train + ds_valid

print('len(ds_train) = ',len(ds_train))
print('len(ds_valid) = ',len(ds_valid))
print('len(ds_train+ds_valid) = ',len(ds_data))

print(type(ds_data))
```



**2. æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºå›¾ç‰‡æ•°æ®é›†**

```python
import numpy as np 
import torch 
from torch.utils.data import DataLoader
from torchvision import transforms,datasets 
```

```python
# æ¼”ç¤ºä¸€äº›å¸¸ç”¨çš„å›¾ç‰‡å¢å¼ºæ“ä½œ
```

```python
from PIL import Image
img = Image.open('./data/cat.jpeg')
img
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-632941.png)

```python
# éšæœºæ•°å€¼ç¿»è½¬
transforms.RandomVerticalFlip()(img)
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-640192.png)

```python
#éšæœºæ—‹è½¬
transforms.RandomRotation(45)(img)
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-636235.png)

```python
# å®šä¹‰å›¾ç‰‡å¢å¼ºæ“ä½œ
transform_train = transforms.Compose([transforms.RandomHorizontalFlip(), #éšæœºæ°´å¹³ç¿»è½¬
                                      transforms.RandomVerticalFlip(), #éšæœºå‚ç›´ç¿»è½¬
                                      transforms.RandomRotation(45),  #éšæœºåœ¨45åº¦è§’åº¦å†…æ—‹è½¬
                                      transforms.ToTensor() #è½¬æ¢æˆå¼ é‡
                                     ]
) 

transform_valid = transforms.Compose([transforms.ToTensor()])
```

```python
# æ ¹æ®å›¾ç‰‡ç›®å½•åˆ›å»ºæ•°æ®é›†
ds_train = datasets.ImageFolder("./data/cifar2/train/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())
ds_valid = datasets.ImageFolder("./data/cifar2/test/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())

print(ds_train.class_to_idx)
```

```
{'0_airplane': 0, '1_automobile': 1}
```

```python
# ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†
dl_train = DataLoader(ds_train,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)
dl_valid = DataLoader(ds_valid,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)
```

```python
for features,labels in dl_train:
    print(features.shape)
    print(labels.shape)
    break
```

```
torch.Size([50, 3, 32, 32])
torch.Size([50, 1])
```



**3. åˆ›å»ºè‡ªå®šä¹‰æ•°æ®é›†**


ä¸‹é¢é€šè¿‡ç»§æ‰¿Datasetç±»åˆ›å»ºimdbæ–‡æœ¬åˆ†ç±»ä»»åŠ¡çš„è‡ªå®šä¹‰æ•°æ®é›†ã€‚

å¤§æ¦‚æ€è·¯å¦‚ä¸‹ï¼š

1. é¦–å…ˆï¼Œå¯¹è®­ç»ƒé›†æ–‡æœ¬åˆ†è¯æ„å»ºè¯å…¸ã€‚
2. ç„¶åå°†è®­ç»ƒé›†æ–‡æœ¬å’Œæµ‹è¯•é›†æ–‡æœ¬æ•°æ®è½¬æ¢æˆtokenå•è¯ç¼–ç ã€‚
3. æ¥ç€å°†è½¬æ¢æˆå•è¯ç¼–ç çš„è®­ç»ƒé›†æ•°æ®å’Œæµ‹è¯•é›†æ•°æ®æŒ‰æ ·æœ¬åˆ†å‰²æˆå¤šä¸ªæ–‡ä»¶ï¼Œä¸€ä¸ªæ–‡ä»¶ä»£è¡¨ä¸€ä¸ªæ ·æœ¬ã€‚
4. æœ€åï¼Œæˆ‘ä»¬å¯ä»¥æ ¹æ®æ–‡ä»¶ååˆ—è¡¨è·å–å¯¹åº”åºå·çš„æ ·æœ¬å†…å®¹ï¼Œä»è€Œæ„å»ºDatasetæ•°æ®é›†ã€‚


```python
import numpy as np 
import pandas as pd 
from collections import OrderedDict
import re,string

MAX_WORDS = 10000  # ä»…è€ƒè™‘æœ€é«˜é¢‘çš„10000ä¸ªè¯
MAX_LEN = 200  # æ¯ä¸ªæ ·æœ¬ä¿ç•™200ä¸ªè¯çš„é•¿åº¦
BATCH_SIZE = 20 

train_data_path = 'data/imdb/train.tsv'
test_data_path = 'data/imdb/test.tsv'

train_token_path = 'data/imdb/train_token.tsv'
test_token_path =  'data/imdb/test_token.tsv'

train_samples_path = 'data/imdb/train_samples/'
test_samples_path =  'data/imdb/test_samples/'
```

é¦–å…ˆæˆ‘ä»¬æ„å»ºè¯å…¸ï¼Œå¹¶ä¿ç•™æœ€é«˜é¢‘çš„MAX_WORDSä¸ªè¯ã€‚

```python
##æ„å»ºè¯å…¸
word_count_dict = {}

#æ¸…æ´—æ–‡æœ¬
def clean_text(text):
    lowercase = text.lower().replace("\n"," ")
    stripped_html = re.sub('<br />', ' ',lowercase)
    cleaned_punctuation = re.sub('[%s]' % re.escape(string.punctuation), '', stripped_html)
    return cleaned_punctuation

with open(train_data_path,"r",encoding = 'utf-8') as f:
    for line in f:
        label,text = line.split("\t")
        cleaned_text = clean_text(text)
        
        for word in cleaned_text.split(" "):
            word_count_dict[word] = word_count_dict.get(word,0)+1 

df_word_dict = pd.DataFrame(pd.Series(word_count_dict,name = "count"))
df_word_dict = df_word_dict.sort_values(by = "count",ascending =False)

df_word_dict = df_word_dict[0:MAX_WORDS-2] #  
df_word_dict["word_id"] = range(2,MAX_WORDS) #ç¼–å·0å’Œ1åˆ†åˆ«ç•™ç»™æœªçŸ¥è¯<unkown>å’Œå¡«å……<padding>

word_id_dict = df_word_dict["word_id"].to_dict()

df_word_dict.head(10)
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-644144.png)


ç„¶åæˆ‘ä»¬åˆ©ç”¨æ„å»ºå¥½çš„è¯å…¸ï¼Œå°†æ–‡æœ¬è½¬æ¢æˆtokenåºå·ã€‚

```python
#è½¬æ¢token

# å¡«å……æ–‡æœ¬
def pad(data_list,pad_length):
    padded_list = data_list.copy()
    
    if len(data_list)> pad_length:
         padded_list = data_list[-pad_length:]
            
    if len(data_list)< pad_length:
         padded_list = [1]*(pad_length-len(data_list))+data_list
    return padded_list

def text_to_token(text_file,token_file):
    with open(text_file,"r",encoding = 'utf-8') as fin, open(token_file,"w",encoding = 'utf-8') as fout:
        for line in fin:
            label,text = line.split("\t")
            cleaned_text = clean_text(text)
            word_token_list = [word_id_dict.get(word, 0) for word in cleaned_text.split(" ")]
            pad_list = pad(word_token_list,MAX_LEN)
            out_line = label+"\t"+" ".join([str(x) for x in pad_list])
            fout.write(out_line+"\n")
        
text_to_token(train_data_path,train_token_path)
text_to_token(test_data_path,test_token_path)
```

æ¥ç€å°†tokenæ–‡æœ¬æŒ‰ç…§æ ·æœ¬åˆ†å‰²ï¼Œæ¯ä¸ªæ–‡ä»¶å­˜æ”¾ä¸€ä¸ªæ ·æœ¬çš„æ•°æ®ã€‚

```python
# åˆ†å‰²æ ·æœ¬
import os

if not os.path.exists(train_samples_path):
    os.mkdir(train_samples_path)
    
if not os.path.exists(test_samples_path):
    os.mkdir(test_samples_path)
    
    
def split_samples(token_path,samples_dir):
    with open(token_path,"r",encoding = 'utf-8') as fin:
        i = 0
        for line in fin:
            with open(samples_dir+"%d.txt"%i,"w",encoding = "utf-8") as fout:
                fout.write(line)
            i = i+1

split_samples(train_token_path,train_samples_path)
split_samples(test_token_path,test_samples_path)
```

```python
print(os.listdir(train_samples_path)[0:100])
```

```
['11303.txt', '3644.txt', '19987.txt', '18441.txt', '5235.txt', '17772.txt', '1053.txt', '13514.txt', '8711.txt', '15165.txt', '7422.txt', '8077.txt', '15603.txt', '7344.txt', '1735.txt', '13272.txt', '9369.txt', '18327.txt', '5553.txt', '17014.txt', '4895.txt', '11465.txt', '3122.txt', '19039.txt', '5547.txt', '18333.txt', '17000.txt', '4881.txt', '2228.txt', '11471.txt', '3136.txt', '4659.txt', '15617.txt', '8063.txt', '7350.txt', '12178.txt', '1721.txt', '13266.txt', '14509.txt', '6728.txt', '1047.txt', '13500.txt', '15171.txt', '8705.txt', '7436.txt', '16478.txt', '11317.txt', '3650.txt', '19993.txt', '10009.txt', '5221.txt', '18455.txt', '17766.txt', '3888.txt', '6700.txt', '14247.txt', '9433.txt', '13528.txt', '12636.txt', '15159.txt', '16450.txt', '4117.txt', '19763.txt', '3678.txt', '17996.txt', '2566.txt', '10021.txt', '5209.txt', '17028.txt', '2200.txt', '10747.txt', '11459.txt', '16336.txt', '4671.txt', '19005.txt', '7378.txt', '12150.txt', '1709.txt', '6066.txt', '14521.txt', '9355.txt', '12144.txt', '289.txt', '6072.txt', '9341.txt', '14535.txt', '2214.txt', '10753.txt', '16322.txt', '19011.txt', '4665.txt', '16444.txt', '19777.txt', '4103.txt', '17982.txt', '2572.txt', '10035.txt', '18469.txt', '6714.txt', '9427.txt']
```


ä¸€åˆ‡å‡†å¤‡å°±ç»ªï¼Œæˆ‘ä»¬å¯ä»¥åˆ›å»ºæ•°æ®é›†Dataset, ä»æ–‡ä»¶åç§°åˆ—è¡¨ä¸­è¯»å–æ–‡ä»¶å†…å®¹äº†ã€‚

```python
import os

class imdbDataset(Dataset):
    def __init__(self,samples_dir):
        self.samples_dir = samples_dir
        self.samples_paths = os.listdir(samples_dir)
    
    def __len__(self):
        return len(self.samples_paths)
    
    def __getitem__(self,index):
        path = self.samples_dir + self.samples_paths[index]
        with open(path,"r",encoding = "utf-8") as f:
            line = f.readline()
            label,tokens = line.split("\t")
            label = torch.tensor([float(label)],dtype = torch.float)
            feature = torch.tensor([int(x) for x in tokens.split(" ")],dtype = torch.long)
            return  (feature,label)
```

```python
ds_train = imdbDataset(train_samples_path)
ds_test = imdbDataset(test_samples_path)
```



```python
print(len(ds_train))
print(len(ds_test))
```

```
20000
5000
```

```python
dl_train = DataLoader(ds_train,batch_size = BATCH_SIZE,shuffle = True,num_workers=4)
dl_test = DataLoader(ds_test,batch_size = BATCH_SIZE,num_workers=4)

for features,labels in dl_train:
    print(features)
    print(labels)
    break
```

```
tensor([[   1,    1,    1,  ...,   29,    8,    8],
        [  13,   11,  247,  ...,    0,    0,    8],
        [8587,  555,   12,  ...,    3,    0,    8],
        ...,
        [   1,    1,    1,  ...,    2,    0,    8],
        [ 618,   62,   25,  ...,   20,  204,    8],
        [   1,    1,    1,  ...,   71,   85,    8]])
tensor([[1.],
        [0.],
        [0.],
        [1.],
        [0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [0.],
        [0.],
        [0.],
        [1.],
        [0.],
        [1.],
        [1.],
        [1.],
        [0.],
        [1.]])
```


æœ€åæ„å»ºæ¨¡å‹æµ‹è¯•ä¸€ä¸‹æ•°æ®é›†ç®¡é“æ˜¯å¦å¯ç”¨ã€‚

```python
import torch
from torch import nn 
import importlib 
from torchkeras import Model,summary

class Net(Model):
    
    def __init__(self):
        super(Net, self).__init__()
        
        #è®¾ç½®padding_idxå‚æ•°åå°†åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å°†å¡«å……çš„tokenå§‹ç»ˆèµ‹å€¼ä¸º0å‘é‡
        self.embedding = nn.Embedding(num_embeddings = MAX_WORDS,
                                      embedding_dim = 3,
                                      padding_idx = 1)
        self.conv = nn.Sequential()
        self.conv.add_module("conv_1",
                             nn.Conv1d(in_channels = 3,out_channels = 16,kernel_size = 5))
        self.conv.add_module("pool_1",
                             nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_1",
                             nn.ReLU())
        self.conv.add_module("conv_2",
                             nn.Conv1d(in_channels = 16,out_channels = 128,kernel_size = 2))
        self.conv.add_module("pool_2",
                             nn.MaxPool1d(kernel_size = 2))
        self.conv.add_module("relu_2",
                             nn.ReLU())
        
        self.dense = nn.Sequential()
        self.dense.add_module("flatten",nn.Flatten())
        self.dense.add_module("linear",nn.Linear(6144,1))
        self.dense.add_module("sigmoid",nn.Sigmoid())
        
    def forward(self,x):
        x = self.embedding(x).transpose(1,2)
        x = self.conv(x)
        y = self.dense(x)
        return y
        
model = Net()
print(model)

model.summary(input_shape = (200,),input_dtype = torch.LongTensor)
```

```
Net(
  (embedding): Embedding(10000, 3, padding_idx=1)
  (conv): Sequential(
    (conv_1): Conv1d(3, 16, kernel_size=(5,), stride=(1,))
    (pool_1): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_1): ReLU()
    (conv_2): Conv1d(16, 128, kernel_size=(2,), stride=(1,))
    (pool_2): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu_2): ReLU()
  )
  (dense): Sequential(
    (flatten): Flatten()
    (linear): Linear(in_features=6144, out_features=1, bias=True)
    (sigmoid): Sigmoid()
  )
)
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
         Embedding-1               [-1, 200, 3]          30,000
            Conv1d-2              [-1, 16, 196]             256
         MaxPool1d-3               [-1, 16, 98]               0
              ReLU-4               [-1, 16, 98]               0
            Conv1d-5              [-1, 128, 97]           4,224
         MaxPool1d-6              [-1, 128, 48]               0
              ReLU-7              [-1, 128, 48]               0
           Flatten-8                 [-1, 6144]               0
            Linear-9                    [-1, 1]           6,145
          Sigmoid-10                    [-1, 1]               0
================================================================
Total params: 40,625
Trainable params: 40,625
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000763
Forward/backward pass size (MB): 0.287796
Params size (MB): 0.154972
Estimated Total Size (MB): 0.443531
----------------------------------------------------------------
```

```python
# ç¼–è¯‘æ¨¡å‹
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,
                         torch.ones_like(y_pred,dtype = torch.float32),
                         torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1-torch.abs(y_true-y_pred))
    return acc

model.compile(loss_func = nn.BCELoss(),
              optimizer= torch.optim.Adagrad(model.parameters(),lr = 0.02),
              metrics_dict={"accuracy":accuracy})
```

```python
# è®­ç»ƒæ¨¡å‹
dfhistory = model.fit(10,dl_train,dl_val=dl_test,log_step_freq= 200)
```

```
Start Training ...

================================================================================2020-07-11 23:21:53
{'step': 200, 'loss': 0.956, 'accuracy': 0.521}
{'step': 400, 'loss': 0.823, 'accuracy': 0.53}
{'step': 600, 'loss': 0.774, 'accuracy': 0.545}
{'step': 800, 'loss': 0.747, 'accuracy': 0.56}
{'step': 1000, 'loss': 0.726, 'accuracy': 0.572}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.726 |  0.572   |  0.661   |    0.613     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:22:20
{'step': 200, 'loss': 0.605, 'accuracy': 0.668}
{'step': 400, 'loss': 0.602, 'accuracy': 0.674}
{'step': 600, 'loss': 0.592, 'accuracy': 0.681}
{'step': 800, 'loss': 0.584, 'accuracy': 0.687}
{'step': 1000, 'loss': 0.575, 'accuracy': 0.696}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.575 |  0.696   |  0.553   |    0.716     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:25:53
{'step': 200, 'loss': 0.294, 'accuracy': 0.877}
{'step': 400, 'loss': 0.299, 'accuracy': 0.875}
{'step': 600, 'loss': 0.298, 'accuracy': 0.875}
{'step': 800, 'loss': 0.296, 'accuracy': 0.876}
{'step': 1000, 'loss': 0.298, 'accuracy': 0.875}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   10  | 0.298 |  0.875   |  0.464   |    0.795     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:26:19
Finished Training...
```



### ä½¿ç”¨DataLoaderåŠ è½½æ•°æ®é›†


DataLoaderèƒ½å¤Ÿæ§åˆ¶batchçš„å¤§å°ï¼Œbatchä¸­å…ƒç´ çš„é‡‡æ ·æ–¹æ³•ï¼Œä»¥åŠå°†batchç»“æœæ•´ç†æˆæ¨¡å‹æ‰€éœ€è¾“å…¥å½¢å¼çš„æ–¹æ³•ï¼Œå¹¶ä¸”èƒ½å¤Ÿä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ã€‚

DataLoaderçš„å‡½æ•°ç­¾åå¦‚ä¸‹ã€‚

```python
DataLoader(
    dataset,
    batch_size=1,
    shuffle=False,
    sampler=None,
    batch_sampler=None,
    num_workers=0,
    collate_fn=None,
    pin_memory=False,
    drop_last=False,
    timeout=0,
    worker_init_fn=None,
    multiprocessing_context=None,
)
```

ä¸€èˆ¬æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬ä»…ä»…ä¼šé…ç½® dataset, batch_size, shuffle, num_workers, drop_lastè¿™äº”ä¸ªå‚æ•°ï¼Œå…¶ä»–å‚æ•°ä½¿ç”¨é»˜è®¤å€¼å³å¯ã€‚

DataLoaderé™¤äº†å¯ä»¥åŠ è½½æˆ‘ä»¬å‰é¢è®²çš„ torch.utils.data.Dataset å¤–ï¼Œè¿˜èƒ½å¤ŸåŠ è½½å¦å¤–ä¸€ç§æ•°æ®é›† torch.utils.data.IterableDatasetã€‚

å’ŒDatasetæ•°æ®é›†ç›¸å½“äºä¸€ç§åˆ—è¡¨ç»“æ„ä¸åŒï¼ŒIterableDatasetç›¸å½“äºä¸€ç§è¿­ä»£å™¨ç»“æ„ã€‚ å®ƒæ›´åŠ å¤æ‚ï¼Œä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

- dataset : æ•°æ®é›†
- batch_size: æ‰¹æ¬¡å¤§å°
- shuffle: æ˜¯å¦ä¹±åº
- sampler: æ ·æœ¬é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- batch_sampler: æ‰¹æ¬¡é‡‡æ ·å‡½æ•°ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- num_workers: ä½¿ç”¨å¤šè¿›ç¨‹è¯»å–æ•°æ®ï¼Œè®¾ç½®çš„è¿›ç¨‹æ•°ã€‚
- collate_fn: æ•´ç†ä¸€ä¸ªæ‰¹æ¬¡æ•°æ®çš„å‡½æ•°ã€‚
- pin_memory: æ˜¯å¦è®¾ç½®ä¸ºé”ä¸šå†…å­˜ã€‚é»˜è®¤ä¸ºFalseï¼Œé”ä¸šå†…å­˜ä¸ä¼šä½¿ç”¨è™šæ‹Ÿå†…å­˜(ç¡¬ç›˜)ï¼Œä»é”ä¸šå†…å­˜æ‹·è´åˆ°GPUä¸Šé€Ÿåº¦ä¼šæ›´å¿«ã€‚
- drop_last: æ˜¯å¦ä¸¢å¼ƒæœ€åä¸€ä¸ªæ ·æœ¬æ•°é‡ä¸è¶³batch_sizeæ‰¹æ¬¡æ•°æ®ã€‚
- timeout: åŠ è½½ä¸€ä¸ªæ•°æ®æ‰¹æ¬¡çš„æœ€é•¿ç­‰å¾…æ—¶é—´ï¼Œä¸€èˆ¬æ— éœ€è®¾ç½®ã€‚
- worker_init_fn: æ¯ä¸ªworkerä¸­datasetçš„åˆå§‹åŒ–å‡½æ•°ï¼Œå¸¸ç”¨äº IterableDatasetã€‚ä¸€èˆ¬ä¸ä½¿ç”¨ã€‚



```python
#æ„å»ºè¾“å…¥æ•°æ®ç®¡é“
ds = TensorDataset(torch.arange(1,50))
dl = DataLoader(ds,
                batch_size = 10,
                shuffle= True,
                num_workers=2,
                drop_last = True)
#è¿­ä»£æ•°æ®
for batch, in dl:
    print(batch)
```

```
tensor([43, 44, 21, 36,  9,  5, 28, 16, 20, 14])
tensor([23, 49, 35, 38,  2, 34, 45, 18, 15, 40])
tensor([26,  6, 27, 39,  8,  4, 24, 19, 32, 17])
tensor([ 1, 29, 11, 47, 12, 22, 48, 42, 10,  7])
```



## æ¨¡å‹å±‚layers

æ·±åº¦å­¦ä¹ æ¨¡å‹ä¸€èˆ¬ç”±å„ç§æ¨¡å‹å±‚ç»„åˆè€Œæˆã€‚

torch.nnä¸­å†…ç½®äº†éå¸¸ä¸°å¯Œçš„å„ç§æ¨¡å‹å±‚ã€‚å®ƒä»¬éƒ½å±äºnn.Moduleçš„å­ç±»ï¼Œå…·å¤‡å‚æ•°ç®¡ç†åŠŸèƒ½ã€‚

ä¾‹å¦‚ï¼š

* nn.Linear
* nn.Flatten
* nn.Dropout
* nn.BatchNorm2d
* nn.Conv2d
* nn.AvgPool2d
* nn.Conv1d
* nn.ConvTranspose2d
* nn.Embedding
* nn.GRU
* nn.LSTM
* nn.Transformer

å¦‚æœè¿™äº›å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚

å®é™…ä¸Šï¼Œpytorchä¸åŒºåˆ†æ¨¡å‹å’Œæ¨¡å‹å±‚ï¼Œéƒ½æ˜¯é€šè¿‡ç»§æ‰¿nn.Moduleè¿›è¡Œæ„å»ºã€‚

å› æ­¤ï¼Œæˆ‘ä»¬åªè¦ç»§æ‰¿nn.ModuleåŸºç±»å¹¶å®ç°forwardæ–¹æ³•å³å¯è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚



### å†…ç½®æ¨¡å‹å±‚

```python
import numpy as np 
import torch 
from torch import nn 
```


ä¸€äº›å¸¸ç”¨çš„å†…ç½®æ¨¡å‹å±‚ç®€å•ä»‹ç»å¦‚ä¸‹ã€‚

#### **åŸºç¡€å±‚**

##### nn.Linear

**å…¨è¿æ¥å±‚**ã€‚å‚æ•°ä¸ªæ•° = è¾“å…¥å±‚ç‰¹å¾æ•°Ã— è¾“å‡ºå±‚ç‰¹å¾æ•°(weight)ï¼‹ è¾“å‡ºå±‚ç‰¹å¾æ•°(bias)

##### nn.Flatten

**å‹å¹³å±‚**ï¼Œç”¨äºå°†å¤šç»´å¼ é‡æ ·æœ¬å‹æˆä¸€ç»´å¼ é‡æ ·æœ¬ã€‚

##### nn.BatchNorm1d

**ä¸€ç»´æ‰¹æ ‡å‡†åŒ–å±‚**ã€‚é€šè¿‡çº¿æ€§å˜æ¢å°†è¾“å…¥æ‰¹æ¬¡ç¼©æ”¾å¹³ç§»åˆ°ç¨³å®šçš„å‡å€¼å’Œæ ‡å‡†å·®ã€‚å¯ä»¥å¢å¼ºæ¨¡å‹å¯¹è¾“å…¥ä¸åŒåˆ†å¸ƒçš„é€‚åº”æ€§ï¼ŒåŠ å¿«æ¨¡å‹è®­ç»ƒé€Ÿåº¦ï¼Œæœ‰è½»å¾®æ­£åˆ™åŒ–æ•ˆæœã€‚ä¸€èˆ¬åœ¨æ¿€æ´»å‡½æ•°ä¹‹å‰ä½¿ç”¨ã€‚å¯ä»¥ç”¨afineå‚æ•°è®¾ç½®è¯¥å±‚æ˜¯å¦å«æœ‰å¯ä»¥è®­ç»ƒçš„å‚æ•°ã€‚

##### nn.BatchNorm2d

**äºŒç»´æ‰¹æ ‡å‡†åŒ–å±‚**ã€‚

##### nn.BatchNorm3d

**ä¸‰ç»´æ‰¹æ ‡å‡†åŒ–å±‚**ã€‚

##### nn.Dropout

**ä¸€ç»´éšæœºä¸¢å¼ƒå±‚**ã€‚ä¸€ç§æ­£åˆ™åŒ–æ‰‹æ®µã€‚

##### nn.Dropout2d

**äºŒç»´éšæœºä¸¢å¼ƒå±‚**ã€‚

##### nn.Dropout3d

**ä¸‰ç»´éšæœºä¸¢å¼ƒå±‚**ã€‚

##### nn.Threshold

**é™å¹…å±‚**ã€‚å½“è¾“å…¥å¤§äºæˆ–å°äºé˜ˆå€¼èŒƒå›´æ—¶ï¼Œæˆªæ–­ä¹‹ã€‚

##### nn.ConstantPad2d

 **äºŒç»´å¸¸æ•°å¡«å……å±‚**ã€‚å¯¹äºŒç»´å¼ é‡æ ·æœ¬å¡«å……å¸¸æ•°æ‰©å±•é•¿åº¦ã€‚

##### nn.ReplicationPad1d

 **ä¸€ç»´å¤åˆ¶å¡«å……å±‚**ã€‚å¯¹ä¸€ç»´å¼ é‡æ ·æœ¬é€šè¿‡å¤åˆ¶è¾¹ç¼˜å€¼å¡«å……æ‰©å±•é•¿åº¦ã€‚

##### nn.ZeroPad2d

**äºŒç»´é›¶å€¼å¡«å……å±‚**ã€‚å¯¹äºŒç»´å¼ é‡æ ·æœ¬åœ¨è¾¹ç¼˜å¡«å……0å€¼.

##### nn.GroupNorm

**ç»„å½’ä¸€åŒ–**ã€‚ä¸€ç§æ›¿ä»£æ‰¹å½’ä¸€åŒ–çš„æ–¹æ³•ï¼Œå°†é€šé“åˆ†æˆè‹¥å¹²ç»„è¿›è¡Œå½’ä¸€ã€‚ä¸å—batchå¤§å°é™åˆ¶ï¼Œæ®ç§°æ€§èƒ½å’Œæ•ˆæœéƒ½ä¼˜äºBatchNormã€‚

##### nn.LayerNorm

**å±‚å½’ä¸€åŒ–**ã€‚è¾ƒå°‘ä½¿ç”¨ã€‚

##### nn.InstanceNorm2d

**æ ·æœ¬å½’ä¸€åŒ–**ã€‚è¾ƒå°‘ä½¿ç”¨ã€‚



å„ç§å½’ä¸€åŒ–æŠ€æœ¯å‚è€ƒå¦‚ä¸‹çŸ¥ä¹æ–‡ç« ã€ŠFAIRä½•æºæ˜ç­‰äººæå‡ºç»„å½’ä¸€åŒ–ï¼šæ›¿ä»£æ‰¹å½’ä¸€åŒ–ï¼Œä¸å—æ‰¹é‡å¤§å°é™åˆ¶ã€‹

https://zhuanlan.zhihu.com/p/34858971

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-653372.png)



#### **å·ç§¯ç½‘ç»œç›¸å…³å±‚**

##### nn.Conv1d

**æ™®é€šä¸€ç»´å·ç§¯**ï¼Œå¸¸ç”¨äºæ–‡æœ¬ã€‚**å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ï¼‰**

##### nn.Conv2d

**æ™®é€šäºŒç»´å·ç§¯**ï¼Œå¸¸ç”¨äºå›¾åƒã€‚**å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3)**

* é€šè¿‡è°ƒæ•´dilationå‚æ•°å¤§äº1ï¼Œå¯ä»¥å˜æˆç©ºæ´å·ç§¯ï¼Œå¢å¤§å·ç§¯æ ¸æ„Ÿå—é‡ã€‚
* é€šè¿‡è°ƒæ•´groupså‚æ•°ä¸ä¸º1ï¼Œå¯ä»¥å˜æˆåˆ†ç»„å·ç§¯ã€‚åˆ†ç»„å·ç§¯ä¸­ä¸åŒåˆ†ç»„ä½¿ç”¨ç›¸åŒçš„å·ç§¯æ ¸ï¼Œæ˜¾è‘—å‡å°‘å‚æ•°æ•°é‡ã€‚
* å½“groupså‚æ•°ç­‰äºé€šé“æ•°æ—¶ï¼Œç›¸å½“äºtensorflowä¸­çš„äºŒç»´æ·±åº¦å·ç§¯å±‚tf.keras.layers.DepthwiseConv2Dã€‚
	åˆ©ç”¨åˆ†ç»„å·ç§¯å’Œ1ä¹˜1å·ç§¯çš„ç»„åˆæ“ä½œï¼Œå¯ä»¥æ„é€ ç›¸å½“äºKerasä¸­çš„äºŒç»´æ·±åº¦å¯åˆ†ç¦»å·ç§¯å±‚tf.keras.layers.SeparableConv2Dã€‚

##### nn.Conv3d

**æ™®é€šä¸‰ç»´å·ç§¯**ï¼Œå¸¸ç”¨äºè§†é¢‘ã€‚**å‚æ•°ä¸ªæ•° = è¾“å…¥é€šé“æ•°Ã—å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3)Ã—å·ç§¯æ ¸ä¸ªæ•° + å·ç§¯æ ¸å°ºå¯¸(å¦‚3ä¹˜3ä¹˜3)** ã€‚

##### nn.MaxPool1d

**ä¸€ç»´æœ€å¤§æ± åŒ–**ã€‚

##### nn.MaxPool2d

**äºŒç»´æœ€å¤§æ± åŒ–**ã€‚ä¸€ç§ä¸‹é‡‡æ ·æ–¹å¼ã€‚æ²¡æœ‰éœ€è¦è®­ç»ƒçš„å‚æ•°ã€‚

##### nn.MaxPool3d

**ä¸‰ç»´æœ€å¤§æ± åŒ–**ã€‚

##### nn.AdaptiveMaxPool2d

**äºŒç»´è‡ªé€‚åº”æœ€å¤§æ± åŒ–**ã€‚æ— è®ºè¾“å…¥å›¾åƒçš„å°ºå¯¸å¦‚ä½•å˜åŒ–ï¼Œè¾“å‡ºçš„å›¾åƒå°ºå¯¸æ˜¯å›ºå®šçš„ã€‚
è¯¥å‡½æ•°çš„å®ç°åŸç†ï¼Œå¤§æ¦‚æ˜¯é€šè¿‡è¾“å…¥å›¾åƒçš„å°ºå¯¸å’Œè¦å¾—åˆ°çš„è¾“å‡ºå›¾åƒçš„å°ºå¯¸æ¥åå‘æ¨ç®—æ± åŒ–ç®—å­çš„padding,strideç­‰å‚æ•°ã€‚

##### nn.FractionalMaxPool2d

**äºŒç»´åˆ†æ•°æœ€å¤§æ± åŒ–**ã€‚æ™®é€šæœ€å¤§æ± åŒ–é€šå¸¸è¾“å…¥å°ºå¯¸æ˜¯è¾“å‡ºçš„æ•´æ•°å€ã€‚è€Œåˆ†æ•°æœ€å¤§æ± åŒ–åˆ™å¯ä»¥ä¸å¿…æ˜¯æ•´æ•°ã€‚åˆ†æ•°æœ€å¤§æ± åŒ–ä½¿ç”¨äº†ä¸€äº›éšæœºé‡‡æ ·ç­–ç•¥ï¼Œæœ‰ä¸€å®šçš„æ­£åˆ™æ•ˆæœï¼Œå¯ä»¥ç”¨å®ƒæ¥ä»£æ›¿æ™®é€šæœ€å¤§æ± åŒ–å’ŒDropoutå±‚ã€‚

##### nn.AvgPool2d

**äºŒç»´å¹³å‡æ± åŒ–**ã€‚

##### nn.AdaptiveAvgPool2d

**äºŒç»´è‡ªé€‚åº”å¹³å‡æ± åŒ–**ã€‚æ— è®ºè¾“å…¥çš„ç»´åº¦å¦‚ä½•å˜åŒ–ï¼Œè¾“å‡ºçš„ç»´åº¦æ˜¯å›ºå®šçš„ã€‚

##### nn.ConvTranspose2d

**äºŒç»´å·ç§¯è½¬ç½®å±‚ï¼Œä¿—ç§°åå·ç§¯å±‚**ã€‚å¹¶éå·ç§¯çš„é€†æ“ä½œï¼Œä½†åœ¨å·ç§¯æ ¸ç›¸åŒçš„æƒ…å†µä¸‹ï¼Œå½“å…¶è¾“å…¥å°ºå¯¸æ˜¯å·ç§¯æ“ä½œè¾“å‡ºå°ºå¯¸çš„æƒ…å†µä¸‹ï¼Œå·ç§¯è½¬ç½®çš„è¾“å‡ºå°ºå¯¸æ°å¥½æ˜¯å·ç§¯æ“ä½œçš„è¾“å…¥å°ºå¯¸ã€‚åœ¨è¯­ä¹‰åˆ†å‰²ä¸­å¯ç”¨äºä¸Šé‡‡æ ·ã€‚

##### nn.Upsample

**ä¸Šé‡‡æ ·å±‚ï¼Œæ“ä½œæ•ˆæœå’Œæ± åŒ–ç›¸å**ã€‚å¯ä»¥é€šè¿‡modeå‚æ•°æ§åˆ¶ä¸Šé‡‡æ ·ç­–ç•¥ä¸º"nearest"æœ€é‚»è¿‘ç­–ç•¥æˆ–"linear"çº¿æ€§æ’å€¼ç­–ç•¥ã€‚

##### nn.Unfold

**æ»‘åŠ¨çª—å£æå–å±‚**ã€‚å…¶å‚æ•°å’Œå·ç§¯æ“ä½œnn.Conv2dç›¸åŒã€‚å®é™…ä¸Šï¼Œå·ç§¯æ“ä½œå¯ä»¥ç­‰ä»·äºnn.Unfoldå’Œnn.Linearä»¥åŠnn.Foldçš„ä¸€ä¸ªç»„åˆã€‚

* å…¶ä¸­nn.Unfoldæ“ä½œå¯ä»¥ä»è¾“å…¥ä¸­æå–å„ä¸ªæ»‘åŠ¨çª—å£çš„æ•°å€¼çŸ©é˜µï¼Œå¹¶å°†å…¶å‹å¹³æˆä¸€ç»´ã€‚åˆ©ç”¨nn.Linearå°†nn.Unfoldçš„è¾“å‡ºå’Œå·ç§¯æ ¸åšä¹˜æ³•åï¼Œå†ä½¿ç”¨nn.Foldæ“ä½œå°†ç»“æœè½¬æ¢æˆè¾“å‡ºå›¾ç‰‡å½¢çŠ¶ã€‚

##### nn.Fold

**é€†æ»‘åŠ¨çª—å£æå–å±‚**ã€‚



#### **å¾ªç¯ç½‘ç»œç›¸å…³å±‚**

##### nn.Embedding

**åµŒå…¥å±‚**ã€‚ä¸€ç§æ¯”Onehotæ›´åŠ æœ‰æ•ˆçš„å¯¹ç¦»æ•£ç‰¹å¾è¿›è¡Œç¼–ç çš„æ–¹æ³•ã€‚ä¸€èˆ¬ç”¨äºå°†è¾“å…¥ä¸­çš„å•è¯æ˜ å°„ä¸ºç¨ å¯†å‘é‡ã€‚åµŒå…¥å±‚çš„å‚æ•°éœ€è¦å­¦ä¹ ã€‚

torch.nn.Embedding(num_embeddings, embedding_dim)çš„æ„æ€æ˜¯åˆ›å»ºä¸€ä¸ªè¯åµŒå…¥æ¨¡å‹ï¼Œnum_embeddingsä»£è¡¨ä¸€å…±æœ‰å¤šå°‘ä¸ªè¯, embedding_dimä»£è¡¨ä½ æƒ³è¦ä¸ºæ¯ä¸ªè¯åˆ›å»ºä¸€ä¸ªå¤šå°‘ç»´çš„å‘é‡æ¥è¡¨ç¤ºå®ƒï¼Œå¦‚ä¸‹é¢çš„ä¾‹å­ã€‚

```python
import torch
from torch import nn

embedding = nn.Embedding(5, 4) # å‡å®šå­—å…¸ä¸­åªæœ‰5ä¸ªè¯ï¼Œè¯å‘é‡ç»´åº¦ä¸º4
word = [[1, 2, 3],
        [2, 3, 4]] 
# æ¯ä¸ªæ•°å­—ä»£è¡¨ä¸€ä¸ªè¯ï¼Œä¾‹å¦‚ {'!':0,'how':1, 'are':2, 'you':3,  'ok':4}
# è€Œä¸”è¿™äº›æ•°å­—çš„èŒƒå›´åªèƒ½åœ¨0ï½4ä¹‹é—´ï¼Œå› ä¸ºä¸Šé¢å®šä¹‰äº†åªæœ‰5ä¸ªè¯

embed = embedding(torch.LongTensor(word))
print(embed) 
print(embed.size())
```

è¾“å‡ºï¼š

```
tensor([[[-0.4093, -1.0110,  0.6731,  0.0790],
         [-0.6557, -0.9846, -0.1647,  2.2633],
         [-0.5706, -1.1936, -0.2704,  0.0708]],

        [[-0.6557, -0.9846, -0.1647,  2.2633],
         [-0.5706, -1.1936, -0.2704,  0.0708],
         [ 0.2242, -0.5989,  0.4237,  2.2405]]], grad_fn=<EmbeddingBackward>)
         
torch.Size([2, 3, 4])
```

embedè¾“å‡ºçš„ç»´åº¦æ˜¯[2,3,4],è¿™å°±ä»£è¡¨å¯¹äºè¾“å…¥ç»´åº¦ä¸º2x3çš„è¯ï¼Œæ¯ä¸ªè¯éƒ½è¢«æ˜ å°„æˆäº†ä¸€ä¸ª4ç»´çš„å‘é‡ã€‚

##### nn.LSTM

**é•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘**ã€‚æœ€æ™®éä½¿ç”¨çš„å¾ªç¯ç½‘ç»œå±‚ã€‚å…·æœ‰æºå¸¦è½¨é“ï¼Œé—å¿˜é—¨ï¼Œæ›´æ–°é—¨ï¼Œè¾“å‡ºé—¨ã€‚å¯ä»¥è¾ƒä¸ºæœ‰æ•ˆåœ°ç¼“è§£æ¢¯åº¦æ¶ˆå¤±é—®é¢˜ï¼Œä»è€Œèƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚è®¾ç½®bidirectional = Trueæ—¶å¯ä»¥å¾—åˆ°åŒå‘LSTMã€‚éœ€è¦æ³¨æ„çš„æ—¶ï¼Œé»˜è®¤çš„è¾“å…¥å’Œè¾“å‡ºå½¢çŠ¶æ˜¯(seq,batch,feature), å¦‚æœéœ€è¦å°†batchç»´åº¦æ”¾åœ¨ç¬¬0ç»´ï¼Œåˆ™è¦è®¾ç½®batch_firstå‚æ•°è®¾ç½®ä¸ºTrueã€‚

**è¾“å…¥çš„å‚æ•°åˆ—è¡¨åŒ…æ‹¬:**

- input_size è¾“å…¥æ•°æ®çš„ç‰¹å¾ç»´æ•°ï¼Œé€šå¸¸å°±æ˜¯embedding_dim(è¯å‘é‡çš„ç»´åº¦)

- hidden_sizeã€€LSTMä¸­éšå±‚çš„ç»´åº¦

- num_layersã€€å¾ªç¯ç¥ç»ç½‘ç»œçš„å±‚æ•°

- biasã€€ç”¨ä¸ç”¨åç½®ï¼Œdefault=True

- batch_first è¿™ä¸ªè¦æ³¨æ„ï¼Œé€šå¸¸æˆ‘ä»¬è¾“å…¥çš„æ•°æ®shape=(batch_size,seq_length,embedding_dim),è€Œbatch_firsté»˜è®¤æ˜¯False,æ‰€ä»¥æˆ‘ä»¬çš„è¾“å…¥æ•°æ®æœ€å¥½é€è¿›LSTMä¹‹å‰å°†batch_sizeä¸seq_lengthè¿™ä¸¤ä¸ªç»´åº¦è°ƒæ¢

- dropoutã€€é»˜è®¤æ˜¯0ï¼Œä»£è¡¨ä¸ç”¨dropout

- bidirectionalé»˜è®¤æ˜¯falseï¼Œä»£è¡¨ä¸ç”¨åŒå‘LSTM

    

**è¾“å…¥æ•°æ®åŒ…æ‹¬input, (h_0,c_0):**

- inputå°±æ˜¯shape=(seq_length, batch_size, input_size)çš„å¼ é‡

- h_0æ˜¯shape=(num_layers\*num_directions, batch_size, hidden_size)çš„å¼ é‡ï¼Œå®ƒåŒ…å«äº†åœ¨å½“å‰è¿™ä¸ªbatch_sizeä¸­æ¯ä¸ªå¥å­çš„åˆå§‹éšè—çŠ¶æ€ã€‚å…¶ä¸­num_layerså°±æ˜¯LSTMçš„å±‚æ•°ã€‚å¦‚æœbidirectional=True, num_directions=2, å¦åˆ™å°±æ˜¯ï¼‘ï¼Œè¡¨ç¤ºåªæœ‰ä¸€ä¸ªæ–¹å‘ã€‚

- c_0å’Œh_0çš„å½¢çŠ¶ç›¸åŒï¼Œå®ƒåŒ…å«çš„æ˜¯åœ¨å½“å‰è¿™ä¸ªbatch_sizeä¸­çš„æ¯ä¸ªå¥å­çš„åˆå§‹ç»†èƒçŠ¶æ€ã€‚h_0, c_0å¦‚æœä¸æä¾›ï¼Œé‚£ä¹ˆé»˜è®¤æ˜¯ï¼ã€‚

    

**è¾“å‡ºæ•°æ®åŒ…æ‹¬output,(h_n,c_n):**

- outputçš„shape=(seq_length, batch_size, num_directions*hidden_size),å®ƒåŒ…å«çš„æ˜¯LSTMçš„æœ€åä¸€æ—¶é—´æ­¥çš„è¾“å‡ºç‰¹å¾(h_t),ï½”æ˜¯batch_sizeä¸­æ¯ä¸ªå¥å­çš„é•¿åº¦ã€‚
- h_n.shape==(num_directions * num_layers, batch, hidden_size)
- c_n.shape==h_n.shape
- h_nåŒ…å«çš„æ˜¯å¥å­çš„æœ€åä¸€ä¸ªå•è¯ï¼ˆä¹Ÿå°±æ˜¯æœ€åä¸€ä¸ªæ—¶é—´æ­¥ï¼‰çš„éšè—çŠ¶æ€ï¼Œc_nåŒ…å«çš„æ˜¯å¥å­çš„æœ€åä¸€ä¸ªå•è¯çš„ç»†èƒçŠ¶æ€ï¼Œæ‰€ä»¥å®ƒä»¬éƒ½ä¸å¥å­çš„é•¿åº¦seq_lengthæ— å…³ã€‚
- output[-1]ä¸h_næ˜¯ç›¸ç­‰çš„ï¼Œå› ä¸ºoutput[-1]åŒ…å«çš„æ­£æ˜¯batch_sizeä¸ªå¥å­ä¸­æ¯ä¸€ä¸ªå¥å­çš„æœ€åä¸€ä¸ªå•è¯çš„éšè—çŠ¶æ€ï¼Œæ³¨æ„LSTMä¸­çš„éšè—çŠ¶æ€å…¶å®å°±æ˜¯è¾“å‡ºï¼Œcell stateç»†èƒçŠ¶æ€æ‰æ˜¯LSTMä¸­ä¸€ç›´éšè—çš„ï¼Œè®°å½•ç€ä¿¡æ¯.



##### nn.GRU

**é—¨æ§å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘**ã€‚LSTMçš„ä½é…ç‰ˆï¼Œä¸å…·æœ‰æºå¸¦è½¨é“ï¼Œå‚æ•°æ•°é‡å°‘äºLSTMï¼Œè®­ç»ƒé€Ÿåº¦æ›´å¿«ã€‚

##### nn.RNN

**ç®€å•å¾ªç¯ç½‘ç»œå±‚ã€æ”¯æŒå¤šå±‚ã€‘**ã€‚å®¹æ˜“å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±ï¼Œä¸èƒ½å¤Ÿé€‚ç”¨é•¿æœŸä¾èµ–é—®é¢˜ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

##### nn.LSTMCell

**é•¿çŸ­è®°å¿†å¾ªç¯ç½‘ç»œå•å…ƒ**ã€‚å’Œnn.LSTMåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

##### nn.GRUCell

**é—¨æ§å¾ªç¯ç½‘ç»œå•å…ƒ**ã€‚å’Œnn.GRUåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚

##### nn.RNNCell

**ç®€å•å¾ªç¯ç½‘ç»œå•å…ƒ**ã€‚å’Œnn.RNNåœ¨æ•´ä¸ªåºåˆ—ä¸Šè¿­ä»£ç›¸æ¯”ï¼Œå®ƒä»…åœ¨åºåˆ—ä¸Šè¿­ä»£ä¸€æ­¥ã€‚ä¸€èˆ¬è¾ƒå°‘ä½¿ç”¨ã€‚



#### **Transformerç›¸å…³å±‚**

##### nn.Transformer

**Transformerç½‘ç»œç»“æ„**ã€‚Transformerç½‘ç»œç»“æ„æ˜¯æ›¿ä»£å¾ªç¯ç½‘ç»œçš„ä¸€ç§ç»“æ„ï¼Œè§£å†³äº†å¾ªç¯ç½‘ç»œéš¾ä»¥å¹¶è¡Œï¼Œéš¾ä»¥æ•æ‰é•¿æœŸä¾èµ–çš„ç¼ºé™·ã€‚å®ƒæ˜¯ç›®å‰NLPä»»åŠ¡çš„ä¸»æµæ¨¡å‹çš„ä¸»è¦æ„æˆéƒ¨åˆ†ã€‚Transformerç½‘ç»œç»“æ„ç”±TransformerEncoderç¼–ç å™¨å’ŒTransformerDecoderè§£ç å™¨ç»„æˆã€‚ç¼–ç å™¨å’Œè§£ç å™¨çš„æ ¸å¿ƒæ˜¯MultiheadAttentionå¤šå¤´æ³¨æ„åŠ›å±‚ã€‚

##### nn.TransformerEncoder

**Transformerç¼–ç å™¨ç»“æ„**ã€‚ç”±å¤šä¸ª nn.TransformerEncoderLayerç¼–ç å™¨å±‚ç»„æˆã€‚

##### nn.TransformerDecoder

Transformerè§£ç å™¨ç»“æ„ã€‚ç”±å¤šä¸ª nn.TransformerDecoderLayerè§£ç å™¨å±‚ç»„æˆã€‚

##### nn.TransformerEncoderLayer

**Transformerçš„ç¼–ç å™¨å±‚**ã€‚

##### nn.TransformerDecoderLayer

**Transformerçš„è§£ç å™¨å±‚**ã€‚

##### nn.MultiheadAttention

**å¤šå¤´æ³¨æ„åŠ›å±‚**ã€‚

TransformeråŸç†ä»‹ç»å¯ä»¥å‚è€ƒå¦‚ä¸‹çŸ¥ä¹æ–‡ç« ã€Šè¯¦è§£Transformer(Attention Is All You Need)ã€‹

https://zhuanlan.zhihu.com/p/48508221

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-648757.jpg)



### è‡ªå®šä¹‰æ¨¡å‹å±‚


å¦‚æœPytorchçš„å†…ç½®æ¨¡å‹å±‚ä¸èƒ½å¤Ÿæ»¡è¶³éœ€æ±‚ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥é€šè¿‡ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰çš„æ¨¡å‹å±‚ã€‚

å®é™…ä¸Šï¼Œpytorchä¸åŒºåˆ†æ¨¡å‹å’Œæ¨¡å‹å±‚ï¼Œéƒ½æ˜¯é€šè¿‡ç»§æ‰¿nn.Moduleè¿›è¡Œæ„å»ºã€‚

å› æ­¤ï¼Œæˆ‘ä»¬åªè¦ç»§æ‰¿nn.ModuleåŸºç±»å¹¶å®ç°forwardæ–¹æ³•å³å¯è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚

ä¸‹é¢æ˜¯Pytorchçš„nn.Linearå±‚çš„æºç ï¼Œæˆ‘ä»¬å¯ä»¥ä»¿ç…§å®ƒæ¥è‡ªå®šä¹‰æ¨¡å‹å±‚ã€‚


```python
import torch
from torch import nn
import torch.nn.functional as F


class Linear(nn.Module):
    __constants__ = ['in_features', 'out_features']

    def __init__(self, in_features, out_features, bias=True):
        super(Linear, self).__init__()
        self.in_features = in_features
        self.out_features = out_features
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        nn.init.kaiming_uniform_(self.weight, a=math.sqrt(5))
        if self.bias is not None:
            fan_in, _ = nn.init._calculate_fan_in_and_fan_out(self.weight)
            bound = 1 / math.sqrt(fan_in)
            nn.init.uniform_(self.bias, -bound, bound)

    def forward(self, input):
        return F.linear(input, self.weight, self.bias)

    def extra_repr(self):
        return 'in_features={}, out_features={}, bias={}'.format(
            self.in_features, self.out_features, self.bias is not None
        )
```

```python
linear = nn.Linear(20, 30)
inputs = torch.randn(128, 20)
output = linear(inputs)
print(output.size())
```

```
torch.Size([128, 30])
```



## æŸå¤±å‡½æ•°losses

ä¸€èˆ¬æ¥è¯´ï¼Œç›‘ç£å­¦ä¹ çš„ç›®æ ‡å‡½æ•°ç”±æŸå¤±å‡½æ•°å’Œæ­£åˆ™åŒ–é¡¹ç»„æˆã€‚(Objective = Loss + Regularization)

Pytorchä¸­çš„æŸå¤±å‡½æ•°ä¸€èˆ¬åœ¨è®­ç»ƒæ¨¡å‹æ—¶å€™æŒ‡å®šã€‚

æ³¨æ„Pytorchä¸­å†…ç½®çš„æŸå¤±å‡½æ•°çš„å‚æ•°å’Œtensorflowä¸åŒï¼Œæ˜¯y_predåœ¨å‰ï¼Œy_trueåœ¨åï¼Œè€ŒTensorflowæ˜¯y_trueåœ¨å‰ï¼Œy_predåœ¨åã€‚

1. å¯¹äºå›å½’æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„å†…ç½®æŸå¤±å‡½æ•°æ˜¯**å‡æ–¹æŸå¤±å‡½æ•°**nn.MSELoss ã€‚
2. å¯¹äºäºŒåˆ†ç±»æ¨¡å‹ï¼Œé€šå¸¸ä½¿ç”¨çš„æ˜¯**äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°**nn.BCELoss (è¾“å…¥å·²ç»æ˜¯sigmoidæ¿€æ´»å‡½æ•°ä¹‹åçš„ç»“æœ) 
	æˆ–è€… nn.BCEWithLogitsLoss (è¾“å…¥å°šæœªç»è¿‡nn.Sigmoidæ¿€æ´»å‡½æ•°) ã€‚
3. å¯¹äºå¤šåˆ†ç±»æ¨¡å‹ï¼Œä¸€èˆ¬æ¨èä½¿ç”¨**äº¤å‰ç†µæŸå¤±å‡½æ•°** nn.CrossEntropyLossã€‚
	(y_trueéœ€è¦æ˜¯ä¸€ç»´çš„ï¼Œæ˜¯ç±»åˆ«ç¼–ç ã€‚y_predæœªç»è¿‡nn.Softmaxæ¿€æ´»ã€‚) 

æ­¤å¤–ï¼Œå¦‚æœå¤šåˆ†ç±»çš„y_predç»è¿‡äº†nn.LogSoftmaxæ¿€æ´»ï¼Œå¯ä»¥ä½¿ç”¨nn.NLLLossæŸå¤±å‡½æ•°(The negative log likelihood loss)ã€‚
è¿™ç§æ–¹æ³•å’Œç›´æ¥ä½¿ç”¨nn.CrossEntropyLossç­‰ä»·ã€‚


å¦‚æœæœ‰éœ€è¦ï¼Œä¹Ÿå¯ä»¥è‡ªå®šä¹‰æŸå¤±å‡½æ•°ï¼Œè‡ªå®šä¹‰æŸå¤±å‡½æ•°éœ€è¦æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_predï¼Œy_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

Pytorchä¸­çš„æ­£åˆ™åŒ–é¡¹ä¸€èˆ¬é€šè¿‡è‡ªå®šä¹‰çš„æ–¹å¼å’ŒæŸå¤±å‡½æ•°ä¸€èµ·æ·»åŠ ä½œä¸ºç›®æ ‡å‡½æ•°ã€‚

å¦‚æœä»…ä»…ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®ç°ç›¸åŒçš„æ•ˆæœã€‚



### å†…ç½®æŸå¤±å‡½æ•°

```python
import numpy as np
import pandas as pd
import torch 
from torch import nn 
import torch.nn.functional as F 


y_pred = torch.tensor([[10.0,0.0,-10.0],[8.0,8.0,8.0]])
y_true = torch.tensor([0,2])

# ç›´æ¥è°ƒç”¨äº¤å‰ç†µæŸå¤±
ce = nn.CrossEntropyLoss()(y_pred,y_true)
print(ce)

# ç­‰ä»·äºå…ˆè®¡ç®—nn.LogSoftmaxæ¿€æ´»ï¼Œå†è°ƒç”¨NLLLoss
y_pred_logsoftmax = nn.LogSoftmax(dim = 1)(y_pred)
nll = nn.NLLLoss()(y_pred_logsoftmax,y_true)
print(nll)

```

```
tensor(0.5493)
tensor(0.5493)
```



**å†…ç½®çš„æŸå¤±å‡½æ•°ä¸€èˆ¬æœ‰ç±»çš„å®ç°å’Œå‡½æ•°çš„å®ç°ä¸¤ç§å½¢å¼ã€‚**

å¦‚ï¼šnn.BCE å’Œ F.binary_cross_entropy éƒ½æ˜¯äºŒå…ƒäº¤å‰ç†µæŸå¤±å‡½æ•°ï¼Œå‰è€…æ˜¯ç±»çš„å®ç°å½¢å¼ï¼Œåè€…æ˜¯å‡½æ•°çš„å®ç°å½¢å¼ã€‚

å®é™…ä¸Šç±»çš„å®ç°å½¢å¼é€šå¸¸æ˜¯è°ƒç”¨å‡½æ•°çš„å®ç°å½¢å¼å¹¶ç”¨nn.Moduleå°è£…åå¾—åˆ°çš„ã€‚

ä¸€èˆ¬æˆ‘ä»¬å¸¸ç”¨çš„æ˜¯ç±»çš„å®ç°å½¢å¼ã€‚å®ƒä»¬å°è£…åœ¨torch.nnæ¨¡å—ä¸‹ï¼Œå¹¶ä¸”ç±»åä»¥Lossç»“å°¾ã€‚

å¸¸ç”¨çš„ä¸€äº›å†…ç½®æŸå¤±å‡½æ•°è¯´æ˜å¦‚ä¸‹ã€‚


* nn.MSELossï¼ˆ**å‡æ–¹è¯¯å·®æŸå¤±**ï¼Œä¹Ÿå«åšL2æŸå¤±ï¼Œç”¨äºå›å½’ï¼‰
* nn.L1Loss ï¼ˆ**L1æŸå¤±**ï¼Œä¹Ÿå«åšç»å¯¹å€¼è¯¯å·®æŸå¤±ï¼Œç”¨äºå›å½’ï¼‰
* nn.SmoothL1Loss (**å¹³æ»‘L1æŸå¤±**ï¼Œå½“è¾“å…¥åœ¨-1åˆ°1ä¹‹é—´æ—¶ï¼Œå¹³æ»‘ä¸ºL2æŸå¤±ï¼Œç”¨äºå›å½’)
* nn.BCELoss (**äºŒå…ƒäº¤å‰ç†µ**ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œè¾“å…¥å·²ç»è¿‡nn.Sigmoidæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)
* nn.BCEWithLogitsLoss (**äºŒå…ƒäº¤å‰ç†µ**ï¼Œç”¨äºäºŒåˆ†ç±»ï¼Œè¾“å…¥æœªç»è¿‡nn.Sigmoidæ¿€æ´»)
* nn.CrossEntropyLoss (**äº¤å‰ç†µ**ï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥æœªç»è¿‡nn.Softmaxæ¿€æ´»ï¼Œå¯¹ä¸å¹³è¡¡æ•°æ®é›†å¯ä»¥ç”¨weigthså‚æ•°è°ƒæ•´ç±»åˆ«æƒé‡)
* nn.NLLLoss (**è´Ÿå¯¹æ•°ä¼¼ç„¶æŸå¤±**ï¼Œç”¨äºå¤šåˆ†ç±»ï¼Œè¦æ±‚labelä¸ºç¨€ç–ç¼–ç ï¼Œè¾“å…¥ç»è¿‡nn.LogSoftmaxæ¿€æ´»)
* nn.CosineSimilarity(**ä½™å¼¦ç›¸ä¼¼åº¦**ï¼Œå¯ç”¨äºå¤šåˆ†ç±»)
* nn.AdaptiveLogSoftmaxWithLoss (**ä¸€ç§é€‚åˆéå¸¸å¤šç±»åˆ«ä¸”ç±»åˆ«åˆ†å¸ƒå¾ˆä¸å‡è¡¡çš„æŸå¤±å‡½æ•°**ï¼Œä¼šè‡ªé€‚åº”åœ°å°†å¤šä¸ªå°ç±»åˆ«åˆæˆä¸€ä¸ªcluster)



æ›´å¤šæŸå¤±å‡½æ•°çš„ä»‹ç»å‚è€ƒå¦‚ä¸‹çŸ¥ä¹æ–‡ç« ï¼š

- [ã€ŠPyTorchçš„åå…«ä¸ªæŸå¤±å‡½æ•°ã€‹](https://zhuanlan.zhihu.com/p/61379965)



### è‡ªå®šä¹‰æŸå¤±å‡½æ•°


è‡ªå®šä¹‰æŸå¤±å‡½æ•°æ¥æ”¶ä¸¤ä¸ªå¼ é‡y_pred,y_trueä½œä¸ºè¾“å…¥å‚æ•°ï¼Œå¹¶è¾“å‡ºä¸€ä¸ªæ ‡é‡ä½œä¸ºæŸå¤±å‡½æ•°å€¼ã€‚

ä¹Ÿå¯ä»¥å¯¹nn.Moduleè¿›è¡Œå­ç±»åŒ–ï¼Œé‡å†™forwardæ–¹æ³•å®ç°æŸå¤±çš„è®¡ç®—é€»è¾‘ï¼Œä»è€Œå¾—åˆ°æŸå¤±å‡½æ•°çš„ç±»çš„å®ç°ã€‚

ä¸‹é¢æ˜¯ä¸€ä¸ªFocal Lossçš„è‡ªå®šä¹‰å®ç°ç¤ºèŒƒã€‚Focal Lossæ˜¯ä¸€ç§å¯¹binary_crossentropyçš„æ”¹è¿›æŸå¤±å‡½æ•°å½¢å¼ã€‚

å®ƒåœ¨æ ·æœ¬ä¸å‡è¡¡å’Œå­˜åœ¨è¾ƒå¤šæ˜“åˆ†ç±»çš„æ ·æœ¬æ—¶ç›¸æ¯”binary_crossentropyå…·æœ‰æ˜æ˜¾çš„ä¼˜åŠ¿ã€‚

å®ƒæœ‰ä¸¤ä¸ªå¯è°ƒå‚æ•°ï¼Œalphaå‚æ•°å’Œgammaå‚æ•°ã€‚å…¶ä¸­alphaå‚æ•°ä¸»è¦ç”¨äºè¡°å‡è´Ÿæ ·æœ¬çš„æƒé‡ï¼Œgammaå‚æ•°ä¸»è¦ç”¨äºè¡°å‡å®¹æ˜“è®­ç»ƒæ ·æœ¬çš„æƒé‡ã€‚

ä»è€Œè®©æ¨¡å‹æ›´åŠ èšç„¦åœ¨æ­£æ ·æœ¬å’Œå›°éš¾æ ·æœ¬ä¸Šã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¿™ä¸ªæŸå¤±å‡½æ•°å«åšFocal Lossã€‚

è¯¦è§[ã€Š5åˆ†é’Ÿç†è§£Focal Lossä¸GHMâ€”â€”è§£å†³æ ·æœ¬ä¸å¹³è¡¡åˆ©å™¨ã€‹](https://zhuanlan.zhihu.com/p/80594704)



$$
focal\_loss(y,p) = 
\begin{cases} -\alpha (1-p)^{\gamma}\log(p) & \text{if y = 1}\\
-(1-\alpha) p^{\gamma}\log(1-p) & \text{if y = 0} 
\end{cases}
$$


```python
class FocalLoss(nn.Module):
    
    def __init__(self,gamma=2.0,alpha=0.75):
        super().__init__()
        self.gamma = gamma
        self.alpha = alpha

    def forward(self,y_pred,y_true):
        bce = torch.nn.BCELoss(reduction = "none")(y_pred,y_true)
        p_t = (y_true * y_pred) + ((1 - y_true) * (1 - y_pred))
        alpha_factor = y_true * self.alpha + (1 - y_true) * (1 - self.alpha)
        modulating_factor = torch.pow(1.0 - p_t, self.gamma)
        loss = torch.mean(alpha_factor * modulating_factor * bce)
        return loss
```

```python
#å›°éš¾æ ·æœ¬
y_pred_hard = torch.tensor([[0.5],[0.5]])
y_true_hard = torch.tensor([[1.0],[0.0]])

#å®¹æ˜“æ ·æœ¬
y_pred_easy = torch.tensor([[0.9],[0.1]])
y_true_easy = torch.tensor([[1.0],[0.0]])

focal_loss = FocalLoss()
bce_loss = nn.BCELoss()

print("focal_loss(hard samples):", focal_loss(y_pred_hard,y_true_hard))
print("bce_loss(hard samples):", bce_loss(y_pred_hard,y_true_hard))
print("focal_loss(easy samples):", focal_loss(y_pred_easy,y_true_easy))
print("bce_loss(easy samples):", bce_loss(y_pred_easy,y_true_easy))

#å¯è§ focal_lossè®©å®¹æ˜“æ ·æœ¬çš„æƒé‡è¡°å‡åˆ°åŸæ¥çš„ 0.0005/0.1054 = 0.00474
#è€Œè®©å›°éš¾æ ·æœ¬çš„æƒé‡åªè¡°å‡åˆ°åŸæ¥çš„ 0.0866/0.6931=0.12496

# å› æ­¤ç›¸å¯¹è€Œè¨€ï¼Œfocal_losså¯ä»¥è¡°å‡å®¹æ˜“æ ·æœ¬çš„æƒé‡ã€‚
```

```
focal_loss(hard samples): tensor(0.0866)
bce_loss(hard samples): tensor(0.6931)
focal_loss(easy samples): tensor(0.0005)
bce_loss(easy samples): tensor(0.1054)
```


FocalLossçš„ä½¿ç”¨å®Œæ•´èŒƒä¾‹å¯ä»¥å‚è€ƒä¸‹é¢ä¸­`è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹`ä¸­çš„èŒƒä¾‹ï¼Œè¯¥èŒƒä¾‹æ—¢æ¼”ç¤ºäº†è‡ªå®šä¹‰æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ï¼Œä¹Ÿæ¼”ç¤ºäº†FocalLossçš„ä½¿ç”¨æ–¹æ³•ã€‚



### è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹


é€šå¸¸è®¤ä¸ºL1 æ­£åˆ™åŒ–å¯ä»¥äº§ç”Ÿç¨€ç–æƒå€¼çŸ©é˜µï¼Œå³äº§ç”Ÿä¸€ä¸ªç¨€ç–æ¨¡å‹ï¼Œå¯ä»¥ç”¨äºç‰¹å¾é€‰æ‹©ã€‚

è€ŒL2 æ­£åˆ™åŒ–å¯ä»¥é˜²æ­¢æ¨¡å‹è¿‡æ‹Ÿåˆï¼ˆoverfittingï¼‰ã€‚ä¸€å®šç¨‹åº¦ä¸Šï¼ŒL1ä¹Ÿå¯ä»¥é˜²æ­¢è¿‡æ‹Ÿåˆã€‚

ä¸‹é¢ä»¥ä¸€ä¸ªäºŒåˆ†ç±»é—®é¢˜ä¸ºä¾‹ï¼Œæ¼”ç¤ºç»™æ¨¡å‹çš„ç›®æ ‡å‡½æ•°æ·»åŠ è‡ªå®šä¹‰L1å’ŒL2æ­£åˆ™åŒ–é¡¹çš„æ–¹æ³•ã€‚

è¿™ä¸ªèŒƒä¾‹åŒæ—¶æ¼”ç¤ºäº†ä¸Šä¸€ä¸ªéƒ¨åˆ†çš„FocalLossçš„ä½¿ç”¨ã€‚




**1ï¼Œå‡†å¤‡æ•°æ®**

```python
import numpy as np 
import pandas as pd 
from matplotlib import pyplot as plt
import torch
from torch import nn
import torch.nn.functional as F
from torch.utils.data import Dataset,DataLoader,TensorDataset
import torchkeras 
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æ­£è´Ÿæ ·æœ¬æ•°é‡
n_positive,n_negative = 200,6000

#ç”Ÿæˆæ­£æ ·æœ¬, å°åœ†ç¯åˆ†å¸ƒ
r_p = 5.0 + torch.normal(0.0,1.0,size = [n_positive,1]) 
theta_p = 2*np.pi*torch.rand([n_positive,1])
Xp = torch.cat([r_p*torch.cos(theta_p), r_p*torch.sin(theta_p)], axis = 1)
Yp = torch.ones_like(r_p)

#ç”Ÿæˆè´Ÿæ ·æœ¬, å¤§åœ†ç¯åˆ†å¸ƒ
r_n = 8.0 + torch.normal(0.0,1.0,size = [n_negative,1]) 
theta_n = 2*np.pi*torch.rand([n_negative,1])
Xn = torch.cat([r_n*torch.cos(theta_n),r_n*torch.sin(theta_n)],axis = 1)
Yn = torch.zeros_like(r_n)

#æ±‡æ€»æ ·æœ¬
X = torch.cat([Xp,Xn],axis = 0)
Y = torch.cat([Yp,Yn],axis = 0)


#å¯è§†åŒ–
plt.figure(figsize = (6,6))
plt.scatter(Xp[:,0],Xp[:,1],c = "r")
plt.scatter(Xn[:,0],Xn[:,1],c = "g")
plt.legend(["positive","negative"]);
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-661941.png)

```python
ds = TensorDataset(X,Y)

ds_train,ds_valid = torch.utils.data.random_split(ds,[int(len(ds)*0.7),len(ds)-int(len(ds)*0.7)])
dl_train = DataLoader(ds_train,batch_size = 100,shuffle=True,num_workers=2)
dl_valid = DataLoader(ds_valid,batch_size = 100,num_workers=2)
```



**2ï¼Œå®šä¹‰æ¨¡å‹**

```python
class DNNModel(torchkeras.Model):
    def __init__(self):
        super(DNNModel, self).__init__()
        self.fc1 = nn.Linear(2,4)
        self.fc2 = nn.Linear(4,8) 
        self.fc3 = nn.Linear(8,1)
        
    def forward(self,x):
        x = F.relu(self.fc1(x))
        x = F.relu(self.fc2(x))
        y = nn.Sigmoid()(self.fc3(x))
        return y
        
model = DNNModel()

model.summary(input_shape =(2,))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Linear-1                    [-1, 4]              12
            Linear-2                    [-1, 8]              40
            Linear-3                    [-1, 1]               9
================================================================
Total params: 61
Trainable params: 61
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.000008
Forward/backward pass size (MB): 0.000099
Params size (MB): 0.000233
Estimated Total Size (MB): 0.000340
----------------------------------------------------------------
```



**3ï¼Œè®­ç»ƒæ¨¡å‹**

```python
# å‡†ç¡®ç‡
def accuracy(y_pred,y_true):
    y_pred = torch.where(y_pred>0.5,
                         torch.ones_like(y_pred,dtype = torch.float32),
                         torch.zeros_like(y_pred,dtype = torch.float32))
    acc = torch.mean(1 - torch.abs(y_true-y_pred))
    return acc

# L2æ­£åˆ™åŒ–
def L2Loss(model,alpha):
    l2_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name: #ä¸€èˆ¬ä¸å¯¹åç½®é¡¹ä½¿ç”¨æ­£åˆ™
            l2_loss = l2_loss + (0.5 * alpha * torch.sum(torch.pow(param, 2)))
    return l2_loss

# L1æ­£åˆ™åŒ–
def L1Loss(model,beta):
    l1_loss = torch.tensor(0.0, requires_grad=True)
    for name, param in model.named_parameters():
        if 'bias' not in name:
            l1_loss = l1_loss +  beta * torch.sum(torch.abs(param))
    return l1_loss

# å°†L2æ­£åˆ™å’ŒL1æ­£åˆ™æ·»åŠ åˆ°FocalLossæŸå¤±ï¼Œä¸€èµ·ä½œä¸ºç›®æ ‡å‡½æ•°
def focal_loss_with_regularization(y_pred,y_true):
    focal = FocalLoss()(y_pred,y_true) 
    l2_loss = L2Loss(model,0.001) #æ³¨æ„è®¾ç½®æ­£åˆ™åŒ–é¡¹ç³»æ•°
    l1_loss = L1Loss(model,0.001)
    total_loss = focal + l2_loss + l1_loss
    return total_loss

model.compile(loss_func =focal_loss_with_regularization,
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.01),
              metrics_dict={"accuracy":accuracy})

dfhistory = model.fit(30,
                      dl_train = dl_train,
                      dl_val = dl_valid,
                      log_step_freq = 30)
```

```
Start Training ...

================================================================================2020-07-11 23:34:17
{'step': 30, 'loss': 0.021, 'accuracy': 0.972}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.022 |  0.971   |  0.025   |     0.96     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:34:27
{'step': 30, 'loss': 0.016, 'accuracy': 0.984}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   30  | 0.016 |  0.981   |  0.017   |    0.983     |
+-------+-------+----------+----------+--------------+

================================================================================2020-07-11 23:34:27
Finished Training...
```



```python
# ç»“æœå¯è§†åŒ–
fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize = (12,5))
ax1.scatter(Xp[:,0],Xp[:,1], c="r")
ax1.scatter(Xn[:,0],Xn[:,1],c = "g")
ax1.legend(["positive","negative"]);
ax1.set_title("y_true");

Xp_pred = X[torch.squeeze(model.forward(X)>=0.5)]
Xn_pred = X[torch.squeeze(model.forward(X)<0.5)]

ax2.scatter(Xp_pred[:,0],Xp_pred[:,1],c = "r")
ax2.scatter(Xn_pred[:,0],Xn_pred[:,1],c = "g")
ax2.legend(["positive","negative"]);
ax2.set_title("y_pred");
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-657328.png)



### é€šè¿‡ä¼˜åŒ–å™¨å®ç°L2æ­£åˆ™åŒ–


å¦‚æœä»…ä»…éœ€è¦ä½¿ç”¨L2æ­£åˆ™åŒ–ï¼Œé‚£ä¹ˆä¹Ÿå¯ä»¥åˆ©ç”¨ä¼˜åŒ–å™¨çš„weight_decayå‚æ•°æ¥å®ç°ã€‚

weight_decayå‚æ•°å¯ä»¥è®¾ç½®å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„è¡°å‡ï¼Œè¿™å’ŒL2æ­£åˆ™åŒ–çš„ä½œç”¨æ•ˆæœç­‰ä»·ã€‚


```
before L2 regularization:

gradient descent: w = w - lr * dloss_dw 

after L2 regularization:

gradient descent: w = w - lr * (dloss_dw+beta*w) = (1-lr*beta)*w - lr*dloss_dw

so ï¼ˆ1-lr*betaï¼‰is the weight decay ratio.
```


Pytorchçš„ä¼˜åŒ–å™¨æ”¯æŒä¸€ç§ç§°ä¹‹ä¸ºPer-parameter optionsçš„æ“ä½œï¼Œå°±æ˜¯å¯¹æ¯ä¸€ä¸ªå‚æ•°è¿›è¡Œç‰¹å®šçš„å­¦ä¹ ç‡ï¼Œæƒé‡è¡°å‡ç‡æŒ‡å®šï¼Œä»¥æ»¡è¶³æ›´ä¸ºç»†è‡´çš„è¦æ±‚ã€‚

```python
weight_params = [param for name, param in model.named_parameters() if "bias" not in name]
bias_params = [param for name, param in model.named_parameters() if "bias" in name]

optimizer = torch.optim.SGD([{'params': weight_params, 'weight_decay':1e-5},
                             {'params': bias_params, 'weight_decay':0}],
                            lr=1e-2, momentum=0.9)
```



## TensorBoardå¯è§†åŒ–

åœ¨æˆ‘ä»¬çš„ç‚¼ä¸¹è¿‡ç¨‹ä¸­ï¼Œå¦‚æœèƒ½å¤Ÿä½¿ç”¨ä¸°å¯Œçš„å›¾åƒæ¥å±•ç¤ºæ¨¡å‹çš„ç»“æ„ï¼ŒæŒ‡æ ‡çš„å˜åŒ–ï¼Œå‚æ•°çš„åˆ†å¸ƒï¼Œè¾“å…¥çš„å½¢æ€ç­‰ä¿¡æ¯ï¼Œæ— ç–‘ä¼šæå‡æˆ‘ä»¬å¯¹é—®é¢˜çš„æ´å¯ŸåŠ›ï¼Œå¹¶å¢åŠ è®¸å¤šç‚¼ä¸¹çš„ä¹è¶£ã€‚

TensorBoardæ­£æ˜¯è¿™æ ·ä¸€ä¸ªç¥å¥‡çš„ç‚¼ä¸¹å¯è§†åŒ–è¾…åŠ©å·¥å…·ã€‚å®ƒåŸæ˜¯TensorFlowçš„å°å¼Ÿï¼Œä½†å®ƒä¹Ÿèƒ½å¤Ÿå¾ˆå¥½åœ°å’ŒPytorchè¿›è¡Œé…åˆã€‚ç”šè‡³åœ¨Pytorchä¸­ä½¿ç”¨TensorBoardæ¯”TensorFlowä¸­ä½¿ç”¨TensorBoardè¿˜è¦æ¥çš„æ›´åŠ ç®€å•å’Œè‡ªç„¶ã€‚

Pytorchä¸­åˆ©ç”¨TensorBoardå¯è§†åŒ–çš„å¤§æ¦‚è¿‡ç¨‹å¦‚ä¸‹ï¼š

1. é¦–å…ˆåœ¨Pytorchä¸­æŒ‡å®šä¸€ä¸ªç›®å½•åˆ›å»ºä¸€ä¸ªtorch.utils.tensorboard.SummaryWriteræ—¥å¿—å†™å…¥å™¨ã€‚
2. ç„¶åæ ¹æ®éœ€è¦å¯è§†åŒ–çš„ä¿¡æ¯ï¼Œåˆ©ç”¨æ—¥å¿—å†™å…¥å™¨å°†ç›¸åº”ä¿¡æ¯æ—¥å¿—å†™å…¥æˆ‘ä»¬æŒ‡å®šçš„ç›®å½•ã€‚
3. æœ€åå°±å¯ä»¥ä¼ å…¥æ—¥å¿—ç›®å½•ä½œä¸ºå‚æ•°å¯åŠ¨TensorBoardï¼Œç„¶åå°±å¯ä»¥åœ¨TensorBoardä¸­æ„‰å¿«åœ°çœ‹ç‰‡äº†ã€‚



æˆ‘ä»¬ä¸»è¦ä»‹ç»Pytorchä¸­åˆ©ç”¨TensorBoardè¿›è¡Œå¦‚ä¸‹æ–¹é¢ä¿¡æ¯çš„å¯è§†åŒ–çš„æ–¹æ³•ã€‚

* å¯è§†åŒ–æ¨¡å‹ç»“æ„ï¼š writer.add_graph
* å¯è§†åŒ–æŒ‡æ ‡å˜åŒ–ï¼š writer.add_scalar
* å¯è§†åŒ–å‚æ•°åˆ†å¸ƒï¼š writer.add_histogram
* å¯è§†åŒ–åŸå§‹å›¾åƒï¼š writer.add_image æˆ– writer.add_images
* å¯è§†åŒ–äººå·¥ç»˜å›¾ï¼š writer.add_figure



### å¯è§†åŒ–æ¨¡å‹ç»“æ„

```python
import torch 
from torch import nn
from torch.utils.tensorboard import SummaryWriter
from torchkeras import Model,summary
```

```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)
        self.pool = nn.MaxPool2d(kernel_size = 2,stride = 2)
        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)
        self.dropout = nn.Dropout2d(p = 0.1)
        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(64,32)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(32,1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self,x):
        x = self.conv1(x)
        x = self.pool(x)
        x = self.conv2(x)
        x = self.pool(x)
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        y = self.sigmoid(x)
        return y
        
net = Net()
print(net)
```

```
Net(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```



```python
summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```

```python
writer = SummaryWriter('./data/tensorboard')
writer.add_graph(net,input_to_model = torch.rand(1,3,32,32))
writer.close()
```

```python
%load_ext tensorboard
#%tensorboard --logdir ./data/tensorboard
```



```python
from tensorboard import notebook
#æŸ¥çœ‹å¯åŠ¨çš„tensorboardç¨‹åº
notebook.list() 
```

```python
#å¯åŠ¨tensorboardç¨‹åº
notebook.start("--logdir ./data/tensorboard")
#ç­‰ä»·äºåœ¨å‘½ä»¤è¡Œä¸­æ‰§è¡Œ tensorboard --logdir ./data/tensorboard
#å¯ä»¥åœ¨æµè§ˆå™¨ä¸­æ‰“å¼€ http://localhost:6006/ æŸ¥çœ‹
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-666555.png)



### å¯è§†åŒ–æŒ‡æ ‡å˜åŒ–


æœ‰æ—¶å€™åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œå¦‚æœèƒ½å¤Ÿå®æ—¶åŠ¨æ€åœ°æŸ¥çœ‹losså’Œå„ç§metricçš„å˜åŒ–æ›²çº¿ï¼Œé‚£ä¹ˆæ— ç–‘å¯ä»¥å¸®åŠ©æˆ‘ä»¬æ›´åŠ ç›´è§‚åœ°äº†è§£æ¨¡å‹çš„è®­ç»ƒæƒ…å†µã€‚

æ³¨æ„ï¼Œwriter.add_scalarä»…èƒ½å¯¹æ ‡é‡çš„å€¼çš„å˜åŒ–è¿›è¡Œå¯è§†åŒ–ã€‚å› æ­¤å®ƒä¸€èˆ¬ç”¨äºå¯¹losså’Œmetricçš„å˜åŒ–è¿›è¡Œå¯è§†åŒ–åˆ†æã€‚


```python
import numpy as np 
import torch 
from torch.utils.tensorboard import SummaryWriter

# f(x) = a*x**2 + b*x + cçš„æœ€å°å€¼
x = torch.tensor(0.0,requires_grad = True) # xéœ€è¦è¢«æ±‚å¯¼
a = torch.tensor(1.0)
b = torch.tensor(-2.0)
c = torch.tensor(1.0)

optimizer = torch.optim.SGD(params=[x],lr = 0.01)

def f(x):
    result = a*torch.pow(x,2) + b*x + c 
    return(result)

writer = SummaryWriter('./data/tensorboard')
for i in range(500):
    optimizer.zero_grad()
    y = f(x)
    y.backward()
    optimizer.step()
    writer.add_scalar("x",x.item(),i) #æ—¥å¿—ä¸­è®°å½•xåœ¨ç¬¬step i çš„å€¼
    writer.add_scalar("y",y.item(),i) #æ—¥å¿—ä¸­è®°å½•yåœ¨ç¬¬step i çš„å€¼

writer.close()
    
print("y=",f(x).data,";","x=",x.data)
```

```
y= tensor(0.) ; x= tensor(1.0000)
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-685670.png)



### å¯è§†åŒ–å‚æ•°åˆ†å¸ƒ


å¦‚æœéœ€è¦å¯¹æ¨¡å‹çš„å‚æ•°(ä¸€èˆ¬éæ ‡é‡)åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å˜åŒ–è¿›è¡Œå¯è§†åŒ–ï¼Œå¯ä»¥ä½¿ç”¨ writer.add_histogramã€‚

å®ƒèƒ½å¤Ÿè§‚æµ‹å¼ é‡å€¼åˆ†å¸ƒçš„ç›´æ–¹å›¾éšè®­ç»ƒæ­¥éª¤çš„å˜åŒ–è¶‹åŠ¿ã€‚

```python
import numpy as np 
import torch 
from torch.utils.tensorboard import SummaryWriter


# åˆ›å»ºæ­£æ€åˆ†å¸ƒçš„å¼ é‡æ¨¡æ‹Ÿå‚æ•°çŸ©é˜µ
def norm(mean,std):
    t = std*torch.randn((100,20))+mean
    return t

writer = SummaryWriter('./data/tensorboard')
for step,mean in enumerate(range(-10,10,1)):
    w = norm(mean,1)
    writer.add_histogram("w",w, step)
    writer.flush()
writer.close()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-682375.png)



### å¯è§†åŒ–åŸå§‹å›¾åƒ


å¦‚æœæˆ‘ä»¬åšå›¾åƒç›¸å…³çš„ä»»åŠ¡ï¼Œä¹Ÿå¯ä»¥å°†åŸå§‹çš„å›¾ç‰‡åœ¨tensorboardä¸­è¿›è¡Œå¯è§†åŒ–å±•ç¤ºã€‚

å¦‚æœåªå†™å…¥ä¸€å¼ å›¾ç‰‡ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨writer.add_imageã€‚

å¦‚æœè¦å†™å…¥å¤šå¼ å›¾ç‰‡ä¿¡æ¯ï¼Œå¯ä»¥ä½¿ç”¨writer.add_imagesã€‚

ä¹Ÿå¯ä»¥ç”¨ torchvision.utils.make_gridå°†å¤šå¼ å›¾ç‰‡æ‹¼æˆä¸€å¼ å›¾ç‰‡ï¼Œç„¶åç”¨writer.add_imageå†™å…¥ã€‚

æ³¨æ„ï¼Œä¼ å…¥çš„æ˜¯ä»£è¡¨å›¾ç‰‡ä¿¡æ¯çš„Pytorchä¸­çš„å¼ é‡æ•°æ®ã€‚


```python
import torch
import torchvision
from torch import nn
from torch.utils.data import Dataset,DataLoader
from torchvision import transforms,datasets 

transform_train = transforms.Compose([transforms.ToTensor()])
transform_valid = transforms.Compose([transforms.ToTensor()])
```

```python
ds_train = datasets.ImageFolder("./data/cifar2/train/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())
ds_valid = datasets.ImageFolder("./data/cifar2/test/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())

print(ds_train.class_to_idx)

dl_train = DataLoader(ds_train,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)
dl_valid = DataLoader(ds_valid,
                      batch_size = 50,
                      shuffle = True,
                      num_workers=3)

dl_train_iter = iter(dl_train)
images, labels = dl_train_iter.next()

# ä»…æŸ¥çœ‹ä¸€å¼ å›¾ç‰‡
writer = SummaryWriter('./data/tensorboard')
writer.add_image('images[0]', images[0])
writer.close()

# å°†å¤šå¼ å›¾ç‰‡æ‹¼æ¥æˆä¸€å¼ å›¾ç‰‡ï¼Œä¸­é—´ç”¨é»‘è‰²ç½‘æ ¼åˆ†å‰²
writer = SummaryWriter('./data/tensorboard')
# create grid of images
img_grid = torchvision.utils.make_grid(images)
writer.add_image('image_grid', img_grid)
writer.close()

# å°†å¤šå¼ å›¾ç‰‡ç›´æ¥å†™å…¥
writer = SummaryWriter('./data/tensorboard')
writer.add_images("images",images,global_step = 0)
writer.close()
```

```
{'0_airplane': 0, '1_automobile': 1}
```



![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-673804.png)



### å¯è§†åŒ–äººå·¥ç»˜å›¾


å¦‚æœæˆ‘ä»¬å°†matplotlibç»˜å›¾çš„ç»“æœå† tensorboardä¸­å±•ç¤ºï¼Œå¯ä»¥ä½¿ç”¨ add_figure.

æ³¨æ„ï¼Œå’Œwriter.add_imageä¸åŒçš„æ˜¯ï¼Œwriter.add_figureéœ€è¦ä¼ å…¥matplotlibçš„figureå¯¹è±¡ã€‚


```python
import torch
import torchvision
from torch import nn
from torch.utils.data import Dataset,DataLoader
from torchvision import transforms,datasets 


transform_train = transforms.Compose([transforms.ToTensor()])
transform_valid = transforms.Compose([transforms.ToTensor()])

ds_train = datasets.ImageFolder("./data/cifar2/train/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())
ds_valid = datasets.ImageFolder("./data/cifar2/test/",
                                transform = transform_train,
                                target_transform= lambda t:torch.tensor([t]).float())

print(ds_train.class_to_idx)
```

```
{'0_airplane': 0, '1_automobile': 1}
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'
from matplotlib import pyplot as plt 

figure = plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_train[i]
    img = img.permute(1,2,0)
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label.item())
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-670510.png)

```python
writer = SummaryWriter('./data/tensorboard')
writer.add_figure('figure', figure, global_step=0)
writer.close()                         
```

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20220302-125625-678418.png)





# Pytorchçš„é«˜é˜¶API

Pytorchæ²¡æœ‰å®˜æ–¹çš„é«˜é˜¶APIã€‚ä¸€èˆ¬é€šè¿‡nn.Moduleæ¥æ„å»ºæ¨¡å‹å¹¶ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ã€‚

ä¸ºäº†æ›´åŠ æ–¹ä¾¿åœ°è®­ç»ƒæ¨¡å‹ï¼Œä½œè€…ç¼–å†™äº†ä»¿kerasçš„Pytorchæ¨¡å‹æ¥å£ï¼štorchkerasï¼Œ ä½œä¸ºPytorchçš„é«˜é˜¶APIã€‚

æœ¬ç« æˆ‘ä»¬ä¸»è¦è¯¦ç»†ä»‹ç»Pytorchçš„é«˜é˜¶APIå¦‚ä¸‹ç›¸å…³çš„å†…å®¹ã€‚

* æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•(ä½¿ç”¨nn.Sequentialï¼Œç»§æ‰¿nn.ModuleåŸºç±»ï¼Œè¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨)
* è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•(è„šæœ¬é£æ ¼ï¼Œå‡½æ•°é£æ ¼ï¼Œtorchkeras.Modelç±»é£æ ¼)
* ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹(å•GPUè®­ç»ƒï¼Œå¤šGPUè®­ç»ƒ)



## æ„å»ºæ¨¡å‹çš„3ç§æ–¹æ³•

å¯ä»¥ä½¿ç”¨ä»¥ä¸‹3ç§æ–¹å¼æ„å»ºæ¨¡å‹ï¼š

1. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹ã€‚
2. ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹ã€‚
3. ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨è¿›è¡Œå°è£…(nn.Sequential,nn.ModuleList,nn.ModuleDict)ã€‚

å…¶ä¸­ ç¬¬1ç§æ–¹å¼æœ€ä¸ºå¸¸è§ï¼Œç¬¬2ç§æ–¹å¼æœ€ç®€å•ï¼Œç¬¬3ç§æ–¹å¼æœ€ä¸ºçµæ´»ä¹Ÿè¾ƒä¸ºå¤æ‚ã€‚

æ¨èä½¿ç”¨ç¬¬1ç§æ–¹å¼æ„å»ºæ¨¡å‹ã€‚


```python
import torch 
from torch import nn
from torchkeras import summary
```



### ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹


ä»¥ä¸‹æ˜¯ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºè‡ªå®šä¹‰æ¨¡å‹çš„ä¸€ä¸ªèŒƒä¾‹ã€‚æ¨¡å‹ä¸­çš„ç”¨åˆ°çš„å±‚ä¸€èˆ¬åœ¨`__init__`å‡½æ•°ä¸­å®šä¹‰ï¼Œç„¶ååœ¨`forward`æ–¹æ³•ä¸­å®šä¹‰æ¨¡å‹çš„æ­£å‘ä¼ æ’­é€»è¾‘ã€‚


```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)
        self.pool1 = nn.MaxPool2d(kernel_size = 2,stride = 2)
        self.conv2 = nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)
        self.pool2 = nn.MaxPool2d(kernel_size = 2,stride = 2)
        self.dropout = nn.Dropout2d(p = 0.1)
        self.adaptive_pool = nn.AdaptiveMaxPool2d((1,1))
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(64,32)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(32,1)
        self.sigmoid = nn.Sigmoid()
        
    def forward(self,x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        y = self.sigmoid(x)
        return y
        
net = Net()
print(net)
```

```
Net(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```



```python
summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```



### ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹


ä½¿ç”¨nn.SequentialæŒ‰å±‚é¡ºåºæ„å»ºæ¨¡å‹æ— éœ€å®šä¹‰forwardæ–¹æ³•ã€‚ä»…ä»…é€‚åˆäºç®€å•çš„æ¨¡å‹ã€‚

ä»¥ä¸‹æ˜¯ä½¿ç”¨nn.Sequentialæ­å»ºæ¨¡å‹çš„ä¸€äº›ç­‰ä»·æ–¹æ³•ã€‚

#### åˆ©ç”¨add_moduleæ–¹æ³•

```python
net = nn.Sequential()
net.add_module("conv1",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3))
net.add_module("pool1",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("conv2",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))
net.add_module("pool2",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("dropout",nn.Dropout2d(p = 0.1))
net.add_module("adaptive_pool",nn.AdaptiveMaxPool2d((1,1)))
net.add_module("flatten",nn.Flatten())
net.add_module("linear1",nn.Linear(64,32))
net.add_module("relu",nn.ReLU())
net.add_module("linear2",nn.Linear(32,1))
net.add_module("sigmoid",nn.Sigmoid())

print(net)
```

```
Sequential(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```



#### åˆ©ç”¨å˜é•¿å‚æ•°

è¿™ç§æ–¹å¼æ„å»ºæ—¶ä¸èƒ½ç»™æ¯ä¸ªå±‚æŒ‡å®šåç§°ã€‚

```python
net = nn.Sequential(
    nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),
    nn.MaxPool2d(kernel_size = 2,stride = 2),
    nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
    nn.MaxPool2d(kernel_size = 2,stride = 2),
    nn.Dropout2d(p = 0.1),
    nn.AdaptiveMaxPool2d((1,1)),
    nn.Flatten(),
    nn.Linear(64,32),
    nn.ReLU(),
    nn.Linear(32,1),
    nn.Sigmoid()
)

print(net)
```

```
Sequential(
  (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (4): Dropout2d(p=0.1, inplace=False)
  (5): AdaptiveMaxPool2d(output_size=(1, 1))
  (6): Flatten()
  (7): Linear(in_features=64, out_features=32, bias=True)
  (8): ReLU()
  (9): Linear(in_features=32, out_features=1, bias=True)
  (10): Sigmoid()
)
```



#### åˆ©ç”¨OrderedDict

```python
from collections import OrderedDict

net = nn.Sequential(OrderedDict(
          [("conv1",nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3)),
            ("pool1",nn.MaxPool2d(kernel_size = 2,stride = 2)),
            ("conv2",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5)),
            ("pool2",nn.MaxPool2d(kernel_size = 2,stride = 2)),
            ("dropout",nn.Dropout2d(p = 0.1)),
            ("adaptive_pool",nn.AdaptiveMaxPool2d((1,1))),
            ("flatten",nn.Flatten()),
            ("linear1",nn.Linear(64,32)),
            ("relu",nn.ReLU()),
            ("linear2",nn.Linear(32,1)),
            ("sigmoid",nn.Sigmoid())
          ])
        )
print(net)
```

```
Sequential(
  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=1, bias=True)
  (sigmoid): Sigmoid()
)
```



```python
summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```



### ç»§æ‰¿nn.ModuleåŸºç±»æ„å»ºæ¨¡å‹å¹¶è¾…åŠ©åº”ç”¨æ¨¡å‹å®¹å™¨è¿›è¡Œå°è£…


å½“æ¨¡å‹çš„ç»“æ„æ¯”è¾ƒå¤æ‚æ—¶ï¼Œæˆ‘ä»¬å¯ä»¥åº”ç”¨æ¨¡å‹å®¹å™¨(nn.Sequential,nn.ModuleList,nn.ModuleDict)å¯¹æ¨¡å‹çš„éƒ¨åˆ†ç»“æ„è¿›è¡Œå°è£…ã€‚

è¿™æ ·åšä¼šè®©æ¨¡å‹æ•´ä½“æ›´åŠ æœ‰å±‚æ¬¡æ„Ÿï¼Œæœ‰æ—¶å€™ä¹Ÿèƒ½å‡å°‘ä»£ç é‡ã€‚

æ³¨æ„ï¼Œåœ¨ä¸‹é¢çš„èŒƒä¾‹ä¸­æˆ‘ä»¬æ¯æ¬¡ä»…ä»…ä½¿ç”¨ä¸€ç§æ¨¡å‹å®¹å™¨ï¼Œä½†å®é™…ä¸Šè¿™äº›æ¨¡å‹å®¹å™¨çš„ä½¿ç”¨æ˜¯éå¸¸çµæ´»çš„ï¼Œå¯ä»¥åœ¨ä¸€ä¸ªæ¨¡å‹ä¸­ä»»æ„ç»„åˆä»»æ„åµŒå¥—ä½¿ç”¨ã€‚



#### nn.Sequentialä½œä¸ºæ¨¡å‹å®¹å™¨

```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1))
        )
        self.dense = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,1),
            nn.Sigmoid()
        )
    def forward(self,x):
        x = self.conv(x)
        y = self.dense(x)
        return y 
    
net = Net()
print(net)
```

```
Net(
  (conv): Sequential(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Dropout2d(p=0.1, inplace=False)
    (5): AdaptiveMaxPool2d(output_size=(1, 1))
  )
  (dense): Sequential(
    (0): Flatten()
    (1): Linear(in_features=64, out_features=32, bias=True)
    (2): ReLU()
    (3): Linear(in_features=32, out_features=1, bias=True)
    (4): Sigmoid()
  )
)
```



#### nn.ModuleListä½œä¸ºæ¨¡å‹å®¹å™¨

æ³¨æ„ä¸‹é¢ä¸­çš„ModuleListä¸èƒ½ç”¨Pythonä¸­çš„åˆ—è¡¨ä»£æ›¿ã€‚

```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,1),
            nn.Sigmoid()]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x
    
net = Net()
print(net)
```

```
Net(
  (layers): ModuleList(
    (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Dropout2d(p=0.1, inplace=False)
    (5): AdaptiveMaxPool2d(output_size=(1, 1))
    (6): Flatten()
    (7): Linear(in_features=64, out_features=32, bias=True)
    (8): ReLU()
    (9): Linear(in_features=32, out_features=1, bias=True)
    (10): Sigmoid()
  )
)
```



```python
summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```



#### nn.ModuleDictä½œä¸ºæ¨¡å‹å®¹å™¨

æ³¨æ„ä¸‹é¢ä¸­çš„ModuleDictä¸èƒ½ç”¨Pythonä¸­çš„å­—å…¸ä»£æ›¿ã€‚

```python
class Net(nn.Module):
    
    def __init__(self):
        super(Net, self).__init__()
        self.layers_dict = nn.ModuleDict(
            {"conv1":nn.Conv2d(in_channels=3,out_channels=32,kernel_size = 3),
             "pool": nn.MaxPool2d(kernel_size = 2,stride = 2),
             "conv2":nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
             "dropout": nn.Dropout2d(p = 0.1),
             "adaptive":nn.AdaptiveMaxPool2d((1,1)),
             "flatten": nn.Flatten(),
             "linear1": nn.Linear(64,32),
             "relu":nn.ReLU(),
             "linear2": nn.Linear(32,1),
             "sigmoid": nn.Sigmoid()
              })
        
    def forward(self,x):
        layers = ["conv1","pool","conv2","pool","dropout","adaptive",
                  "flatten","linear1","relu","linear2","sigmoid"]
        for layer in layers:
            x = self.layers_dict[layer](x)
        return x
    
net = Net()
print(net)
```

```
Net(
  (layers_dict): ModuleDict(
    (adaptive): AdaptiveMaxPool2d(output_size=(1, 1))
    (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1))
    (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
    (dropout): Dropout2d(p=0.1, inplace=False)
    (flatten): Flatten()
    (linear1): Linear(in_features=64, out_features=32, bias=True)
    (linear2): Linear(in_features=32, out_features=1, bias=True)
    (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (relu): ReLU()
    (sigmoid): Sigmoid()
  )
)
```



```python
summary(net,input_shape= (3,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             896
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                    [-1, 1]              33
          Sigmoid-11                    [-1, 1]               0
================================================================
Total params: 54,273
Trainable params: 54,273
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.011719
Forward/backward pass size (MB): 0.359634
Params size (MB): 0.207035
Estimated Total Size (MB): 0.578388
----------------------------------------------------------------
```



## è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•

Pytorché€šå¸¸éœ€è¦ç”¨æˆ·ç¼–å†™è‡ªå®šä¹‰è®­ç»ƒå¾ªç¯ï¼Œè®­ç»ƒå¾ªç¯çš„ä»£ç é£æ ¼å› äººè€Œå¼‚ã€‚

æœ‰3ç±»å…¸å‹çš„è®­ç»ƒå¾ªç¯ä»£ç é£æ ¼ï¼š

1. è„šæœ¬å½¢å¼è®­ç»ƒå¾ªç¯
2. å‡½æ•°å½¢å¼è®­ç»ƒå¾ªç¯
3. ç±»å½¢å¼è®­ç»ƒå¾ªç¯

ä¸‹é¢ä»¥ministæ•°æ®é›†çš„åˆ†ç±»æ¨¡å‹çš„è®­ç»ƒä¸ºä¾‹ï¼Œæ¼”ç¤ºè¿™3ç§è®­ç»ƒæ¨¡å‹çš„é£æ ¼ã€‚



### å‡†å¤‡æ•°æ®

```python
import torch 
from torch import nn 
from torchkeras import summary,Model 

import torchvision 
from torchvision import transforms
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=True,
                                      download=True,
                                      transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=False,
                                      download=True,
                                      transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, 
                                        batch_size=128, 
                                        shuffle=True, 
                                        num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, 
                                        batch_size=128, 
                                        shuffle=False, 
                                        num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```

```
60000
10000
```



```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬
from matplotlib import pyplot as plt 

plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_train[i]
    img = torch.squeeze(img)
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```



![](https://gitee.com/liuhuihe/Ehe/raw/master/images/eat_pytorch_in_20_days-20210118-084825-757455.png)



### è„šæœ¬é£æ ¼


è„šæœ¬é£æ ¼çš„è®­ç»ƒå¾ªç¯æœ€ä¸ºå¸¸è§ã€‚

```python
net = nn.Sequential()
net.add_module("conv1",nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3))
net.add_module("pool1",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("conv2",nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5))
net.add_module("pool2",nn.MaxPool2d(kernel_size = 2,stride = 2))
net.add_module("dropout",nn.Dropout2d(p = 0.1))
net.add_module("adaptive_pool",nn.AdaptiveMaxPool2d((1,1)))
net.add_module("flatten",nn.Flatten())
net.add_module("linear1",nn.Linear(64,32))
net.add_module("relu",nn.ReLU())
net.add_module("linear2",nn.Linear(32,10))

print(net)
```

```
Sequential(
  (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (dropout): Dropout2d(p=0.1, inplace=False)
  (adaptive_pool): AdaptiveMaxPool2d(output_size=(1, 1))
  (flatten): Flatten()
  (linear1): Linear(in_features=64, out_features=32, bias=True)
  (relu): ReLU()
  (linear2): Linear(in_features=32, out_features=10, bias=True)
)
```



```python
summary(net,input_shape=(1,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             320
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                   [-1, 10]             330
================================================================
Total params: 53,994
Trainable params: 53,994
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.003906
Forward/backward pass size (MB): 0.359695
Params size (MB): 0.205971
Estimated Total Size (MB): 0.569572
----------------------------------------------------------------
```

```python
import datetime
import numpy as np 
import pandas as pd 
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true,y_pred_cls)

loss_func = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(params=net.parameters(),lr = 0.01)
metric_func = accuracy
metric_name = "accuracy"
```

```python
epochs = 3
log_step_freq = 100

dfhistory = pd.DataFrame(columns = ["epoch","loss",metric_name,"val_loss","val_"+metric_name]) 
print("Start Training...")
nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
print("=========="*8 + "%s"%nowtime)

for epoch in range(1,epochs+1):  

    # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
    net.train()
    loss_sum = 0.0
    metric_sum = 0.0
    step = 1
    
    for step, (features,labels) in enumerate(dl_train, 1):
    
        # æ¢¯åº¦æ¸…é›¶
        optimizer.zero_grad()

        # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
        predictions = net(features)
        loss = loss_func(predictions,labels)
        metric = metric_func(predictions,labels)
        
        # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
        loss.backward()
        optimizer.step()

        # æ‰“å°batchçº§åˆ«æ—¥å¿—
        loss_sum += loss.item()
        metric_sum += metric.item()
        if step%log_step_freq == 0:   
            print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                  (step, loss_sum/step, metric_sum/step))
            
    # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
    net.eval()
    val_loss_sum = 0.0
    val_metric_sum = 0.0
    val_step = 1

    for val_step, (features,labels) in enumerate(dl_valid, 1):
        with torch.no_grad():
            predictions = net(features)
            val_loss = loss_func(predictions,labels)
            val_metric = metric_func(predictions,labels)

        val_loss_sum += val_loss.item()
        val_metric_sum += val_metric.item()

    # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
    info = (epoch, loss_sum/step, metric_sum/step, 
            val_loss_sum/val_step, val_metric_sum/val_step)
    dfhistory.loc[epoch-1] = info
    
    # æ‰“å°epochçº§åˆ«æ—¥å¿—
    print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") %info)
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("\n"+"=========="*8 + "%s"%nowtime)
        
print('Finished Training...')
```

```
Start Training...
================================================================================2020-06-26 12:49:16
[step = 100] loss: 0.742, accuracy: 0.745
[step = 200] loss: 0.466, accuracy: 0.843
[step = 300] loss: 0.363, accuracy: 0.880
[step = 400] loss: 0.310, accuracy: 0.898

EPOCH = 1, loss = 0.281,accuracy  = 0.908, val_loss = 0.087, val_accuracy = 0.972

================================================================================2020-06-26 12:50:32
[step = 100] loss: 0.103, accuracy: 0.970
[step = 200] loss: 0.114, accuracy: 0.966
[step = 300] loss: 0.112, accuracy: 0.967
[step = 400] loss: 0.108, accuracy: 0.968

EPOCH = 2, loss = 0.111,accuracy  = 0.967, val_loss = 0.082, val_accuracy = 0.976

================================================================================2020-06-26 12:51:47
[step = 100] loss: 0.093, accuracy: 0.972
[step = 200] loss: 0.095, accuracy: 0.971
[step = 300] loss: 0.092, accuracy: 0.972
[step = 400] loss: 0.093, accuracy: 0.972

EPOCH = 3, loss = 0.098,accuracy  = 0.971, val_loss = 0.113, val_accuracy = 0.970

================================================================================2020-06-26 12:53:09
Finished Training...
```



### å‡½æ•°é£æ ¼


è¯¥é£æ ¼åœ¨è„šæœ¬å½¢å¼ä¸Šä½œäº†ç®€å•çš„å‡½æ•°å°è£…ã€‚

```python
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x
    
net = Net()
print(net)
```

```
Net(
  (layers): ModuleList(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Dropout2d(p=0.1, inplace=False)
    (5): AdaptiveMaxPool2d(output_size=(1, 1))
    (6): Flatten()
    (7): Linear(in_features=64, out_features=32, bias=True)
    (8): ReLU()
    (9): Linear(in_features=32, out_features=10, bias=True)
  )
)
```



```python
summary(net,input_shape=(1,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             320
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                   [-1, 10]             330
================================================================
Total params: 53,994
Trainable params: 53,994
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.003906
Forward/backward pass size (MB): 0.359695
Params size (MB): 0.205971
Estimated Total Size (MB): 0.569572
----------------------------------------------------------------
```



```python
import datetime
import numpy as np 
import pandas as pd 
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true,y_pred_cls)

model = net
model.optimizer = torch.optim.SGD(model.parameters(),lr = 0.01)
model.loss_func = nn.CrossEntropyLoss()
model.metric_func = accuracy
model.metric_name = "accuracy"
```

```python
def train_step(model,features,labels):
    
    # è®­ç»ƒæ¨¡å¼ï¼Œdropoutå±‚å‘ç”Ÿä½œç”¨
    model.train()
    
    # æ¢¯åº¦æ¸…é›¶
    model.optimizer.zero_grad()
    
    # æ­£å‘ä¼ æ’­æ±‚æŸå¤±
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)

    # åå‘ä¼ æ’­æ±‚æ¢¯åº¦
    loss.backward()
    model.optimizer.step()

    return loss.item(),metric.item()

@torch.no_grad()
def valid_step(model,features,labels):
    
    # é¢„æµ‹æ¨¡å¼ï¼Œdropoutå±‚ä¸å‘ç”Ÿä½œç”¨
    model.eval()
    
    predictions = model(features)
    loss = model.loss_func(predictions,labels)
    metric = model.metric_func(predictions,labels)
    
    return loss.item(), metric.item()


# æµ‹è¯•train_stepæ•ˆæœ
features,labels = next(iter(dl_train))
train_step(model,features,labels)
```

```
(2.32741117477417, 0.1015625)
```



```python
def train_model(model,epochs,dl_train,dl_valid,log_step_freq):

    metric_name = model.metric_name
    dfhistory = pd.DataFrame(columns = ["epoch","loss",metric_name,"val_loss","val_"+metric_name]) 
    print("Start Training...")
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("=========="*8 + "%s"%nowtime)

    for epoch in range(1,epochs+1):  
        # 1ï¼Œè®­ç»ƒå¾ªç¯-------------------------------------------------
        loss_sum = 0.0
        metric_sum = 0.0
        step = 1

        for step, (features,labels) in enumerate(dl_train, 1):
            loss,metric = train_step(model,features,labels)

            # æ‰“å°batchçº§åˆ«æ—¥å¿—
            loss_sum += loss
            metric_sum += metric
            if step%log_step_freq == 0:   
                print(("[step = %d] loss: %.3f, "+metric_name+": %.3f") %
                      (step, loss_sum/step, metric_sum/step))

        # 2ï¼ŒéªŒè¯å¾ªç¯-------------------------------------------------
        val_loss_sum = 0.0
        val_metric_sum = 0.0
        val_step = 1

        for val_step, (features,labels) in enumerate(dl_valid, 1):
            val_loss,val_metric = valid_step(model,features,labels)
            val_loss_sum += val_loss
            val_metric_sum += val_metric

        # 3ï¼Œè®°å½•æ—¥å¿—-------------------------------------------------
        info = (epoch, loss_sum/step, metric_sum/step, 
                val_loss_sum/val_step, val_metric_sum/val_step)
        dfhistory.loc[epoch-1] = info

        # æ‰“å°epochçº§åˆ«æ—¥å¿—
        print(("\nEPOCH = %d, loss = %.3f,"+ metric_name + \
              "  = %.3f, val_loss = %.3f, "+"val_"+ metric_name+" = %.3f") 
              %info)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n"+"=========="*8 + "%s"%nowtime)

    print('Finished Training...')
    return dfhistory
```

```python
epochs = 3
dfhistory = train_model(model,epochs,dl_train,dl_valid,log_step_freq = 100)
```

```
Start Training...
================================================================================2020-06-26 13:10:00
[step = 100] loss: 2.298, accuracy: 0.137
[step = 200] loss: 2.288, accuracy: 0.145
[step = 300] loss: 2.278, accuracy: 0.165
[step = 400] loss: 2.265, accuracy: 0.183

EPOCH = 1, loss = 2.254,accuracy  = 0.195, val_loss = 2.158, val_accuracy = 0.301

================================================================================2020-06-26 13:11:23
[step = 100] loss: 2.127, accuracy: 0.302
[step = 200] loss: 2.080, accuracy: 0.338
[step = 300] loss: 2.025, accuracy: 0.374
[step = 400] loss: 1.957, accuracy: 0.411

EPOCH = 2, loss = 1.905,accuracy  = 0.435, val_loss = 1.469, val_accuracy = 0.710

================================================================================2020-06-26 13:12:43
[step = 100] loss: 1.435, accuracy: 0.615
[step = 200] loss: 1.324, accuracy: 0.647
[step = 300] loss: 1.221, accuracy: 0.672
[step = 400] loss: 1.132, accuracy: 0.696

EPOCH = 3, loss = 1.074,accuracy  = 0.711, val_loss = 0.582, val_accuracy = 0.878

================================================================================2020-06-26 13:13:59
Finished Training...
```



### ç±»é£æ ¼


æ­¤å¤„ä½¿ç”¨torchkerasä¸­å®šä¹‰çš„æ¨¡å‹æ¥å£æ„å»ºæ¨¡å‹ï¼Œå¹¶è°ƒç”¨compileæ–¹æ³•å’Œfitæ–¹æ³•è®­ç»ƒæ¨¡å‹ã€‚

ä½¿ç”¨è¯¥å½¢å¼è®­ç»ƒæ¨¡å‹éå¸¸ç®€æ´æ˜äº†ã€‚æ¨èä½¿ç”¨è¯¥å½¢å¼ã€‚

```python
class CnnModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x
    
model = torchkeras.Model(CnnModel())
print(model)
```

```
CnnModel(
  (layers): ModuleList(
    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))
    (1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (2): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))
    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (4): Dropout2d(p=0.1, inplace=False)
    (5): AdaptiveMaxPool2d(output_size=(1, 1))
    (6): Flatten()
    (7): Linear(in_features=64, out_features=32, bias=True)
    (8): ReLU()
    (9): Linear(in_features=32, out_features=10, bias=True)
  )
)
```



```python
model.summary(input_shape=(1,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             320
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                   [-1, 10]             330
================================================================
Total params: 53,994
Trainable params: 53,994
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.003906
Forward/backward pass size (MB): 0.359695
Params size (MB): 0.205971
Estimated Total Size (MB): 0.569572
----------------------------------------------------------------
```

```python
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1).data
    return accuracy_score(y_true.numpy(), y_pred_cls.numpy())

model.compile(loss_func = nn.CrossEntropyLoss(),
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
              metrics_dict={"accuracy":accuracy})
```

```python
dfhistory = model.fit(3,
                      dl_train = dl_train, 
                      dl_val=dl_valid, 
                      log_step_freq=100) 
```

```
Start Training ...

================================================================================2020-06-26 13:22:39
{'step': 100, 'loss': 0.976, 'accuracy': 0.664}
{'step': 200, 'loss': 0.611, 'accuracy': 0.795}
{'step': 300, 'loss': 0.478, 'accuracy': 0.841}
{'step': 400, 'loss': 0.403, 'accuracy': 0.868}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.371 |  0.879   |  0.087   |    0.972     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-26 13:23:59
{'step': 100, 'loss': 0.182, 'accuracy': 0.948}
{'step': 200, 'loss': 0.176, 'accuracy': 0.949}
{'step': 300, 'loss': 0.173, 'accuracy': 0.95}
{'step': 400, 'loss': 0.174, 'accuracy': 0.951}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.175 |  0.951   |  0.152   |    0.958     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-26 13:25:22
{'step': 100, 'loss': 0.143, 'accuracy': 0.961}
{'step': 200, 'loss': 0.151, 'accuracy': 0.959}
{'step': 300, 'loss': 0.149, 'accuracy': 0.96}
{'step': 400, 'loss': 0.152, 'accuracy': 0.959}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   3   | 0.153 |  0.959   |  0.086   |    0.975     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-26 13:26:48
Finished Training...
```



## ä½¿ç”¨GPUè®­ç»ƒæ¨¡å‹


æ·±åº¦å­¦ä¹ çš„è®­ç»ƒè¿‡ç¨‹å¸¸å¸¸éå¸¸è€—æ—¶ï¼Œä¸€ä¸ªæ¨¡å‹è®­ç»ƒå‡ ä¸ªå°æ—¶æ˜¯å®¶å¸¸ä¾¿é¥­ï¼Œè®­ç»ƒå‡ å¤©ä¹Ÿæ˜¯å¸¸æœ‰çš„äº‹æƒ…ï¼Œæœ‰æ—¶å€™ç”šè‡³è¦è®­ç»ƒå‡ åå¤©ã€‚

è®­ç»ƒè¿‡ç¨‹çš„è€—æ—¶ä¸»è¦æ¥è‡ªäºä¸¤ä¸ªéƒ¨åˆ†ï¼Œä¸€éƒ¨åˆ†æ¥è‡ªæ•°æ®å‡†å¤‡ï¼Œå¦ä¸€éƒ¨åˆ†æ¥è‡ªå‚æ•°è¿­ä»£ã€‚

å½“æ•°æ®å‡†å¤‡è¿‡ç¨‹è¿˜æ˜¯æ¨¡å‹è®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨æ›´å¤šè¿›ç¨‹æ¥å‡†å¤‡æ•°æ®ã€‚

å½“å‚æ•°è¿­ä»£è¿‡ç¨‹æˆä¸ºè®­ç»ƒæ—¶é—´çš„ä¸»è¦ç“¶é¢ˆæ—¶ï¼Œæˆ‘ä»¬é€šå¸¸çš„æ–¹æ³•æ˜¯åº”ç”¨GPUæ¥è¿›è¡ŒåŠ é€Ÿã€‚

Pytorchä¸­ä½¿ç”¨GPUåŠ é€Ÿæ¨¡å‹éå¸¸ç®€å•ï¼Œåªè¦å°†æ¨¡å‹å’Œæ•°æ®ç§»åŠ¨åˆ°GPUä¸Šã€‚æ ¸å¿ƒä»£ç åªæœ‰ä»¥ä¸‹å‡ è¡Œã€‚

```python
# å®šä¹‰æ¨¡å‹
... 

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.to(device) # ç§»åŠ¨æ¨¡å‹åˆ°cuda

# è®­ç»ƒæ¨¡å‹
...

features = features.to(device) # ç§»åŠ¨æ•°æ®åˆ°cuda
labels = labels.to(device) # æˆ–è€…  labels = labels.cuda() if torch.cuda.is_available() else labels
...
```

å¦‚æœè¦ä½¿ç”¨å¤šä¸ªGPUè®­ç»ƒæ¨¡å‹ï¼Œä¹Ÿéå¸¸ç®€å•ã€‚åªéœ€è¦åœ¨å°†æ¨¡å‹è®¾ç½®ä¸ºæ•°æ®å¹¶è¡Œé£æ ¼æ¨¡å‹ã€‚

åˆ™æ¨¡å‹ç§»åŠ¨åˆ°GPUä¸Šä¹‹åï¼Œä¼šåœ¨æ¯ä¸€ä¸ªGPUä¸Šæ‹·è´ä¸€ä¸ªå‰¯æœ¬ï¼Œå¹¶æŠŠæ•°æ®å¹³åˆ†åˆ°å„ä¸ªGPUä¸Šè¿›è¡Œè®­ç»ƒã€‚æ ¸å¿ƒä»£ç å¦‚ä¸‹ã€‚

```python
# å®šä¹‰æ¨¡å‹
... 

if torch.cuda.device_count() > 1:
    model = nn.DataParallel(model) # åŒ…è£…ä¸ºå¹¶è¡Œé£æ ¼æ¨¡å‹

# è®­ç»ƒæ¨¡å‹
...
features = features.to(device) # ç§»åŠ¨æ•°æ®åˆ°cuda
labels = labels.to(device) # æˆ–è€… labels = labels.cuda() if torch.cuda.is_available() else labels
...
```



**ä»¥ä¸‹æ˜¯ä¸€äº›å’ŒGPUæœ‰å…³çš„åŸºæœ¬æ“ä½œæ±‡æ€»** 


åœ¨Colabç¬”è®°æœ¬ä¸­ï¼šä¿®æ”¹->ç¬”è®°æœ¬è®¾ç½®->ç¡¬ä»¶åŠ é€Ÿå™¨ ä¸­é€‰æ‹© GPU

æ³¨ï¼šä»¥ä¸‹ä»£ç åªèƒ½åœ¨Colab ä¸Šæ‰èƒ½æ­£ç¡®æ‰§è¡Œã€‚

å¯ç‚¹å‡»å¦‚ä¸‹é“¾æ¥ï¼Œç›´æ¥åœ¨colabä¸­è¿è¡ŒèŒƒä¾‹ä»£ç ã€‚

[ã€Štorchä½¿ç”¨gpuè®­ç»ƒæ¨¡å‹ã€‹](https://colab.research.google.com/drive/1FDmi44-U3TFRCt9MwGn4HIj2SaaWIjHu?usp=sharing)



```python
import torch 
from torch import nn 
```

```python
# 1ï¼ŒæŸ¥çœ‹gpuä¿¡æ¯
if_cuda = torch.cuda.is_available()
print("if_cuda=",if_cuda)

gpu_count = torch.cuda.device_count()
print("gpu_count=",gpu_count)
```

```
if_cuda= True
gpu_count= 1
```



```python
# 2ï¼Œå°†å¼ é‡åœ¨gpuå’Œcpué—´ç§»åŠ¨
tensor = torch.rand((100,100))
tensor_gpu = tensor.to("cuda:0") # æˆ–è€… tensor_gpu = tensor.cuda()
print(tensor_gpu.device)
print(tensor_gpu.is_cuda)

tensor_cpu = tensor_gpu.to("cpu") # æˆ–è€… tensor_cpu = tensor_gpu.cpu() 
print(tensor_cpu.device)
```

```
cuda:0
True
cpu
```



```python
# 3ï¼Œå°†æ¨¡å‹ä¸­çš„å…¨éƒ¨å¼ é‡ç§»åŠ¨åˆ°gpuä¸Š
net = nn.Linear(2,1)
print(next(net.parameters()).is_cuda)
net.to("cuda:0") # å°†æ¨¡å‹ä¸­çš„å…¨éƒ¨å‚æ•°å¼ é‡ä¾æ¬¡åˆ°GPUä¸Šï¼Œæ³¨æ„ï¼Œæ— éœ€é‡æ–°èµ‹å€¼ä¸º net = net.to("cuda:0")
print(next(net.parameters()).is_cuda)
print(next(net.parameters()).device)
```

```
False
True
cuda:0
```



```python
# 4ï¼Œåˆ›å»ºæ”¯æŒå¤šä¸ªgpuæ•°æ®å¹¶è¡Œçš„æ¨¡å‹
linear = nn.Linear(2,1)
print(next(linear.parameters()).device)

model = nn.DataParallel(linear)
print(model.device_ids)
print(next(model.module.parameters()).device) 

#æ³¨æ„ä¿å­˜å‚æ•°æ—¶è¦æŒ‡å®šä¿å­˜model.moduleçš„å‚æ•°
torch.save(model.module.state_dict(), "./data/model_parameter.pkl") 

linear = nn.Linear(2,1)
linear.load_state_dict(torch.load("./data/model_parameter.pkl")) 
```

```
cpu
[0]
cuda:0
```

```python
# 5ï¼Œæ¸…ç©ºcudaç¼“å­˜

# è¯¥æ–¹æ³•åœ¨cudaè¶…å†…å­˜æ—¶ååˆ†æœ‰ç”¨
torch.cuda.empty_cache()
```



### çŸ©é˜µä¹˜æ³•èŒƒä¾‹


ä¸‹é¢åˆ†åˆ«ä½¿ç”¨CPUå’ŒGPUä½œä¸€ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œå¹¶æ¯”è¾ƒå…¶è®¡ç®—æ•ˆç‡ã€‚

```python
import time
import torch 
from torch import nn
```

```python
# ä½¿ç”¨cpu
a = torch.rand((10000,200))
b = torch.rand((200,10000))
tic = time.time()
c = torch.matmul(a,b)
toc = time.time()

print(toc-tic)
print(a.device)
print(b.device)
```

```
0.6454010009765625
cpu
cpu
```

```python
# ä½¿ç”¨gpu
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
a = torch.rand((10000,200), device = device) #å¯ä»¥æŒ‡å®šåœ¨GPUä¸Šåˆ›å»ºå¼ é‡
b = torch.rand((200,10000)) #ä¹Ÿå¯ä»¥åœ¨CPUä¸Šåˆ›å»ºå¼ é‡åç§»åŠ¨åˆ°GPUä¸Š
b = b.to(device) #æˆ–è€… b = b.cuda() if torch.cuda.is_available() else b 
tic = time.time()
c = torch.matmul(a,b)
toc = time.time()
print(toc-tic)
print(a.device)
print(b.device)
```

```
0.014541149139404297
cuda:0
cuda:0
```



### çº¿æ€§å›å½’èŒƒä¾‹

ä¸‹é¢å¯¹æ¯”ä½¿ç”¨CPUå’ŒGPUè®­ç»ƒä¸€ä¸ªçº¿æ€§å›å½’æ¨¡å‹çš„æ•ˆç‡

**1. ä½¿ç”¨CPU**

```python
# å‡†å¤‡æ•°æ®
n = 1000000 #æ ·æœ¬æ•°é‡

X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0,-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0.t() + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨
```

```python
# å®šä¹‰æ¨¡å‹
class LinearRegression(nn.Module): 
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn_like(w0))
        self.b = nn.Parameter(torch.zeros_like(b0))
        
    #æ­£å‘ä¼ æ’­
    def forward(self,x): 
        return x@self.w.t() + self.b
        
linear = LinearRegression() 
```

```python
# è®­ç»ƒæ¨¡å‹
optimizer = torch.optim.Adam(linear.parameters(),lr = 0.1)
loss_func = nn.MSELoss()

def train(epoches):
    tic = time.time()
    for epoch in range(epoches):
        optimizer.zero_grad()
        Y_pred = linear(X) 
        loss = loss_func(Y_pred,Y)
        loss.backward() 
        optimizer.step()
        if epoch%50==0:
            print({"epoch":epoch,"loss":loss.item()})
    toc = time.time()
    print("time used:",toc-tic)

train(500)
```

```
{'epoch': 0, 'loss': 3.996487855911255}
{'epoch': 50, 'loss': 3.9969770908355713}
{'epoch': 100, 'loss': 3.9964890480041504}
{'epoch': 150, 'loss': 3.996488332748413}
{'epoch': 200, 'loss': 3.996488094329834}
{'epoch': 250, 'loss': 3.996488332748413}
{'epoch': 300, 'loss': 3.996488332748413}
{'epoch': 350, 'loss': 3.996488094329834}
{'epoch': 400, 'loss': 3.996488332748413}
{'epoch': 450, 'loss': 3.996488094329834}
time used: 5.4090576171875
```



**2. ä½¿ç”¨GPU**

```python
# å‡†å¤‡æ•°æ®
n = 1000000 #æ ·æœ¬æ•°é‡

X = 10*torch.rand([n,2])-5.0  #torch.randæ˜¯å‡åŒ€åˆ†å¸ƒ 
w0 = torch.tensor([[2.0,-3.0]])
b0 = torch.tensor([[10.0]])
Y = X@w0.t() + b0 + torch.normal( 0.0,2.0,size = [n,1])  # @è¡¨ç¤ºçŸ©é˜µä¹˜æ³•,å¢åŠ æ­£æ€æ‰°åŠ¨

# ç§»åŠ¨åˆ°GPUä¸Š
print("torch.cuda.is_available() = ",torch.cuda.is_available())
X = X.cuda()
Y = Y.cuda()
print("X.device:",X.device)
print("Y.device:",Y.device)
```

```
torch.cuda.is_available() =  True
X.device: cuda:0
Y.device: cuda:0
```



```python
# å®šä¹‰æ¨¡å‹
class LinearRegression(nn.Module): 
    def __init__(self):
        super().__init__()
        self.w = nn.Parameter(torch.randn_like(w0))
        self.b = nn.Parameter(torch.zeros_like(b0))
    #æ­£å‘ä¼ æ’­
    def forward(self,x): 
        return x@self.w.t() + self.b
        
linear = LinearRegression() 

# ç§»åŠ¨æ¨¡å‹åˆ°GPUä¸Š
device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
linear.to(device)

#æŸ¥çœ‹æ¨¡å‹æ˜¯å¦å·²ç»ç§»åŠ¨åˆ°GPUä¸Š
print("if on cuda:",next(linear.parameters()).is_cuda)
```

```
if on cuda: True
```



```python
# è®­ç»ƒæ¨¡å‹
optimizer = torch.optim.Adam(linear.parameters(),lr = 0.1)
loss_func = nn.MSELoss()

def train(epoches):
    tic = time.time()
    for epoch in range(epoches):
        optimizer.zero_grad()
        Y_pred = linear(X) 
        loss = loss_func(Y_pred,Y)
        loss.backward() 
        optimizer.step()
        if epoch%50==0:
            print({"epoch":epoch,"loss":loss.item()})
    toc = time.time()
    print("time used:",toc-tic)
    
train(500)
```

```
{'epoch': 0, 'loss': 3.9982845783233643}
{'epoch': 50, 'loss': 3.998818874359131}
{'epoch': 100, 'loss': 3.9982895851135254}
{'epoch': 150, 'loss': 3.9982845783233643}
{'epoch': 200, 'loss': 3.998284339904785}
{'epoch': 250, 'loss': 3.9982845783233643}
{'epoch': 300, 'loss': 3.9982845783233643}
{'epoch': 350, 'loss': 3.9982845783233643}
{'epoch': 400, 'loss': 3.9982845783233643}
{'epoch': 450, 'loss': 3.9982845783233643}
time used: 0.4889392852783203
```



### torchkerasä½¿ç”¨å•GPUèŒƒä¾‹


ä¸‹é¢æ¼”ç¤ºä½¿ç”¨torchkerasæ¥åº”ç”¨GPUè®­ç»ƒæ¨¡å‹çš„æ–¹æ³•ã€‚

å…¶å¯¹åº”çš„CPUè®­ç»ƒæ¨¡å‹ä»£ç å‚è§ã€Š6-2,è®­ç»ƒæ¨¡å‹çš„3ç§æ–¹æ³•ã€‹

æœ¬ä¾‹ä»…éœ€è¦åœ¨å®ƒçš„åŸºç¡€ä¸Šå¢åŠ ä¸€è¡Œä»£ç ï¼Œåœ¨model.compileæ—¶æŒ‡å®š deviceå³å¯ã€‚



**1. å‡†å¤‡æ•°æ®**

```python
!pip install -U torchkeras 
```



```python
import torch 
from torch import nn 

import torchvision 
from torchvision import transforms

import torchkeras 
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=True,
                                      download=True,
                                      transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=False,
                                      download=True,
                                      transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, 
                                        batch_size=128, 
                                        shuffle=True, 
                                        num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, 
                                        batch_size=128, 
                                        shuffle=False, 
                                        num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

#æŸ¥çœ‹éƒ¨åˆ†æ ·æœ¬
from matplotlib import pyplot as plt 

plt.figure(figsize=(8,8)) 
for i in range(9):
    img,label = ds_train[i]
    img = torch.squeeze(img)
    ax=plt.subplot(3,3,i+1)
    ax.imshow(img.numpy())
    ax.set_title("label = %d"%label)
    ax.set_xticks([])
    ax.set_yticks([]) 
plt.show()
```



**2. å®šä¹‰æ¨¡å‹**

```python
class CnnModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
        
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)
        return x

net = CnnModel()
model = torchkeras.Model(net)
model.summary(input_shape=(1,32,32))
```

```
----------------------------------------------------------------
        Layer (type)               Output Shape         Param #
================================================================
            Conv2d-1           [-1, 32, 30, 30]             320
         MaxPool2d-2           [-1, 32, 15, 15]               0
            Conv2d-3           [-1, 64, 11, 11]          51,264
         MaxPool2d-4             [-1, 64, 5, 5]               0
         Dropout2d-5             [-1, 64, 5, 5]               0
 AdaptiveMaxPool2d-6             [-1, 64, 1, 1]               0
           Flatten-7                   [-1, 64]               0
            Linear-8                   [-1, 32]           2,080
              ReLU-9                   [-1, 32]               0
           Linear-10                   [-1, 10]             330
================================================================
Total params: 53,994
Trainable params: 53,994
Non-trainable params: 0
----------------------------------------------------------------
Input size (MB): 0.003906
Forward/backward pass size (MB): 0.359695
Params size (MB): 0.205971
Estimated Total Size (MB): 0.569572
----------------------------------------------------------------
```



**3. è®­ç»ƒæ¨¡å‹**

```python
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred),dim=1).data
    return accuracy_score(y_true.cpu().numpy(),y_pred_cls.cpu().numpy()) 
    # æ³¨æ„æ­¤å¤„è¦å°†æ•°æ®å…ˆç§»åŠ¨åˆ°cpuä¸Šï¼Œç„¶åæ‰èƒ½è½¬æ¢æˆnumpyæ•°ç»„

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")

model.compile(loss_func = nn.CrossEntropyLoss(),
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
              metrics_dict={"accuracy":accuracy},
              device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

dfhistory = model.fit(3,dl_train = dl_train, dl_val=dl_valid, log_step_freq=100) 
```

```
Start Training ...

================================================================================2020-06-27 00:24:29
{'step': 100, 'loss': 1.063, 'accuracy': 0.619}
{'step': 200, 'loss': 0.681, 'accuracy': 0.764}
{'step': 300, 'loss': 0.534, 'accuracy': 0.818}
{'step': 400, 'loss': 0.458, 'accuracy': 0.847}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.412 |  0.863   |  0.128   |    0.961     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:35
{'step': 100, 'loss': 0.147, 'accuracy': 0.956}
{'step': 200, 'loss': 0.156, 'accuracy': 0.954}
{'step': 300, 'loss': 0.156, 'accuracy': 0.954}
{'step': 400, 'loss': 0.157, 'accuracy': 0.955}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.153 |  0.956   |  0.085   |    0.976     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:42
{'step': 100, 'loss': 0.126, 'accuracy': 0.965}
{'step': 200, 'loss': 0.147, 'accuracy': 0.96}
{'step': 300, 'loss': 0.153, 'accuracy': 0.959}
{'step': 400, 'loss': 0.147, 'accuracy': 0.96}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   3   | 0.146 |   0.96   |  0.119   |    0.968     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:48
Finished Training...
```



**4. è¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory,"loss")
```

```python
plot_metric(dfhistory,"accuracy")
```

```python
model.evaluate(dl_valid)
```

```
{'val_accuracy': 0.967068829113924, 'val_loss': 0.11601964030650598}
```



**5. ä½¿ç”¨æ¨¡å‹**

```python
model.predict(dl_valid)[0:10]
```

```
tensor([[ -9.2092,   3.1997,   1.4028,  -2.7135,  -0.7320,  -2.0518, -20.4938,
          14.6774,   1.7616,   5.8549],
        [  2.8509,   4.9781,  18.0946,   0.0928,  -1.6061,  -4.1437,   4.8697,
           3.8811,   4.3869,  -3.5929],
        [-22.5231,  13.6643,   5.0244, -11.0188, -16.8147,  -9.5894,  -6.2556,
         -10.5648, -12.1022, -19.4685],
        [ 23.2670, -12.0711,  -7.3968,  -8.2715,  -1.0915, -12.6050,   8.0444,
         -16.9339,   1.8827,  -0.2497],
        [ -4.1159,   3.2102,   0.4971, -11.8064,  12.1460,  -5.1650,  -6.5918,
           1.0088,   0.8362,   2.5132],
        [-26.1764,  15.6251,   6.1191, -12.2424, -13.9725, -10.0540,  -7.8669,
          -5.9602, -11.1944, -18.7890],
        [ -5.0602,   3.3779,  -0.6647,  -8.5185,  10.0320,  -5.5107,  -6.9579,
           2.3811,   0.2542,   3.2860],
        [  4.1017,  -0.4282,   7.2220,   3.3700,  -3.6813,   1.1576,  -1.8479,
           0.7450,   3.9768,   6.2640],
        [  1.9689,  -0.3960,   7.4414, -10.4789,   2.7066,   1.7482,   5.7971,
          -4.5808,   3.0911,  -5.1971],
        [ -2.9680,  -1.2369,  -0.0829,  -1.8577,   1.9380,  -0.8374,  -8.2207,
           3.5060,   3.8735,  13.6762]], device='cuda:0')
```



**6. ä¿å­˜æ¨¡å‹**

```python
# save the model parameters
torch.save(model.state_dict(), "model_parameter.pkl")

model_clone = torchkeras.Model(CnnModel())
model_clone.load_state_dict(torch.load("model_parameter.pkl"))

model_clone.compile(loss_func = nn.CrossEntropyLoss(),
                    optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
                    metrics_dict={"accuracy":accuracy},
                    device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

model_clone.evaluate(dl_valid)
```

```
{'val_accuracy': 0.967068829113924, 'val_loss': 0.11601964030650598}
```



### torchkerasä½¿ç”¨å¤šGPUèŒƒä¾‹


æ³¨ï¼šä»¥ä¸‹èŒƒä¾‹éœ€è¦åœ¨æœ‰å¤šä¸ªGPUçš„æœºå™¨ä¸Šè·‘ã€‚å¦‚æœåœ¨å•GPUçš„æœºå™¨ä¸Šè·‘ï¼Œä¹Ÿèƒ½è·‘é€šï¼Œä½†æ˜¯å®é™…ä¸Šä½¿ç”¨çš„æ˜¯å•ä¸ªGPUã€‚

**1. å‡†å¤‡æ•°æ®**

```python
import torch 
from torch import nn 

import torchvision 
from torchvision import transforms

import torchkeras 
```

```python
transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=True,
                                      download=True,
                                      transform=transform)
ds_valid = torchvision.datasets.MNIST(root="./data/minist/",
                                      train=False,
                                      download=True,
                                      transform=transform)

dl_train =  torch.utils.data.DataLoader(ds_train, 
                                        batch_size=128, 
                                        shuffle=True, 
                                        num_workers=4)
dl_valid =  torch.utils.data.DataLoader(ds_valid, 
                                        batch_size=128, 
                                        shuffle=False, 
                                        num_workers=4)

print(len(ds_train))
print(len(ds_valid))
```



**2. å®šä¹‰æ¨¡å‹**

```python
class CnnModule(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1,out_channels=32,kernel_size = 3),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Conv2d(in_channels=32,out_channels=64,kernel_size = 5),
            nn.MaxPool2d(kernel_size = 2,stride = 2),
            nn.Dropout2d(p = 0.1),
            nn.AdaptiveMaxPool2d((1,1)),
            nn.Flatten(),
            nn.Linear(64,32),
            nn.ReLU(),
            nn.Linear(32,10)]
        )
        
    def forward(self,x):
        for layer in self.layers:
            x = layer(x)  
        return x

net = nn.DataParallel(CnnModule())  #Attention this line!!!
model = torchkeras.Model(net)

model.summary(input_shape=(1,32,32))
```



**3. è®­ç»ƒæ¨¡å‹**

```python
from sklearn.metrics import accuracy_score

def accuracy(y_pred,y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1).data
    return accuracy_score(y_true.cpu().numpy(), y_pred_cls.cpu().numpy()) 
    # æ³¨æ„æ­¤å¤„è¦å°†æ•°æ®å…ˆç§»åŠ¨åˆ°cpuä¸Šï¼Œç„¶åæ‰èƒ½è½¬æ¢æˆnumpyæ•°ç»„

device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
model.compile(loss_func = nn.CrossEntropyLoss(),
              optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
              metrics_dict={"accuracy":accuracy},
              device = device) # æ³¨æ„æ­¤å¤„compileæ—¶æŒ‡å®šäº†device

dfhistory = model.fit(3,
                      dl_train = dl_train, 
                      dl_val=dl_valid, 
                      log_step_freq=100) 
```

```
Start Training ...

================================================================================2020-06-27 00:24:29
{'step': 100, 'loss': 1.063, 'accuracy': 0.619}
{'step': 200, 'loss': 0.681, 'accuracy': 0.764}
{'step': 300, 'loss': 0.534, 'accuracy': 0.818}
{'step': 400, 'loss': 0.458, 'accuracy': 0.847}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   1   | 0.412 |  0.863   |  0.128   |    0.961     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:35
{'step': 100, 'loss': 0.147, 'accuracy': 0.956}
{'step': 200, 'loss': 0.156, 'accuracy': 0.954}
{'step': 300, 'loss': 0.156, 'accuracy': 0.954}
{'step': 400, 'loss': 0.157, 'accuracy': 0.955}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   2   | 0.153 |  0.956   |  0.085   |    0.976     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:42
{'step': 100, 'loss': 0.126, 'accuracy': 0.965}
{'step': 200, 'loss': 0.147, 'accuracy': 0.96}
{'step': 300, 'loss': 0.153, 'accuracy': 0.959}
{'step': 400, 'loss': 0.147, 'accuracy': 0.96}

 +-------+-------+----------+----------+--------------+
| epoch |  loss | accuracy | val_loss | val_accuracy |
+-------+-------+----------+----------+--------------+
|   3   | 0.146 |   0.96   |  0.119   |    0.968     |
+-------+-------+----------+----------+--------------+

================================================================================2020-06-27 00:24:48
Finished Training...
```



**4. è¯„ä¼°æ¨¡å‹**

```python
%matplotlib inline
%config InlineBackend.figure_format = 'svg'

import matplotlib.pyplot as plt

def plot_metric(dfhistory, metric):
    train_metrics = dfhistory[metric]
    val_metrics = dfhistory['val_'+metric]
    epochs = range(1, len(train_metrics) + 1)
    plt.plot(epochs, train_metrics, 'bo--')
    plt.plot(epochs, val_metrics, 'ro-')
    plt.title('Training and validation '+ metric)
    plt.xlabel("Epochs")
    plt.ylabel(metric)
    plt.legend(["train_"+metric, 'val_'+metric])
    plt.show()
```

```python
plot_metric(dfhistory, "loss")
```

```python
plot_metric(dfhistory,"accuracy")
```

```python
model.evaluate(dl_valid)
```

```
{'val_accuracy': 0.9603441455696202, 'val_loss': 0.14203246376371081}
```



**5. ä½¿ç”¨æ¨¡å‹**

```python
model.predict(dl_valid)[0:10]
```

```
tensor([[ -9.2092,   3.1997,   1.4028,  -2.7135,  -0.7320,  -2.0518, -20.4938,
          14.6774,   1.7616,   5.8549],
        [  2.8509,   4.9781,  18.0946,   0.0928,  -1.6061,  -4.1437,   4.8697,
           3.8811,   4.3869,  -3.5929],
        [-22.5231,  13.6643,   5.0244, -11.0188, -16.8147,  -9.5894,  -6.2556,
         -10.5648, -12.1022, -19.4685],
        [ 23.2670, -12.0711,  -7.3968,  -8.2715,  -1.0915, -12.6050,   8.0444,
         -16.9339,   1.8827,  -0.2497],
        [ -4.1159,   3.2102,   0.4971, -11.8064,  12.1460,  -5.1650,  -6.5918,
           1.0088,   0.8362,   2.5132],
        [-26.1764,  15.6251,   6.1191, -12.2424, -13.9725, -10.0540,  -7.8669,
          -5.9602, -11.1944, -18.7890],
        [ -5.0602,   3.3779,  -0.6647,  -8.5185,  10.0320,  -5.5107,  -6.9579,
           2.3811,   0.2542,   3.2860],
        [  4.1017,  -0.4282,   7.2220,   3.3700,  -3.6813,   1.1576,  -1.8479,
           0.7450,   3.9768,   6.2640],
        [  1.9689,  -0.3960,   7.4414, -10.4789,   2.7066,   1.7482,   5.7971,
          -4.5808,   3.0911,  -5.1971],
        [ -2.9680,  -1.2369,  -0.0829,  -1.8577,   1.9380,  -0.8374,  -8.2207,
           3.5060,   3.8735,  13.6762]], device='cuda:0')
```



**6. ä¿å­˜æ¨¡å‹**

```python
# save the model parameters
torch.save(model.net.module.state_dict(), "model_parameter.pkl")

net_clone = CnnModel()
net_clone.load_state_dict(torch.load("model_parameter.pkl"))

model_clone = torchkeras.Model(net_clone)
model_clone.compile(loss_func = nn.CrossEntropyLoss(),
                    optimizer= torch.optim.Adam(model.parameters(),lr = 0.02),
                    metrics_dict={"accuracy":accuracy},
                    device = device)
model_clone.evaluate(dl_valid)
```

```
{'val_accuracy': 0.9603441455696202, 'val_loss': 0.14203246376371081}
```

