# 预测政治倾向例子

我们希望根据一个人的年龄、性别、年收入等相互独立的特征，来预测一个人的政治倾向，有三种可预测结果：民主党、共和党、其他党。假设我们当前有两个逻辑回归模型（参数不同），这两个模型都是通过sigmoid的方式得到对于每个预测结果的概率值：

**模型1**：

| COMPUTED    | TARGETS        | CORRECT? |
| :---------- | :------------- | :------- |
| 0.3 0.3 0.4 | 0 0 1 (民主党) | 正确     |
| 0.3 0.4 0.3 | 0 1 0 (共和党) | 正确     |
| 0.1 0.2 0.7 | 1 0 0 (其他党) | 错误     |

> **模型1**: 
>
> 对于样本1和样本2以非常微弱的优势判断正确，对于样本3的判断则彻底错误。



**模型2**：

| COMPUTED    | TARGETS        | CORRECT? |
| :---------- | :------------- | :------- |
| 0.1 0.2 0.7 | 0 0 1 (民主党) | 正确     |
| 0.1 0.7 0.2 | 0 1 0 (共和党) | 正确     |
| 0.3 0.4 0.3 | 1 0 0 (其他党) | 错误     |

> **模型2**:
>
> 对于样本1和样本2判断非常准确，对于样本3判断错误，但是相对来说没有错得太离谱。



## Classification Error（分类错误率）

最为直接的损失函数定义为：
$$
classification \quad error=\frac{\text {count of error items}}{\text {count of all items}}
$$

- 模型1: $\quad$ classification error $=\frac{1}{3}$
- 模型2: $\quad$ classification error $=\frac{1}{3}$

我们知道，**模型1**和**模型2**虽然都是预测错了1个，但是相对来说**模型2**表现得更好，损失函数值照理来说应该更小，但是，很遗憾的是， classification error并不能判断出来，所以这种损失函数虽然好理解，但表现不太好。



## Mean Squared Error (均方误差)

均方误差损失也是一种比较常见的损失函数，其定义为：
$$
M S E=\frac{1}{n} \sum_{i}^{n}\left(\hat{y}_{i}-y_{i}\right)^{2}
$$



**模型1:**
$$
\begin{array}{l}\text { sample } 1 \operatorname{loss}=(0.3-0)^{2}+(0.3-0)^{2}+(0.4-1)^{2}=0.54 \\\text { sample } 2 \operatorname{loss}=(0.3-0)^{2}+(0.4-1)^{2}+(0.3-0)^{2}=0.54 \\\text { sample } 3 \operatorname{loss}=(0.1-1)^{2}+(0.2-0)^{2}+(0.7-0)^{2}=1.32\end{array}
$$

对所有样本的Ioss求平均：

$$
M S E=\frac{0.54+0.54+1.32}{3}=0.8
$$
**模型2：**



$$
\begin{array}{c}\text { sample } 1 \operatorname{loss}&=(0.1-0)^{2}+(0.2-0)^{2}+(0.7-1)^{2}=0.138 \\
\text { sample } 2 \operatorname{loss}&=(0.1-0)^{2}+(0.7-1)^{2}+(0.2-0)^{2}=0.138 \\
\text { sample } 3 \operatorname{loss}&=(0.3-1)^{2}+(0.4-0)^{2}+(0.3-0)^{2}=0.72\end{array}
$$



对所有样本的loss求平均：
$$
M S E=\frac{0.138+0.138+0.72}{3}=0.332
$$


我们发现，MSE能够判断出来**模型2**优于**模型1**，那为什么不采样这种损失函数呢？主要原因是逻辑回归配合MSE损失函数时，采用梯度下降法进行学习时，会出现模型一开始训练时，**学习速率非常慢**的情况（[MSE损失函数](https://zhuanlan.zhihu.com/p/35707643)）。

有了上面的直观分析，我们可以清楚的看到，对于分类问题的损失函数来说，分类错误率和均方误差损失都不是很好的损失函数，下面我们来看一下交叉熵损失函数的表现情况。



# 交叉熵损失函数

**Cross Entropy Error Function**

## 二分类

在二分的情况下，模型最后需要预测的结果只有两种情况，对于每个类别我们的预测得到的概率为$p$ 和$1-p$ 。此时表达式为：
$$
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]
$$
其中：

- $y_i$ —— 表示样本i的label，正类为1，负类为0
- $p_i$—— 表示样本$i$预测为正的概率



## 多分类

多分类的情况实际上就是对二分类的扩展：
$$
L=\frac{1}{N} \sum_{i} L_{i}=\frac{1}{N} \sum_{i}-\sum_{c=1}^{M} y_{i c} \log \left(p_{i c}\right)
$$
其中：
- $M$ ——类别的数量；
- $y_{ic}$ ——**指示变量**（0或1）,如果该类别和样本$i$的类别相同就是1，否则是0；
- $p_{ic}$——对于观测样本$i$属于类别$c$的**预测概率**。



现在我们利用这个表达式计算上面例子中的损失函数值：

**模型1：**
$$
\begin{aligned}
\text { sample 1 } \operatorname{loss} &=-(0 \times \log 0.3+0 \times \log 0.3+1 \times \log 0.4)=0.91 \\
\text { sample 2 } \operatorname{loss} &=-(0 \times \log 0.3+1 \times \log 0.4+0 \times \log 0.3)=0.91 \\
\text { sample 3  } \operatorname{loss} &=-(1 \times \log 0.1+0 \times \log 0.2+0 \times \log 0.7)=2.30
\end{aligned}
$$
对所有样本的Ioss求平均：

$$
L=\frac{0.91+0.91+2.3}{3}=1.37
$$

**模型2：**
$$
\begin{array}{l}
\text { sample } 1 \text { loss }=-(0 \times \log 0.1+0 \times \log 0.2+1 \times \log 0.7)=0.35 \\
\text { sample } 2 \text { loss }=-(0 \times \log 0.1+1 \times \log 0.7+0 \times \log 0.2)=0.35 \\
\text { sample } 3 \text { loss }=-(1 \times \log 0.3+0 \times \log 0.4+0 \times \log 0.4)=1.20
\end{array}
$$
对所有样本的Ioss求平均：
$$
L=\frac{0.35+0.35+1.2}{3}=0.63
$$
可以发现，交叉熵损失函数可以捕捉到**模型1**和**模型2**预测效果的差异。



## 函数性质

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/损失函数-20201215-224442-556076.png)

可以看出，该函数是凸函数，求导时能够得到**全局最优值**。



# 学习过程

交叉熵损失函数经常用于分类问题中，特别是在神经网络做分类问题时，也经常使用交叉熵作为损失函数，此外，由于交叉熵涉及到计算每个类别的概率，所以交叉熵几乎每次都和**sigmoid(或softmax)函数**一起出现。

我们用神经网络最后一层输出的情况，来看一眼整个模型预测、获得损失和学习的流程：

1. 神经网络最后一层得到每个类别的得分**scores**。
2. 该得分经过**sigmoid(或softmax)函数**获得概率输出。
3. 模型预测的类别概率输出与真实类别的one hot形式进行交叉熵损失函数的计算。

学习任务分为二分类和多分类情况，我们分别讨论这两种情况的学习过程。



## 二分类情况

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/损失函数-20201215-224442-567315.jpg)

<center>二分类交叉熵损失函数学习过程</center>

如上图所示，求导过程可分成三个子过程，即拆成三项偏导的乘积：
$$
\frac{\partial L_{i}}{\partial w_{i}}=\frac{1}{N} \frac{\partial L_{i}}{\partial w_{i}}=\frac{1}{N} \frac{\partial L_{i}}{\partial p_{i}} \cdot \frac{\partial p_{i}}{\partial s_{i}} \cdot \frac{\partial s_{i}}{\partial w_{i}}
$$

1. 计算第一项：$\quad \frac{\partial L_{i}}{\partial p_{i}}$

$$
L_{i}=-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]
$$

- $p_{i}$ 表示样本预测为True的概率;
- $y_{i}$ 表示样本$i$为True时等于1，否则等于0;


$$
\begin{aligned}
\frac{\partial L_{i}}{\partial p_{i}} &=\frac{\partial-\left[y_{i} \cdot \log \left(p_{i}\right)+\left(1-y_{i}\right) \cdot \log \left(1-p_{i}\right)\right]}{\partial p_{i}} \\
&=-\frac{y_{i}}{p_{i}}-\left[\left(1-y_{i}\right) \cdot \frac{1}{1-p_{i}} \cdot(-1)\right] \\
&=-\frac{y_{i}}{p_{i}}+\frac{1-y_{i}}{1-p_{i}}
\end{aligned}
$$

2. 计算第二项：**$\quad \frac{\partial p_{i}}{\partial s_{i}}$**

这一项要计算的是sigmoid函数对于score的导数，我们先回顾一下sigmoid函数和分数求导的公式：
$$
p=\sigma(s)=\frac{e^{s}}{1+e^{s}}
$$

$$
f^{\prime}(x)=\frac{g(x)}{h(x)}=\frac{g^{\prime}(x) h(x)-g(x) h^{\prime}(x)}{h^{2}(x)}
$$

$$
\begin{aligned}
\frac{\partial p_{i}}{\partial s_{i}} &=\frac{\left(e^{s_{i}}\right)^{\prime} \cdot\left(1+e^{s_{i}}\right)-e^{s_{i}} \cdot\left(1+e^{s_{i}}\right)^{\prime}}{\left(1+e^{s_{i}}\right)^{2}} \\
&=\frac{e^{s_{i}} \cdot\left(1+e^{s_{i}}\right)-e^{s_{i}} \cdot e^{s_{i}}}{\left(1+e^{s_{i}}\right)^{2}} \\
&=\frac{e^{s_{i}}}{\left(1+e^{s_{i}}\right)^{2}} \\
&=\frac{e^{s_{i}}}{1+e^{s_{i}}} \cdot \frac{1}{1+e^{s_{i}}} \\
&=\sigma\left(s_{i}\right) \cdot\left[1-\sigma\left(s_{i}\right)\right]
\end{aligned}
$$

3. 计算第三项：**$\quad \frac{\partial s_{i}}{\partial w_{i}}$**

一般来说，scores是输入的线性函数作用的结果，所以有：
$$
\quad \frac{\partial s_{i}}{\partial w_{i}} = x_i
$$


> 计算结果  **$\quad \frac{\partial L_{i}}{\partial w_{i}}$**

$$
\begin{aligned}
\frac{\partial L_{i}}{\partial w_{i}} &=\frac{\partial L_{i}}{\partial p_{i}} \cdot \frac{\partial p_{i}}{\partial s_{i}} \cdot \frac{\partial s_{i}}{\partial w_{i}} \\
&=\left[-\frac{y_{i}}{p_{i}}+\frac{1-y_{i}}{1-p_{i}}\right] \cdot \sigma\left(s_{i}\right) \cdot\left[1-\sigma\left(s_{i}\right)\right] \cdot x_{i} \\
&=\left[-\frac{y_{i}}{\sigma\left(s_{i}\right)}+\frac{1-y_{i}}{1-\sigma\left(s_{i}\right)}\right] \cdot \sigma\left(s_{i}\right) \cdot\left[1-\sigma\left(s_{i}\right)\right] \cdot x_{i} \\
&=\left[-\frac{y_{i}}{\sigma\left(s_{i}\right)} \cdot \sigma\left(s_{i}\right) \cdot\left(1-\sigma\left(s_{i}\right)\right)+\frac{1-y_{i}}{1-\sigma\left(s_{i}\right)} \cdot \sigma\left(s_{i}\right) \cdot\left(1-\sigma\left(s_{i}\right)\right)\right] \cdot x_{i} \\
&=\left[-y_{i}+y_{i} \cdot \sigma\left(s_{i}\right)+\sigma\left(s_{i}\right)-y_{i} \cdot \sigma\left(s_{i}\right)\right] \cdot x_{i} \\
&=\left[\sigma\left(s_{i}\right)-y_{i}\right] \cdot x_{i}
\end{aligned}
$$



可以看到，我们得到了一个非常漂亮的结果，所以，使用交叉熵损失函数，不仅可以很好的衡量模型的效果，又可以很容易的进行求导计算。



## 多分类情况

# 4. 优缺点

**优点**

在用梯度下降法做参数更新的时候，模型学习的速度取决于两个值：一、**学习率**；二、**偏导值**。其中，学习率是我们需要设置的超参数，所以我们重点关注偏导值。从上面的式子中，我们发现，偏导值的大小取决于$x_i$和$\left[\sigma\left(s_{i}\right)-y_{i}\right]$，我们重点关注后者，后者的大小值反映了我们模型的错误程度，该值越大，说明模型效果越差，但是该值越大同时也会使得偏导值越大，从而模型学习速度更快。所以，使用逻辑函数得到概率，并结合交叉熵当损失函数时，在模型效果差的时候学习速度比较快，在模型效果好的时候学习速度变慢。

**缺点**

sigmoid(softmax)+cross-entropy loss 擅长于学习类间的信息，因为它采用了类间竞争机制，它只关心对于正确标签预测概率的准确性，忽略了其他非正确标签的差异，导致学习到的特征比较散。基于这个问题的优化有很多，比如对softmax进行改进，如L-Softmax、SM-Softmax、AM-Softmax等。



# 资料

1. [损失函数 - 交叉熵损失函数](https://zhuanlan.zhihu.com/p/35709485)

