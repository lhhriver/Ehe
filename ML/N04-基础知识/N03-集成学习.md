集成学习(ensemble learning)可以说是现在非常火爆的机器学习方法了。它本身不是一个单独的机器学习算法，而是通过构建并结合多个机器学习器来完成学习任务。也就是我们常说的"博采众长"。集成学习可以用于分类问题集成，回归问题集成，特征选取集成，异常点检测集成等等，可以说所有的机器学习领域都可以看到集成学习的身影。本文就对集成学习的原理做一个总结。



# 集成学习概述

从下图，我们可以对集成学习的思想做一个概括。对于训练集数据，我们通过训练若干个个体学习器，通过一定的结合策略，就可以最终形成一个强学习器，以达到博采众长的目的。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/集成学习-20201215-223657-874618.png)



也就是说，集成学习有两个主要的问题需要解决:

- 第一是如何得到若干个个体学习器
- 第二是如何选择一种结合策略，将这些个体学习器集合成一个强学习器。



# 集成学习之个体学习器

上一节我们讲到，集成学习的第一个问题就是如何得到若干个个体学习器。这里我们有两种选择。

- 第一种就是所有的个体学习器都是一个种类的，或者说是**同质的**。比如都是决策树个体学习器，或者都是神经网络个体学习器。
- 第二种是所有的个体学习器不全是一个种类的，或者说是**异质的**。比如我们有一个分类问题，对训练集采用支持向量机个体学习器，逻辑回归个体学习器和朴素贝叶斯个体学习器来学习，再通过某种结合策略来确定最终的分类强学习器。

目前来说，**同质个体学习器的应用是最广泛的**，一般我们常说的集成学习的方法都是指的同质个体学习器。而同质个体学习器使用最多的模型是CART决策树和神经网络。

同质个体学习器按照个体学习器之间是否存在依赖关系可以分为两类：
- 第一个是个体学习器之间存在**强依赖关系**，一系列个体学习器基本都需要**串行生成**，代表算法是**boosting**系列算法。
- 第二个是个体学习器之间**不存在强依赖关系**，一系列个体学习器可以**并行生成**，代表算法是**bagging**和**随机森林**（Random Forest）系列算法。



# 集成学习之boosting

boosting的算法原理我们可以用一张图做一个概括如下：

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/集成学习-20201215-223657-876616.png)



从图中可以看出，Boosting算法的工作机制是首先从训练集用初始权重训练出一个弱学习器1，根据弱学习的学习误差率表现来更新训练样本的权重，**使得之前弱学习器1学习误差率高的训练样本点的权重变高，使得这些误差率高的点在后面的弱学习器2中得到更多的重视**。然后基于调整权重后的训练集来训练弱学习器2，如此重复进行，直到弱学习器数达到事先指定的数目T，最终将这T个弱学习器通过集合策略进行整合，得到最终的强学习器。

Boosting系列算法里最著名算法主要有**AdaBoost算法**和**提升树(boosting tree)系列算法**。提升树系列算法里面应用最广泛的是**梯度提升树(Gradient Boosting Tree)**。



# 集成学习之bagging

## 随机采样

对于这里的随机采样有必要做进一步的介绍，这里一般采用的是**自助采样法（Bootstap sampling）**,即对于m个样本的原始训练集，我们每次先随机采集一个样本放入采样集，接着把该样本**放回**，也就是说下次采样时该样本仍有可能被采集到，这样采集m次，最终可以得到m个样本的采样集，由于是随机采样，这样每次的采样集是和原始训练集不同的，和其他采样集也是不同的，这样得到多个不同的弱学习器。

**随机森林**是bagging的一个特化进阶版，所谓的特化是因为**随机森林的弱学习器都是决策树**。所谓的进阶是**随机森林在bagging的`样本`随机采样基础上，又加上了`特征`的随机选择**，其基本思想没有脱离bagging的范畴。bagging和随机森林算法的原理在后面的文章中会专门来讲。

Bagging的算法原理和boosting不同，它的弱学习器之间没有依赖关系，可以并行生成，我们可以用一张图做一个概括如下：

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/集成学习-20201215-223657-892824.png)



从上图可以看出，bagging的个体弱学习器的训练集是通过**随机采样**得到的。通过T次的随机采样，我们就可以得到T个采样集，对于这T个采样集，我们可以分别独立的训练出T个弱学习器，再对这T个弱学习器通过集合策略来得到最终的强学习器。

从上图可以看出，Bagging的弱学习器之间的确没有boosting那样的联系。它的特点在"随机采样"。那么什么是随机采样？

**随机采样**(bootsrap)就是从我们的训练集里面采集固定个数的样本，但是每采集一个样本后，都将样本放回。也就是说，之前采集到的样本在放回后有可能继续被采集到。对于我们的Bagging算法，一般会随机采集和训练集样本数m一样个数的样本。这样得到的**采样集和训练集样本的个数相同，但是样本内容不同**。如果我们对有m个样本训练集做T次的随机采样，则由于随机性，T个采样集各不相同。

注意到这和GBDT的子采样是不同的。**GBDT的子采样是无放回采样，而Bagging的子采样是放回采样**。

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1-\frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1-\frac{1}{m})^m$。当$m \to \infty$时，$(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。

对于这部分大约36.8%的没有被采样到的数据，我们常常称之为**袋外数据**(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。

bagging的集合策略也比较简单，对于分类问题，通常使用简单**投票法**，得到最多票数的类别或者类别之一为最终的模型输出。对于回归问题，通常使用简单**平均法**，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于Bagging算法每次都进行采样来训练模型，因此**泛化能力很强**，对于降低模型的方差很有作用。当然对于**训练集的拟合程度就会差一些**，也就是模型的**偏差**会大一些。



## bagging算法流程

相对于Boosting系列的Adaboost和GBDT，bagging算法要简单的多。

- **输入**为样本集$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱学习器算法, 弱分类器迭代次数T。

- **输出**为最终的强分类器$f(x)$ 
	
	1. 对于$t=1,2...,T$: 
		a) 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_t$ 。
		b) 用采样集$D_t$训练第t个弱学习器$G_t(x)$ 。
	2. 如果是分类算法预测，则T个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，T个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。
	
	

# 集成学习之结合策略

在上面几节里面我们主要关注于学习器，提到了学习器的结合策略但没有细讲，本节就对集成学习之结合策略做一个总结。我们假定我得到的T个弱学习器是$\{h_1,h_2,...h_T\}$。

## 平均法

对于数值类的回归预测问题，通常使用的结合策略是平均法，也就是说，对于若干个弱学习器的输出进行平均得到最终的预测输出。

最简单的平均是算术平均，也就是说最终预测是:
$$
H(x) = \frac{1}{T}\sum\limits_{1}^{T}h_i(x)
$$

如果每个个体学习器有一个权重w，则最终预测是:
$$
H(x) = \sum\limits_{i=1}^{T}w_ih_i(x)
$$

其中$w_i$是个体学习器$h_i$的权重，通常有:
$$
w_i \geq 0 ,\;\;\; \sum\limits_{i=1}^{T}w_i = 1
$$

## 投票法

对于分类问题的预测，我们通常使用的是投票法。假设我们的预测类别是$\{c_1,c_2,...c_K\}$,对于任意一个预测样本x，我们的T个弱学习器的预测结果分别是$(h_1(x), h_2(x)...h_T(x))$。

1. 最简单的投票法是相对多数投票法，也就是我们常说的**少数服从多数**，也就是T个弱学习器的对样本x的预测结果中，数量最多的类别$c_i$为最终的分类类别。如果不止一个类别获得最高票，则随机选择一个做最终类别。
2. 稍微复杂的投票法是绝对多数投票法，也就是我们常说的要**票过半数**。在相对多数投票法的基础上，不光要求获得最高票，还要求票过半数。否则会拒绝预测。
3. 更加复杂的是加权投票法，和加权平均法一样，每个弱学习器的分类票数要乘以一个权重，最终将各个类别的**加权票数求和**，最大的值对应的类别为最终类别。

```python
from sklearn.ensemble import VotingClassifier # 投票分类器VotingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC

from sklearn.model_selection import train_test_split
from sklearn.datasets import load_iris

iris = load_iris()
data = iris.data
target = iris.target

X_train,X_test,y_train,y_test = train_test_split(data,target)

# 注意模型数量是奇数个 防止投出平票
knn = KNeighborsClassifier()
lgc = LogisticRegression()
dtree = DecisionTreeClassifier()
gnb = GaussianNB()
svc = SVC()

# estimators  传入各个要使用的模型
# list of (string, estimator) tuples  由好多元组组成的列表
estimators = []
estimators.append(('knn',knn))  # 以 名字 和 模型对象 构成的一个元组
estimators.append(('lgc',lgc))
estimators.append(('dtree',dtree))
estimators.append(('nb',gnb))
estimators.append(('svc',svc))

voting = VotingClassifier(estimators, voting='hard')

voting.fit(X_train,y_train)

voting.score(X_test,y_test)  # 每一个测试样本 使用不同的5个模型 都得到一个结果 看哪一个分类结果 得票多
# 投票的方式 可能不会把分数提高很多
# 但是 如果数据没问题的话 至少能保证 准确率不会低得离谱
```



## 学习法

上两节的方法都是对弱学习器的结果做平均或者投票，相对比较简单，但是可能学习误差较大，于是就有了学习法这种方法，对于学习法，代表方法是**stacking**，当使用stacking的结合策略时， 我们不是对弱学习器的结果做简单的逻辑处理，而是再加上一层学习器，也就是说，我们**将训练集弱学习器的学习结果作为输入，将训练集的输出作为输出，重新训练一个学习器来得到最终结果**。

在这种情况下，我们将弱学习器称为初级学习器，将用于结合的学习器称为次级学习器。对于测试集，我们首先用初级学习器预测一次，得到次级学习器的输入样本，再用次级学习器预测一次，得到最终的预测结果。



# Stacking

层次融合的思想，第一层用多个基本模型，然后将多个基本模型的结果作为第二层的输入，第二层一般用LR进行训练（这么做主要是为了匹配每个基本模型的权重）。

 `Stacking的两种思想`

> **第一种思想**

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/集成学习-20201215-223657-851347.png)



假设有12000条数据样本，将样本集分为训练集(training data)10000条和测试集(testing data)2000条。

**第一层**： 采用4个模型（假设其分别是RF、ET、GBDT、XGB），分别对训练集进行训练，然后将预测的结果作为下一层的输入。

**Step1**：将训练集分为5折

1. 分别用第2、3、4、5折训练一个RF，用训练好的RF直接预测第1折训练数据；
2. 分别用第1、3、4、5折训练一个新的RF，用训练好的RF直接预测第2折训练数据；
3. 分别用第1、2、4、5折训练一个新的RF，用训练好的RF直接预测第3折训练数据；
4. 分别用第1、2、3、5折训练一个新的RF，用训练好的RF直接预测第4折训练数据；
5. 分别用第1、2、3、4折训练一个新的RF，用训练好的RF直接预测第5折训练数据；

训练后，可以得到10000个1维的RF对training data的预测结果(2000*5)，对于testing data，用上面训练得到的5个RF，预测出2000个5维的预测结果，然后对其取平均，2000个1维的RF预测结果。

**Step2**：另外3个模型同理。

最终第一层中，training data会输出10000个4维的预测结果，将这个结果作为第二层训练集的输入。testing data 会输出2000个4维的结果，将这个结果作为第二层预测集的输入。

**第二层**： 将上一层的结果带入新的模型中，进行训练再预测，第二层的模型一般为了防止过拟合会采用简单的模型。




> **第二种思想**

第二层的输入数据，除了第一层的训练结果外，还包括了原始特征。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/集成学习-20201215-223657-871594.png)



使用Stacking，组合多个模型，非常耗时，但它也有优点：

1. 它可以帮你打败当前学术界性能最好的算法；
2. 我们有可能将集成的知识迁移到到简单的分类器上；
3. 自动化的大型集成策略可以通过添加正则项有效的对抗过拟合,且并不需要太多的调参和特征选择。所以从原则上讲, stacking非常适合于那些“懒人”；
4. 这是目前提升机器学习效果最好的方法,或者说是最效率的方法human ensemble learning。
	



```python
'''
Stacking的实现案例
'''
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.datasets.samples_generator import make_blobs

'''创建训练的数据集'''
data, target = make_blobs(n_samples=50000,
                          centers=2,
                          random_state=0,
                          cluster_std=0.60)

'''模型融合中使用到的各个单模型'''
clfs = [
    RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),
    RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),
    ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),
    ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),
    GradientBoostingClassifier(learning_rate=0.05,
                               subsample=0.5,
                               max_depth=6,
                               n_estimators=5)
]

'''切分一部分数据作为测试集'''
X, X_predict, y, y_predict = train_test_split(data,
                                              target,
                                              test_size=0.33,
                                              random_state=2017)

dataset_blend_train = np.zeros((X.shape[0], len(clfs)))
dataset_blend_test = np.zeros((X_predict.shape[0], len(clfs)))

'''5折stacking'''
n_folds = 5
skf = StratifiedKFold(n_folds)

for j, clf in enumerate(clfs):
    '''依次训练各个单模型'''
    dataset_blend_test_j = np.zeros((X_predict.shape[0], skf.get_n_splits()))
    for i, (train, test) in enumerate(skf.split(X,y)):
        '''使用第i个部分作为预测，剩余的部分来训练模型，获得其预测的输出作为第i部分的新特征'''
        X_train, y_train, X_test, y_test = X[train], y[train], X[test], y[test]
        clf.fit(X_train, y_train)
        y_submission = clf.predict_proba(X_test)[:, 1]
        dataset_blend_train[test, j] = y_submission
        dataset_blend_test_j[:, i] = clf.predict_proba(X_predict)[:, 1]
        
    dataset_blend_test[:, j] = dataset_blend_test_j.mean(1)
    print("val auc Score: %f" % roc_auc_score(y_predict, dataset_blend_test[:, j]))

clf = GradientBoostingClassifier(learning_rate=0.02,
                                 subsample=0.5,
                                 max_depth=6,
                                 n_estimators=30)
clf.fit(dataset_blend_train, y)
y_submission = clf.predict_proba(dataset_blend_test)[:, 1]

print("Linear stretch of predictions to [0,1]")
y_submission = (y_submission - y_submission.min()) / (y_submission.max()-y_submission.min())
print("blend result")
print(roc_auc_score(y_predict, y_submission))
```



# Blending

层次融合的思想，使用不相交的数据，将样本集分为训练集train和测试集test，再将训练集train数据划分为两部分(d1,d2)，用对d1训练的模型去预测d2和test。用上一轮d2的预测值和标签去训练新的分类器，然后把test的new features输入作为最终的预测值。

**优点：**

1. 比stacking简单（因为不用进行k次的交叉验证来获得stacker feature）；
2. 避开了一个信息泄露问题：generlizers和stacker使用了不一样的数据集；
3. 在团队建模过程中，不需要给队友分享自己的随机种子；

**缺点：**

1. 使用了很少的数据（是划分hold-out作为测试集，并非cv）；
2. blender可能会过拟合（其实大概率是第一点导致的）；
3. stacking使用多次的CV会比较稳健。
	

```python
'''blending'''
from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
import numpy as np
from sklearn.metrics import roc_auc_score
from sklearn.datasets.samples_generator import make_blobs

'''创建训练的数据集'''
data, target = make_blobs(n_samples=50000, 
                          centers=2,
                          random_state=0, 
                          cluster_std=0.6)

'''模型融合中使用到的各个单模型'''
clfs = [RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),
        RandomForestClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),
        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='gini'),
        ExtraTreesClassifier(n_estimators=5, n_jobs=-1, criterion='entropy'),
        GradientBoostingClassifier(learning_rate=0.05, subsample=0.5, max_depth=6, n_estimators=5)]


'''切分一部分数据作为测试集'''
X, X_predict, y, y_predict = train_test_split(
    data, target, test_size=0.33, random_state=2020)

'''切分训练数据集为d1,d2两部分'''
X_d1, X_d2, y_d1, y_d2 = train_test_split(X, y, test_size=0.5, random_state=2020)
dataset_d2 = np.zeros((X_d2.shape[0], len(clfs)))
dataset_predict = np.zeros((X_predict.shape[0], len(clfs)))

for j, clf in enumerate(clfs):
    clf.fit(X_d1, y_d1)
    y_submission = clf.predict_proba(X_d2)[:, 1]
    dataset_d2[:, j] = y_submission
    '''对于测试集，直接用这k个模型的预测值作为新的特征'''
    dataset_predict[:, j] = clf.predict_proba(X_predict)[:, 1]
    print("val auc Score: %f" % roc_auc_score(y_predict, dataset_predict[:, j]))

    
'''融合使用的模型'''
clf = GradientBoostingClassifier(learning_rate=0.02, 
                                 subsample=0.5, 
                                 max_depth=6, 
                                 n_estimators=30)
clf.fit(dataset_d2, y_d2)
y_submission = clf.predict_proba(dataset_predict)[:, 1]

print("Linear stretch of predictions to [0,1]")
y_submission = (y_submission - y_submission.min()) / (y_submission.max()-y_submission.min())
print("blend result")
print("val auc Score: %f" % (roc_auc_score(y_predict, y_submission)))
```



# 解答题

## Bagging，Boosting二者之间的区别

1. 样本选择：

- Bagging：训练集是在原始集中**有放回**选取的，从原始集中选出的各轮训练集之间是独立的。
- Boosting：每一轮的**训练集不变**，只是训练集中每个样例在分类器中的样本权重发生变化，而权值是根据上一轮的分类结果进行调整。

2. 样例权重：

- Bagging：使用均匀取样，每个样例的**权重相等**。
- Boosting：根据错误率**不断调整**样例的权重，错误率越大则权重越大。

3. 预测函数：

- Bagging：所有预测函数的**权重相等**。
- Boosting：每个弱分类器都有相应的权重，对于分类**误差小的分类器会有更大的权重**。

4. 并行计算：

- Bagging：各个预测函数可以**并行生成**。
- Boosting：各个预测函数只能**顺序生成**，因为后一个模型参数需要前一轮模型的结果。
	



## Stacking，Blending二者之间的区别

Blending方式和Stacking方式很类似，相比Stacking更简单点，两者的区别是：

- blending是直接准备好一部分留出集只在留出集上继续预测，用不相交的数据训练不同的Base Model，将它们的输出取（加权）平均，blending实现简单，但对训练数据利用少了。
-  stacking使用多折交叉验证，比使用单一留出集更加稳健。
- blending和stacking都挺好的，可以根据偏好进行选择，可以一部分做blending，一部分做stacking。



## Boosting和Bagging

1. 随机森林

随机森林改变了决策树容易过拟合的问题，这主要是由两个操作所优化的：

- Boostrap从袋内有放回的抽取样本值。
- 每次随机抽取一定数量的特征（通常为sqr(n)）。

**分类问题**：采用Bagging投票的方式选择类别频次最高的

**回归问题**：直接取每颗树结果的平均值。



2. Boosting之AdaBoost

Boosting的本质实际上是一个加法模型，通过改变训练样本权重学习多个分类器并进行一些线性组合。而Adaboost就是加法模型+指数损失函数+前项分布算法。Adaboost就是从弱分类器出发反复训练，在其中不断调整数据权重或者是概率分布，同时提高前一轮被弱分类器误分的样本的权值。最后用分类器进行投票表决（但是分类器的重要性不同）。



3. Boosting之GBDT

将基分类器变成二叉树，回归用二叉回归树，分类用二叉分类树。和上面的Adaboost相比，回归树的损失函数为平方损失，同样可以用指数损失函数定义分类问题。但是对于一般损失函数怎么计算呢？GBDT（梯度提升决策树）是为了解决一般损失函数的优化问题，方法是**用损失函数的负梯度在当前模型的值来模拟回归问题中残差的近似值**。

注：由于GBDT很容易出现过拟合的问题，所以推荐的GBDT深度不要超过6，而随机森林可以在15以上。



4. Xgboost

这个工具主要有以下几个特点：

- 支持线性分类器。
- 可以自定义损失函数，并且可以用二阶偏导。
- 加入了正则化项：叶节点数、每个叶节点输出score的L2-norm。
- 支持特征抽样。
- 在一定情况下支持并行，只有在建树的阶段才会用到，每个节点可以并行的寻找分裂特征。


