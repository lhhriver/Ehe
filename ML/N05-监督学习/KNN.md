K近邻法(k-nearest neighbors,KNN)是一种很基本的机器学习方法了，在我们平常的生活中也会不自主的应用。比如，我们判断一个人的人品，只需要观察他来往最密切的几个人的人品好坏就可以得出了。这里就运用了KNN的思想。KNN方法既可以做分类，也可以做回归，这点和决策树算法相同。

KNN做回归和分类的主要区别在于最后做预测时候的决策方式不同。
- KNN做分类预测时，一般是选择**多数表决法**，即训练集里和预测的样本特征最近的K个样本，预测为里面有最多类别数的类别。
- 而KNN做回归时，一般是选择**平均法**，即最近的K个样本的样本输出的平均值作为回归预测值。

由于两者区别不大，虽然本文主要是讲解KNN的分类方法，但思想对KNN的回归方法也适用。由于scikit-learn里只使用了**蛮力实现**(brute-force)，**KD树实现**(KDTree)和**球树(BallTree)**实现，本文只讨论这几种算法的实现原理。其余的实现方法比如BBF树，MVP树等，在这里不做讨论。



# KNN算法三要素

KNN算法我们主要要考虑三个重要的要素，对于固定的训练集，只要这三点确定了，算法的预测方式也就决定了。这三个最终的要素是：

1. **k值的选取**。
2. **距离度量的方式**。
3. **分类决策规则**。

对于分类决策规则，一般都是使用前面提到的多数表决法。所以我们重点是关注与k值的选择和距离的度量方式。

对于k值的选择，没有一个固定的经验，一般根据样本的分布，选择一个较小的值，可以通过交叉验证选择一个合适的k值。

选择**较小的k值**，就相当于用较小的领域中的训练实例进行预测，训练误差会减小，只有与输入实例较近或相似的训练实例才会对预测结果起作用，与此同时带来的问题是泛化误差会增大，换句话说，**K值的越小，决策区域越多，就意味着整体模型变得复杂，容易发生过拟合**；
选择**较大的k值**，就相当于用较大领域中的训练实例进行预测，其**优点是可以减少泛化误差，但缺点是训练误差会增大**。这时候，与输入实例较远（不相似的）训练实例也会对预测器作用，使预测发生错误，且**K值的增大就意味着整体的模型变得简单**。

一个极端是k等于样本数m，则完全没有分类，此时无论输入实例是什么，都只是简单的预测它属于在训练实例中最多的类，模型过于简单。

 对于距离的度量，我们有很多的距离度量方式，但是最常用的是欧式距离，即对于两个n维向量x和y，两者的**欧式距离**定义为：

$$
D(x,y) = \sqrt{(x_1-y_1)^2 + (x_2-y_2)^2 + ... + (x_n-y_n)^2} = \sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}
$$
大多数情况下，欧式距离可以满足我们的需求，我们不需要再去操心距离的度量。

当然我们也可以用他的距离度量方式。比如**曼哈顿距离**，定义为：
$$
D(x,y) =|x_1-y_1| + |x_2-y_2| + ... + |x_n-y_n| =\sum\limits_{i=1}^{n}|x_i-y_i|
$$
更加通用点，比如**闵可夫斯基距离**(Minkowski Distance)，定义为：
$$
D(x,y) =\sqrt[p]{(|x_1-y_1|)^p + (|x_2-y_2|)^p + ... + (|x_n-y_n|)^p} =\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}
$$
可以看出，欧式距离是闵可夫斯基距离距离在p=2时的特例，而曼哈顿距离是p=1时的特例。



# KNN算法蛮力实现

从本节起，我们开始讨论KNN算法的实现方式。首先我们看看最想当然的方式。

既然我们要找到k个最近的邻居来做预测，那么我们只需要计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离即可，接着多数表决，很容易做出预测。这个方法的确简单直接，在样本量少，样本特征少的时候有效。但是在实际运用中很多时候用不上，为什么呢？因为我们经常碰到样本的特征数有上千以上，样本量有几十万以上，如果我们这要去预测少量的测试集样本，算法的时间效率很成问题。因此，这个方法我们一般称之为蛮力实现。比较适合于少量样本的简单模型的时候用。

既然蛮力实现在特征多，样本多的时候很有局限性，那么我们有没有其他的好办法呢？有！这里我们讲解两种办法，一个是KD树实现，一个是球树实现。



# KNN算法之KD树实现原理

KD树算法没有一开始就尝试对测试样本分类，而是先对训练集建模，建立的模型就是KD树，建好了模型再对测试集做预测。所谓的KD树就是K个特征维度的树，注意这里的K和KNN中的K的意思不同。KNN中的K代表最近的K个样本，KD树中的K代表样本特征的维数。为了防止混淆，后面我们称特征维数为n。

KD树算法包括三步，第一步是**建树**，第二部是**搜索最近邻**，最后一步是**预测**。

## KD树的建立

我们首先来看建树的方法。

1. KD树建树采用的是从m个样本的n维特征中，分别计算n个特征的取值的方差，用**方差最大的第k维特征$n_k$来作为根节点**。
2. 对于这个特征，我们选择特征$n_k$的**取值的中位数$n_{kv}$对应的样本作为划分点**，对于所有第k维特征的取值小于$n_{kv}$的样本，我们划入左子树，对于第k维特征的取值大于等于$n_{kv}$的样本，我们划入右子树。
3. 对于左子树和右子树，我们采用和刚才同样的办法来找方差最大的特征来做更节点，**递归**的生成KD树。



**例：**有二维样本6个，{(2,3)，(5,4)，(9,6)，(4,7)，(8,1)，(7,2)}，构建kd树的具体步骤为：

**1）**找到划分的特征。6个数据点在x，y维度上的数据方差分别为6.97，5.37，所以在x轴上方差更大，用第1维特征建树。

**2）**确定划分点（7,2）。根据x维上的值将数据排序，6个数据的中值(所谓中值，即中间大小的值)为7，所以划分点的数据是（7,2）。这样，该节点的分割超平面就是通过（7,2）并垂直于：划分点维度的直线x=7；

**3）**确定左子空间和右子空间。 分割超平面x=7将整个空间分为两部分：x<=7的部分为左子空间，包含3个节点={(2,3),(5,4),(4,7)}；另一部分为右子空间，包含2个节点={(9,6)，(8,1)}。

**4）**用同样的办法划分左子树的节点{(2,3), (5,4), (4,7)}和右子树的节点{(9,6)，(8,1)}。最终得到KD树。

最后得到的KD树如下：

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-081405.png)

## KD树搜索最近邻

当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。

1. 对于一个目标点，我们首先在KD树里面找到包含目标点的叶子节点。
2. 以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部。
3. 然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻,有的话就更新最近邻。如果不相交那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。
4. 当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。

从上面的描述可以看出，KD树划分后可以大大减少无效的最近邻搜索，很多样本点由于所在的超矩形体和超球体不相交，根本不需要计算距离。大大节省了计算时间。

我们用3.1建立的KD树，来看对点(2, 4.5)找最近邻的过程。

对应的图如下：

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-092352.png)

先进行二叉查找，先从（7,2）查找到（5,4）节点，在进行查找时是由y = 4为分割超平面的，由于查找点为y值为4.5，因此进入右子空间（上）查找到（4,7），形成搜索路径<(7,2)，(5,4)，(4,7)>，即**包含目标点的叶子节点**，但（4,7）与目标查找点的距离为3.202，而（5,4）与查找点之间的距离为3.041，所以（5,4）为查询点的最近点； 

以（2，4.5）为圆心，以3.041为半径作圆，如下图所示。

可见该圆和y = 4超平面交割，所以需要进入（5,4）左子空间(下)进行查找，也就是将（2,3）节点加入搜索路径中得<(7,2)，(2,3)>；于是接着搜索至（2,3）叶子节点，（2,3）距离（2,4.5）比（5,4）要近，所以最近邻点更新为（2，3），最近距离更新为1.5；

回溯查找至（5,4），直到最后回溯到根结点（7,2）的时候，以（2,4.5）为圆心1.5为半径作圆，并不和x = 7分割超平面交割，如图所示。至此，搜索路径回溯完，返回最近邻点（2,3），最近距离1.5。



## KD树预测

有了KD树搜索最近邻的办法，KD树的预测就很简单了，在KD树搜索最近邻的基础上，我们选择到了第一个最近邻样本，就把它置为已选。在第二轮中，我们忽略置为已选的样本，重新选择最近邻，这样跑k次，就得到了目标的K个最近邻，然后根据多数表决法，如果是KNN**分类**，预测为K个最近邻里面有最多类别数的类别。如果是KNN**回归**，用K个最近邻样本输出的平均值作为回归预测值。

# KNN算法之球树实现原理

KD树算法虽然提高了KNN搜索的效率，但是在某些时候效率并不高，比如当处理不均匀分布的数据集时,不管是近似方形，还是矩形，甚至正方形，都不是最好的使用形状，因为他们都有角。一个例子如下图：

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-100336.png)

如果黑色的实例点离目标点星点再远一点，那么虚线圆会如红线所示那样扩大，导致与左上方矩形的右下角相交，既然相交了，那么就要检查这个左上方矩形，而实际上，最近的点离星点的距离很近，检查左上方矩形区域已是多余。于此我们看见，**KD树把二维平面划分成一个一个矩形，但矩形区域的角却是个难以处理的问题**。

为了优化超矩形体导致的搜索效率的问题，牛人们引入了球树，这种结构可以优化上面的这种问题。

我们现在来看看球树建树和搜索最近邻的算法。

## 球树的建立

球树，顾名思义，就是每个分割块都是超球体，而不是KD树里面的超矩形体。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-108329.png)

我们看看具体的**建树流程**：

**1)** 先构建一个超球体，这个超球体是可以包含所有样本的最小球体。

**2)** 从球中选择一个**离球的中心最远的点**，然后选择**第二个点离第一个点最远**，将球中所有的点分配到离这两个聚类中心最近的一个上，然后计算每个聚类的中心，以及聚类能够包含它所有数据点所需的最小半径。这样我们得到了两个子超球体，和KD树里面的左右子树对应。

**3)** 对于这两个子超球体，递归执行步骤**2)** 最终得到了一个球树。

可以看出KD树和球树类似，主要区别在于球树得到的是节点样本组成的最小超球体，而KD得到的是节点样本组成的超矩形体，这个超球体要与对应的KD树的超矩形体小，这样在做最近邻搜索的时候，可以避免一些无谓的搜索。

## 球树搜索最近邻

使用球树找出给定目标点的最近邻方法是首先**自上而下**贯穿整棵树找出包含目标点所在的叶子，并在这个球里找出与目标点最邻近的点，这将确定出目标点距离它的**最近邻点的一个上限值**。

然后跟KD树查找一样，检查兄弟结点，如果**目标点**到**兄弟结点中心**的距离超过**兄弟结点的半径**与当前的**上限值**之和，那么兄弟结点里不可能存在一个更近的点；否则的话，必须进一步检查位于兄弟结点以下的子树。

检查完兄弟节点后，我们向父节点回溯，继续搜索最小邻近值。当回溯到根节点时，此时的最小邻近值就是最终的搜索结果。

从上面的描述可以看出，**KD树在搜索路径优化时使用的是两点之间的距离来判断，而球树使用的是两边之和大于第三边来判断**，相对来说球树的判断更加复杂，但是却避免了更多的搜索，这是一个权衡。

# KNN算法的扩展

这里我们再讨论下KNN算法的扩展，限定半径最近邻算法。

有时候我们会遇到这样的问题，即样本中某系类别的样本非常的少，甚至少于K，这导致稀有类别样本在找K个最近邻的时候，会把距离其实较远的其他样本考虑进来，而导致预测不准确。为了解决这个问题，我们**限定最近邻的一个最大距离**，也就是说，我们只在一个距离范围内搜索所有的最近邻，这避免了上述问题。这个距离我们一般称为**限定半径**。

接着我们再讨论下另一种扩展，最近**质心算法**。这个算法比KNN还简单。它首先把样本按输出类别归类。对于第$L$类的$C_l$个样本。它会对这$C_l$个样本的$n$维特征中每一维特征求平均值，最终该类别所有维度的$n$个平均值形成所谓的质心点。对于样本中的所有出现的类别，**每个类别会最终得到一个质心点**。当我们做预测时，仅仅需要比较预测样本和这些质心的距离，最小的距离对于的质心类别即为预测的类别。这个算法通常用在**文本分类**处理上。

# KNN算法小结

KNN算法是很基本的机器学习算法了，它非常容易学习，在维度很高的时候也有很好的分类效率，因此运用也很广泛，这里总结下KNN的优缺点。

**KNN的主要优点有：**

1. 理论成熟，思想简单，既可以用来做分类也可以用来做回归。
2. 可用于非线性分类。
3. **训练时间复杂度比支持向量机之类的算法低**，仅为O(n)。
4. 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对**异常点不敏感**。
5. 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于**类域的交叉**或**重叠较多**的待分样本集来说，KNN方法较其他方法更为适合。
6. 该算法比较适用于**样本容量比较大的类域**的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。



**KNN的主要缺点有：**

1. 计算量大，尤其是特征数非常多的时候。
2. 样本不平衡的时候，对**稀有类别的预测准确率低**。
3. KD树，球树之类的模型建立需要**大量的内存**。
4. 使用懒散学习方法，基本上不学习，导致**预测时速度比起逻辑回归之类的算法慢**。
5. 相比决策树模型，KNN模型可解释性不强。



# scikit-learn K近邻法类库使用小结

在K近邻法(KNN)原理小结这篇文章，我们讨论了KNN的原理和优缺点，这里我们就从实践出发，对scikit-learn 中KNN相关的类库使用做一个小结。主要关注于类库调参时的一个经验总结。

## scikit-learn 中KNN相关的类库概述

在scikit-learn 中，与近邻法这一大类相关的类库都在sklearn.neighbors包之中。KNN**分类树**的类是KNeighborsClassifier，KNN**回归树**的类是KNeighborsRegressor。除此之外，还有KNN的扩展，即**限定半径最近邻分类树的类**RadiusNeighborsClassifier和**限定半径最近邻回归树的类**RadiusNeighborsRegressor， 以及最近**质心分类算法**NearestCentroid。

在这些算法中，KNN分类和回归的类参数完全一样。限定半径最近邻法分类和回归的类的主要参数也和KNN基本一样。

比较特别是的最近质心分类算法，由于它是直接选择最近质心来分类，所以仅有两个参数，距离度量和特征选择距离阈值，比较简单，因此后面就不再专门讲述最近质心分类算法的参数。

另外几个在sklearn.neighbors包中但不是做分类回归预测的类也值得关注。kneighbors_graph类返回用KNN时和每个样本最近的K个训练集样本的位置。radius_neighbors_graph返回用限定半径最近邻法时和每个样本在限定半径内的训练集样本的位置。

NearestNeighbors是个大杂烩，它即可以返回用KNN时和每个样本最近的K个训练集样本的位置，也可以返回用限定半径最近邻法时和每个样本最近的训练集样本的位置，常常用在聚类模型中。

## K近邻法和限定半径最近邻法类库参数小结

本节对K近邻法和限定半径最近邻法类库参数做一个总结。包括KNN分类树的类KNeighborsClassifier，KNN回归树的类KNeighborsRegressor， 限定半径最近邻分类树的类RadiusNeighborsClassifier和限定半径最近邻回归树的类RadiusNeighborsRegressor。这些类的重要参数基本相同，因此我们放到一起讲。

|             参数             |                     KNeighborsClassifier                     | KNeighborsRegressor |                  RadiusNeighborsClassifier                   | RadiusNeighborsRegressor |
| :--------------------------: | :----------------------------------------------------------: | :-----------------: | :----------------------------------------------------------: | :----------------------: |
|        K值n_neighbors        | K值的选择与样本分布有关，一般选择一个较小的K值，可以通过交叉验证来选择一个比较优的K值，默认值是5。 |        同左         |                              无                              |            无            |
| 限定半径最近邻法中的半radius |                              无                              |         无          | 半径的选择与样本分布有关，可以通过交叉验证来选择一个较小的半径，尽量保证每类训练样本其他类别样本的距离较远，默认值是1.0。 |           同左           |
|     并行处理任务数n_jobs     | 主要用于多核CPU时的并行处理，加快建立KNN树和预测搜索的速度。一般用默认的-1就可以了，即所有的CPU核都参与计算。 |        同左         |                              无                              |            无            |
| 异常点类别选择outlier_label  |                              无                              |         无          | 主要用于预测时，如果目标点半径内没有任何训练集的样本点时，应该标记的类别，不建议选择默认值 none,因为这样遇到异常点会报错。一般设置为训练集里最多样本的类别。 |            无            |





> 近邻权weights 

主要用于标识每个样本的近邻样本的权重，如果是KNN，就是K个近邻样本的权重，如果是限定半径最近邻，就是在距离在半径以内的近邻样本的权重。可以选择"uniform","distance" 或者自定义权重。

- 默认的"uniform"，意味着所有最近邻样本权重都一样，在做预测时一视同仁。
- 如果是"distance"，则权重和距离成反比例，即距离预测目标更近的近邻具有更高的权重，这样在预测类别或者做回归时，更近的近邻所占的影响因子会更加大。
- 当然，我们也可以自定义权重，即自定义一个函数，输入是距离值，输出是权重值。这样我们可以自己控制不同的距离所对应的权重。

一般来说，如果样本的分布是比较成簇的，即各类样本都在相对分开的簇中时，我们用默认的"uniform"就可以了，如果样本的分布比较乱，规律不好寻找，选择"distance"是一个比较好的选择。如果用"distance"发现预测的效果的还是不好，可以考虑自定义距离权重来调优这个参数。



> KNN和限定半径最近邻法使用的算法algorithm

算法一共有三种，第一种是蛮力实现，第二种是KD树实现，第三种是球树实现。对于这个参数，一共有4种可选输入，

- brute对应第一种蛮力实现，
- kd_tree对应第二种KD树实现，
- ball_tree对应第三种的球树实现， 
- auto则会在上面三种算法中做权衡，

选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现brute。

个人的经验，如果样本少特征也少，使用默认的auto就够了。 如果数据量很大或者特征也很多，用auto建树时间会很长，效率不高，建议选择KD树实现kd_tree，此时如果发现kd_tree速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用ball_tree。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是brute。



> 停止建子树的叶子节点阈值leaf_size

这个值控制了使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 这个值一般依赖于样本的数量，随着样本数量的增加，这个值必须要增加，否则不光建树预测的时间长，还容易过拟合。可以通过交叉验证来选择一个适中的值。

如果使用的算法是蛮力实现，则这个参数可以忽略。



> 距离度量metric

K近邻法和限定半径最近邻法类可以使用的距离度量较多，一般来说默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：

1. 欧式距离&nbsp;“euclidean”: $\sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}$
2. 曼哈顿距离 “manhattan”： $ \sum\limits_{i=1}^{n}|x_i-y_i| $
3. 切比雪夫距离“chebyshev”: $max|x_i-y_i| \ \ (i = 1,2,...n)$
4. &nbsp;闵可夫斯基距离&nbsp;“minkowski”(默认参数):  $\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}$ p=1为曼哈顿距离， p=2为欧式距离。
5. 带权重闵可夫斯基距离&nbsp;“wminkowski”:  $\sqrt[p]{\sum\limits_{i=1}^{n}(w*|x_i-y_i|)^p}$ 其中w为特征权重
6.  标准化欧式距离&nbsp;“seuclidean”: 即对于各特征维度做了归一化以后的欧式距离。此时各样本特征维度的均值为0，方差为1.
7.  马氏距离“mahalanobis”：$\sqrt{(x-y)^TS^{-1}(x-y)}$ 其中，$S^{-1}$为样本协方差矩阵的逆矩阵。当样本分布独立时， S为单位矩阵，此时马氏距离等同于欧式距离。

还有一些其他不是实数的距离度量，一般在KNN之类的算法用不上，这里也就不列了。



> 距离度量附属参数p

p是使用距离度量参数 metric 附属参数，只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。默认为2。



> 距离度量其他附属参数metric_params

一般都用不上，主要是用于带权重闵可夫斯基距离的权重，以及其他一些比较复杂的距离度量的参数。




## 使用KNeighborsClassifier做分类的实例

### 生成随机数据


```python
import numpy as np
import matplotlib.pyplot as plt
%matplotlib inline
from sklearn.datasets.samples_generator import make_classification
# X为样本特征，Y为样本类别输出， 共1000个样本，每个样本2个特征，输出有3个类别，没有冗余特征，每个类别一个簇
X, Y = make_classification(n_samples=1000, 
                           n_features=2, 
                           n_redundant=0,
                           n_clusters_per_class=1, 
                           n_classes=3)
plt.scatter(X[:, 0], X[:, 1], marker='o', c=Y)
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-123286.png)


接着我们用KNN来拟合模型，我们选择K=15，权重为距离远近。代码如下：


```python
from sklearn import neighbors
clf = neighbors.KNeighborsClassifier(n_neighbors = 15 , weights='distance')
clf.fit(X, Y)
```




```python
KNeighborsClassifier(
    algorithm='auto', 
    leaf_size=30, 
    metric='minkowski',
    metric_params=None, 
    n_jobs=None, 
    n_neighbors=15, 
    p=2,
    weights='distance')
```




```python
from matplotlib.colors import ListedColormap
cmap_light = ListedColormap(['#FFAAAA', '#AAFFAA', '#AAAAFF'])
cmap_bold = ListedColormap(['#FF0000', '#00FF00', '#0000FF'])

# 确认训练集的边界
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
# 生成随机数据来做测试集，然后作预测
xx, yy = np.meshgrid(
    np.arange(x_min, x_max, 0.02), 
    np.arange(y_min, y_max, 0.02))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

# 画出测试集数据
Z = Z.reshape(xx.shape)
plt.figure()
plt.pcolormesh(xx, yy, Z, cmap=cmap_light)

# 也画出所有的训练集数据
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=cmap_bold)
plt.xlim(xx.min(), xx.max())
plt.ylim(yy.min(), yy.max())
plt.title("3-Class classification (k = 15, weights = 'distance')")
```




    Text(0.5, 1.0, "3-Class classification (k = 15, weights = 'distance')")




![](https://gitee.com/liuhuihe/Ehe/raw/master/images/KNN-20201215-223658-136252.png)

