DBSCAN(Density-Based Spatial Clustering of Applications with Noise，具有噪声的基于密度的聚类方法)是一种很典型的密度聚类算法，和K-Means，BIRCH这些一般只适用于凸样本集的聚类相比，DBSCAN既可以适用于凸样本集，也可以适用于非凸样本集。下面我们就对DBSCAN算法的原理做一个总结。

# 密度聚类原理

DBSCAN是一种**基于密度的聚类算法**，这类密度聚类算法一般假定类别可以通过样本分布的紧密程度决定。同一类别的样本，他们之间的紧密相连的，也就是说，在该类别任意样本周围不远处一定有同类别的样本存在。

通过将紧密相连的样本划为一类，这样就得到了一个聚类类别。通过将所有各组紧密相连的样本划为各个不同的类别，则我们就得到了最终的所有聚类类别结果。

# DBSCAN密度定义

在上一节我们定性描述了密度聚类的基本思想，本节我们就看看DBSCAN是如何描述密度聚类的。**DBSCAN是基于一组邻域来描述样本集的紧密程度的**，参数($\epsilon$, MinPts)用来描述邻域的样本分布紧密程度。其中，$\epsilon$描述了某一样本的**邻域距离阈值**，MinPts描述了某一样本的距离为ϵ的**邻域中样本个数的阈值**。

假设我的样本集是$D=(x_1,x_2,...,x_m)$,则DBSCAN具体的密度描述定义如下：

**1） $\epsilon$-邻域：**对于$x_j \in D$，其$\epsilon$-邻域包含样本集D中与$x_j$的**距离不大于$\epsilon$的子样本集**，即$N_{\epsilon}(x_j) = \{x_i \in D | distance(x_i,x_j) \leq \epsilon\}$, 这个子样本集的个数记为$|N_{\epsilon}(x_j)|$　  

**2) 核心对象：**对于任一样本$x_j \in D$，如果其$\epsilon$-邻域对应的$N_{\epsilon}(x_j)$**至少包含MinPts个样本**，即如果$|N_{\epsilon}(x_j)| \geq MinPts$，则$x_j$是核心对象。　

**3）密度直达：**如果$x_i$位于$x_j$的$\epsilon$-邻域中，且$x_j$是核心对象，则称$x_i$由$x_j$密度直达。注意反之不一定成立，即此时不能说$x_j$由$x_i$密度直达, 除非且$x_i$也是核心对象。

**4）密度可达：**对于$x_i$和$x_j$, 如果存在样本样本序列$p_1, p_2,...,p_T$,满足$p_1 = x_i, p_T = x_j$, 且$p_{t+1}$由$p_t$密度直达，则称$x_j$由$x_i$密度可达。也就是说，密度可达满足传递性。此时序列中的传递样本$p_1, p_2,...,p_{T-1}$均为核心对象，因为只有核心对象才能使其他样本密度直达。注意密度可达也不满足对称性，这个可以由密度直达的不对称性得出。

**5）密度相连：**对于$x_i$和$x_j$,如果存在核心对象样本$x_k$，使$x_i$和$x_j$均由$x_k$密度可达，则称$x_i$和$x_j$密度相连。注意密度相连关系是满足对称性的。

从下图可以很容易看出理解上述定义，图中MinPts=5，红色的点都是核心对象，因为其ϵ-邻域至少有5个样本。黑色的样本是非核心对象。所有核心对象密度直达的样本在以红色核心对象为中心的超球体内，如果不在超球体内，则不能密度直达。图中用绿色箭头连起来的核心对象组成了密度可达的样本序列。在这些密度可达的样本序列的ϵ-邻域内所有的样本相互都是密度相连的。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-770256.png)

# DBSCAN密度聚类思想

DBSCAN的聚类定义很简单：**由密度可达关系导出的最大密度相连的样本集合，即为我们最终聚类的一个类别**，或者说一个簇。

这个DBSCAN的簇里面可以有一个或者多个核心对象。如果只有一个核心对象，则簇里其他的非核心对象样本都在这个核心对象的$\epsilon$-邻域里；如果有多个核心对象，则簇里的任意一个核心对象的$\epsilon$-邻域中一定有一个其他的核心对象，否则这两个核心对象无法密度可达。这些核心对象的$\epsilon$-邻域里所有的样本的集合组成的一个DBSCAN聚类簇。

那么怎么才能找到这样的簇样本集合呢？DBSCAN使用的方法很简单，它任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。

基本上这就是DBSCAN算法的主要内容了，是不是很简单？但是我们还是有三个问题没有考虑。

第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为**噪音点**。

第二个是**距离的度量**问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。如果大家对于最近邻的思想，距离度量，KD树和球树不熟悉，建议参考之前写的另一篇文章K近邻法(KNN)原理小结。

第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于$\epsilon$，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，那么如果界定这个样本的类别呢？一般来说，此时DBSCAN采用**先来后到**，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法**不是完全稳定的算法**。

# DBSCAN聚类算法

下面我们对DBSCAN聚类算法的流程做一个总结。

**输入**：样本集$D=(x_1,x_2,...,x_m)$，邻域参数$(\epsilon, MinPts)$, 样本距离度量方式

**输出**： 簇划分C

1. 初始化核心对象集合$\Omega = \emptyset$, 初始化聚类簇数k=0，初始化未访问样本集合$\Gamma= D$,  簇划分$C = \emptyset$
2. 对于$j=1,2,...m$, 按下面的步骤找出所有的核心对象：
	- 通过距离度量方式，找到样本$x_j$的$\epsilon$-邻域子样本集$N_{\epsilon}(x_j)$ 
	- 如果子样本集样本个数满足$|N_{\epsilon}(x_j)| \geq MinPts$， 将样本$x_j$加入核心对象样本集合：$\Omega = \Omega \cup \{x_j\}$  
3. 如果核心对象集合$\Omega = \emptyset$，则算法结束，否则转入步骤4。
4. 在核心对象集合$\Omega_{cur} = \{o\}$中，随机选择一个核心对象$o$，初始化当前簇核心对象队列$\Omega_{cur} = \{o\}$, 初始化类别序号k=k+1，初始化当前簇样本集合$C_k =  \{o\}$, 更新未访问样本集合$\Gamma = \Gamma -  \{o\}$ 
5. 如果当前簇核心对象队列$\Omega_{cur} = \emptyset$，则当前聚类簇$C_k$生成完毕, 更新簇划分$C=\{C_1,C_2,...,C_k\}$, 更新核心对象集合$\Omega = \Omega - {C_k}$， 转入步骤3。
6. 在当前簇核心对象队列$\Omega_{cur}$中取出一个核心对象$o^{'}$,通过邻域距离阈值$\epsilon$找出所有的$\epsilon$-邻域子样本集$N_{\epsilon}(o^{'})$，令$\Delta = N_{\epsilon}(o^{'}) \cap \Gamma$, 更新当前簇样本集合$C_k =C_k \cup \Delta$, 更新未访问样本集合$\Gamma = \Gamma - \Delta$,  更新$\Omega_{cur} = \Omega_{cur} \cup (\Delta \cap \Omega) - {o'}$，转入步骤5。

**输出结果**： 簇划分$C=\{C_1,C_2,...,C_k\}$



# DBSCAN小结

和传统的K-Means算法相比，DBSCAN最大的不同就是**不需要输入类别数k**，当然它最大的优势是**可以发现任意形状的聚类簇**，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还**可以找出异常点**，这点和BIRCH算法类似。

那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。

## 优点

DBSCAN的主要优点有：

1. 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。
2. 可以在聚类的同时发现异常点，对数据集中的异常点不敏感。
3. 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

## 缺点

DBSCAN的主要缺点有：

1. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。
2. 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
3.  调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。



# 用scikit-learn学习DBSCAN聚类

在DBSCAN密度聚类算法中，我们对DBSCAN聚类算法的原理做了总结，本文就对如何用scikit-learn来学习DBSCAN聚类做一个总结，重点讲述参数的意义和需要调参的参数。

## scikit-learn中的DBSCAN类

在scikit-learn中，DBSCAN算法类为sklearn.cluster.DBSCAN。要熟练的掌握用DBSCAN类来聚类，除了对DBSCAN本身的原理有较深的理解以外，还要对最近邻的思想有一定的理解。集合这两者，就可以玩转DBSCAN了。

## DBSCAN类重要参数

DBSCAN类的重要参数也分为两类，一类是DBSCAN算法本身的参数，一类是最近邻度量的参数，下面我们对这些参数做一个总结。

1. eps： DBSCAN算法参数，即我们的**$\epsilon$-邻域的距离阈值**，和样本距离超过$\epsilon$的样本点不在$\epsilon$-邻域内。默认值是0.5.一般需要通过在多组值里面选择一个合适的阈值。eps过大，则更多的点会落在核心对象的$\epsilon$-邻域，此时我们的类别数可能会减少， 本来不应该是一类的样本也会被划为一类。反之则类别数可能会增大，本来是一类的样本却被划分开。
2. min_samples： DBSCAN算法参数，即样本点要成为核心对象所需要的**ϵ-邻域的样本数阈值**。默认值是5. 一般需要通过在多组值里面选择一个合适的阈值。通常和eps一起调参。在eps一定的情况下，min_samples过大，则核心对象会过少，此时簇内部分本来是一类的样本可能会被标为噪音点，类别数也会变多。反之min_samples过小的话，则会产生大量的核心对象，可能会导致类别数过少。
3. metric：最近邻距离度量参数。可以使用的距离度量较多，一般来说DBSCAN使用默认的欧式距离（即p=2的闵可夫斯基距离）就可以满足我们的需求。可以使用的距离度量参数有：
	- 欧式距离 “euclidean”: $\sqrt{\sum\limits_{i=1}^{n}(x_i-y_i)^2}$ 
	-  曼哈顿距离 “manhattan”： $\sum\limits_{i=1}^{n}|x_i-y_i|$ 
	- 切比雪夫距离“chebyshev”: $max|x_i-y_i|  (i = 1,2,...n)$  
	- 闵可夫斯基距离 “minkowski”: $\sqrt[p]{\sum\limits_{i=1}^{n}(|x_i-y_i|)^p}$ p=1为曼哈顿距离， p=2为欧式距离。
	- 带权重闵可夫斯基距离 “wminkowski”: $\sqrt[p]{\sum\limits_{i=1}^{n}(w*|x_i-y_i|)^p}$ 其中w为特征权重。
	- 标准化欧式距离 “seuclidean”: 即对于各特征维度做了归一化以后的欧式距离。此时各样本特征维度的均值为0，方差为1.
	- 马氏距离“mahalanobis”：$\sqrt{(x-y)^TS^{-1}(x-y)}$ 其中，$S^{-1}$为样本协方差矩阵的逆矩阵。当样本分布独立时， $S$为单位矩阵，此时马氏距离等同于欧式距离。
	- 还有一些其他不是实数的距离度量，一般在DBSCAN算法用不上，这里也就不列了。
4. algorithm：**最近邻搜索算法参数**，算法一共有三种，第一种是**蛮力实现**，第二种是**KD树实现**，第三种是**球树实现**。这三种方法在K近邻法(KNN)原理小结中都有讲述，如果不熟悉可以去复习下。对于这个参数，一共有4种可选输入，‘brute’对应第一种蛮力实现，‘kd_tree’对应第二种KD树实现，‘ball_tree’对应第三种的球树实现， ‘auto’则会在上面三种算法中做权衡，选择一个拟合最好的最优算法。需要注意的是，如果输入样本特征是稀疏的时候，无论我们选择哪种算法，最后scikit-learn都会去用蛮力实现‘brute’。个人的经验，一般情况使用默认的 ‘auto’就够了。 如果数据量很大或者特征也很多，用"auto"建树时间可能会很长，效率不高，建议选择KD树实现‘kd_tree’，此时如果发现‘kd_tree’速度比较慢或者已经知道样本分布不是很均匀时，可以尝试用‘ball_tree’。而如果输入样本是稀疏的，无论你选择哪个算法最后实际运行的都是‘brute’。
5. leaf_size：最近邻搜索算法参数，为使用KD树或者球树时， 停止建子树的叶子节点数量的阈值。这个值越小，则生成的KD树或者球树就越大，层数越深，建树时间越长，反之，则生成的KD树或者球树会小，层数较浅，建树时间较短。默认是30. 因为这个值一般只影响算法的运行速度和使用内存大小，因此一般情况下可以不管它。
6. p: 最近邻距离度量参数。只用于闵可夫斯基距离和带权重闵可夫斯基距离中p值的选择，p=1为曼哈顿距离， p=2为欧式距离。如果使用默认的欧式距离不需要管这个参数。

以上就是DBSCAN类的主要参数介绍，其实需要调参的就是两个参数eps和min_samples，这两个值的组合对最终的聚类效果有很大的影响。



## scikit-learn DBSCAN聚类实例

首先，我们生成一组随机数据，为了体现DBSCAN在非凸数据的聚类优点，我们生成了三簇数据，两组是非凸的。代码如下：


```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
%matplotlib inline
X1, y1 = datasets.make_circles(n_samples=5000, factor=.6, noise=.05)
X2, y2 = datasets.make_blobs(
    n_samples=1000,
    n_features=2,
    centers=[[1.2, 1.2]],
    cluster_std=[[.1]],
    random_state=9)

X = np.concatenate((X1, X2))
plt.scatter(X[:, 0], X[:, 1], marker='o')
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-783229.png)



```python
# 首先我们看看K-Means的聚类效果，代码如下：

from sklearn.cluster import KMeans
y_pred = KMeans(n_clusters=3, random_state=9).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-797228.png)



```python
# 那么如果使用DBSCAN效果如何呢？我们先不调参，直接用默认参数，看看聚类效果,代码如下：

from sklearn.cluster import DBSCAN
y_pred = DBSCAN().fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-811176.png)


发现输出让我们很不满意，DBSCAN居然认为所有的数据都是一类！


怎么办？看来我们需要对DBSCAN的两个关键的参数eps和min_samples进行调参！从上图我们可以发现，类别数太少，我们需要增加类别数，那么我们可以减少ϵ-邻域的大小，默认是0.5，我们减到0.1看看效果。代码如下：


```python
y_pred = DBSCAN(eps = 0.1).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-829595.png)


可以看到聚类效果有了改进，至少边上的那个簇已经被发现出来了。此时我们需要继续调参增加类别，有两个方向都是可以的，一个是继续减少eps，另一个是增加min_samples。我们现在将min_samples从默认的5增加到10，代码如下：



```python
y_pred = DBSCAN(eps = 0.1, min_samples = 10).fit_predict(X)
plt.scatter(X[:, 0], X[:, 1], c=y_pred)
plt.show()
```


![](https://gitee.com/liuhuihe/Ehe/raw/master/images/DBSCAN-20201215-223659-841567.png)


可见现在聚类效果基本已经可以让我们满意了。

上面这个例子只是帮大家理解DBSCAN调参的一个基本思路，在实际运用中可能要考虑很多问题，以及更多的参数组合，希望这个例子可以给大家一些启发。

