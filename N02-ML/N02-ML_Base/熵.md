# 熵

**熵度量了事物的不确定性，越不确定的事物，它的熵就越大**。如果一个随机变量$X$的可能取值为$X = {x_1, x_2,…, x_k}$，其概率分布为$P(X = x_i) = p(x_i)（i = 1,2, ..., n）$具体的，随机变量$X$的熵的表达式如下：
$$
H(X) = -\sum\limits_xp(x) logp(x)
$$

其中$n$代表$X$的$n$种不同的离散取值。而$p(x)$代表了$X$取值为$x$的概率，$log$为以2或者$e$为底的对数。 把最前面的负号放到最后，便成了：
$$
H(X)=\sum_{x} p(x) \log \frac{1}{p(x)}
$$


> **举个例子**

- 比如X有2个可能的取值，而这两个取值各为1/2时X的熵最大，此时X具有最大的不确定性。值为:
	$$
	H(X) = -(\frac{1}{2}log\frac{1}{2} + \frac{1}{2}log\frac{1}{2}) = log2
	$$

- 如果一个值概率大于1/2，另一个值概率小于1/2，则不确定性减少，对应的熵也会减少。比如一个概率1/3，一个概率2/3，则对应熵为:
	$$
	H(X) = -(\frac{1}{3}log\frac{1}{3} + \frac{2}{3}log\frac{2}{3}) = log3 - \frac{2}{3}log2 < log2)
	$$

# 联合熵

熟悉了一个变量X的熵，很容易推广到多个变量的联合熵，这里给出两个变量X和Y的联合熵表达式：
$$
H(X,Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)
$$

# 条件熵

有了联合熵，又可以得到条件熵的表达式$H(Y|X)$，条件熵类似于条件概率, **用来衡量在已知随机变量X的条件下随机变量Y的不确定性。**且有此式子成立：
$$
H(Y|X) = H(X,Y) – H(X)
$$


> 证明1：

$$
\begin{aligned} 
H(Y|X) & \equiv \sum_{x \in \mathcal{X}} p(x) H(Y|X=x) \\ 
&=-\sum_{x \in \mathcal{X}} p(x) \sum_{y \in \mathcal{Y}} p(y|x) \log p(y|x) \\ 
&=-\sum_{x \in \mathcal{X}} \sum_{y \in \mathcal{Y}} p(y, x) \log p(y|x) \\ 
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(y | x) 
\end{aligned}
$$



$$
\begin{aligned} H(X, Y) &=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y) \\ &=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log (p(y | x) p(x)) \\ &=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(y | x)-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x) \\ &=H(Y | X)-\sum_{x \in \mathcal{X}}(\log p(x)) p(x) \\ &=H(Y | X)-\sum_{x \in \mathcal{X}} p(x) \log p(x) \\ &=H(Y | X)+H(X) \\ &=H(X)+H(Y | X) \end{aligned}
$$

> 证明2：

$$
\begin{align}
H(X, Y)-H(X) &=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y)+\sum_{x \in \mathcal{X}} p(x) \log p(x) \\
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y)+\sum_{x \in \mathcal{X}}\left(\sum_{y \in \mathcal{Y}} p(x, y)\right) \log p(x) \\
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x, y)+\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(x) \\
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log \frac{p(x, y)}{p(x)} \\
&=-\sum_{x \in \mathcal{X}, y \in \mathcal{Y}} p(x, y) \log p(y \mid x) \\
&=H(Y|X)
\end{align}
$$



# 相对熵

又称互熵，交叉熵，Kullback-Leible散度等。设$p(x)$、$q(x)$是$X$中取值的两个概率分布，则$p$对$q$的相对熵是：
$$
D(p \| q)=\sum_{x} p(x) \log \frac{p(x)}{q(x)}=E_{p(x)} \log \frac{p(x)}{q(x)}
$$
在一定程度上，相对熵可以度量两个随机变量的“距离”，且有$D(p||q) ≠D(q||p)$。另外，值得一提的是，$D(p||q)$是必然大于等于0的。



# 互信息

两个随机变量$X$，$Y$的互信息定义为$X$，$Y$的联合分布和各自独立分布乘积的相对熵，用$I(X,Y)$表示：
$$
I(X, Y)=\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)}
$$
且有$I(X,Y)=D(P(X,Y) || P(X)P(Y))$。下面计算下$H(Y)-I(X,Y)$的结果：
$$
\begin{align}
H(Y)-I(X, Y) 
&=-\sum_{y} p(y) \log p(y)-\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
&=-\sum_{y}\left(\sum_{x} p(x, y)\right) \log p(y)-\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
&=-\sum_{x, y} p(x, y) \log p(y)-\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x) p(y)} \\
&=-\sum_{x, y} p(x, y) \log \frac{p(x, y)}{p(x)} \\
&=-\sum_{x, y} p(x, y) \log p(y \mid x) \\
&=H(Y \mid X)
\end{align}
$$
通过上面的计算过程，我们发现竟然有$H(Y)-I(X,Y) = H(Y|X)$。通过条件熵有：$H(Y|X) = H(X,Y) - H(X)$，把前者跟后者结合起来，便有**$I(X,Y)= H(X) + H(Y) - H(X,Y)$**，此结论被多数文献作为互信息的定义。










