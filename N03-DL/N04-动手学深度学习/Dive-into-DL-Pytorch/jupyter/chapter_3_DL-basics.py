# -*- coding: utf-8 -*-
# ---
# jupyter:
#   jupytext:
#     formats: ipynb,py:light
#     text_representation:
#       extension: .py
#       format_name: light
#       format_version: '1.4'
#       jupytext_version: 1.2.4
#   kernelspec:
#     display_name: Python 3
#     language: python
#     name: python3
# ---



# # 3.1 çº¿æ€§å›å½’

# +
import torch
from time import time

print(torch.__version__)
# -

a = torch.ones(1000)
b = torch.ones(1000)

# å°†è¿™ä¸¤ä¸ªå‘é‡æŒ‰å…ƒç´ é€ä¸€åšæ ‡é‡åŠ æ³•:

start = time()
c = torch.zeros(1000)
for i in range(1000):
    c[i] = a[i] + b[i]
print(time() - start)

# å°†è¿™ä¸¤ä¸ªå‘é‡ç›´æ¥åšçŸ¢é‡åŠ æ³•:

start = time()
d = a + b
print(time() - start)

# **ç»“æœå¾ˆæ˜æ˜¾ï¼Œåè€…æ¯”å‰è€…æ›´çœæ—¶ã€‚å› æ­¤ï¼Œæˆ‘ä»¬åº”è¯¥å°½å¯èƒ½é‡‡ç”¨çŸ¢é‡è®¡ç®—ï¼Œä»¥æå‡è®¡ç®—æ•ˆç‡ã€‚**

# å¹¿æ’­æœºåˆ¶ä¾‹å­ğŸŒ°ï¼š

a = torch.ones(3)
b = 10
print(a + b)


# # 3.2 çº¿æ€§å›å½’çš„ä»é›¶å¼€å§‹å®ç°

# +
# %matplotlib inline
import torch
from IPython import display
from matplotlib import pyplot as plt
import numpy as np
import random

print(torch.__version__)
torch.set_default_tensor_type('torch.FloatTensor')
# -

# ## 3.2.1 ç”Ÿæˆæ•°æ®é›†

num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = torch.randn(num_examples, num_inputs)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()),
                       dtype=torch.float)

print(features[0], labels[0])


# +
def use_svg_display():
    # ç”¨çŸ¢é‡å›¾æ˜¾ç¤º
    display.set_matplotlib_formats('svg')


def set_figsize(figsize=(8, 6)):
    use_svg_display()
    # è®¾ç½®å›¾çš„å°ºå¯¸
    plt.rcParams['figure.figsize'] = figsize


# # åœ¨../d2lzh_pytorché‡Œé¢æ·»åŠ ä¸Šé¢ä¸¤ä¸ªå‡½æ•°åå°±å¯ä»¥è¿™æ ·å¯¼å…¥
# import sys
# sys.path.append("..")
# from d2lzh_pytorch import *

set_figsize()
plt.scatter(features[:, 1].numpy(), labels.numpy(), 1)


# -

# ## 3.2.2 è¯»å–æ•°æ®

# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def data_iter(batch_size, features, labels):
    num_examples = len(features)
    indices = list(range(num_examples))
    random.shuffle(indices)  # æ ·æœ¬çš„è¯»å–é¡ºåºæ˜¯éšæœºçš„
    for i in range(0, num_examples, batch_size):
        j = torch.LongTensor(
            indices[i:min(i + batch_size, num_examples)])  # æœ€åä¸€æ¬¡å¯èƒ½ä¸è¶³ä¸€ä¸ªbatch
        yield features.index_select(0, j), labels.index_select(0, j)


# +
batch_size = 10

for X, y in data_iter(batch_size, features, labels):
    print(X, '\n', y)
    break
# -

# ## 3.2.3 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

w = torch.tensor(np.random.normal(0, 0.01, (num_inputs, 1)), dtype=torch.float)
b = torch.zeros(1)

w.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True) 


# ## 3.2.4 å®šä¹‰æ¨¡å‹

def linreg(X, w, b):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return torch.mm(X, w) + b


# ## 3.2.5 å®šä¹‰æŸå¤±å‡½æ•°

def squared_loss(y_hat, y):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨pytorch_d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    return (y_hat - y.view(y_hat.size())) ** 2 / 2


# ## 3.2.6 å®šä¹‰ä¼˜åŒ–ç®—æ³•

def sgd(params, lr, batch_size):  # æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
    for param in params:
        param.data -= lr * param.grad / batch_size # æ³¨æ„è¿™é‡Œæ›´æ”¹paramæ—¶ç”¨çš„param.data


# ## 3.2.7 è®­ç»ƒæ¨¡å‹

# +
lr = 0.03
num_epochs = 3
net = linreg
loss = squared_loss

for epoch in range(num_epochs):  # è®­ç»ƒæ¨¡å‹ä¸€å…±éœ€è¦num_epochsä¸ªè¿­ä»£å‘¨æœŸ
    # åœ¨æ¯ä¸€ä¸ªè¿­ä»£å‘¨æœŸä¸­ï¼Œä¼šä½¿ç”¨è®­ç»ƒæ•°æ®é›†ä¸­æ‰€æœ‰æ ·æœ¬ä¸€æ¬¡ï¼ˆå‡è®¾æ ·æœ¬æ•°èƒ½å¤Ÿè¢«æ‰¹é‡å¤§å°æ•´é™¤ï¼‰ã€‚X
    # å’Œyåˆ†åˆ«æ˜¯å°æ‰¹é‡æ ·æœ¬çš„ç‰¹å¾å’Œæ ‡ç­¾
    for X, y in data_iter(batch_size, features, labels):
        l = loss(net(X, w, b), y).sum()  # læ˜¯æœ‰å…³å°æ‰¹é‡Xå’Œyçš„æŸå¤±
        l.backward()  # å°æ‰¹é‡çš„æŸå¤±å¯¹æ¨¡å‹å‚æ•°æ±‚æ¢¯åº¦
        sgd([w, b], lr, batch_size)  # ä½¿ç”¨å°æ‰¹é‡éšæœºæ¢¯åº¦ä¸‹é™è¿­ä»£æ¨¡å‹å‚æ•°
        
        # ä¸è¦å¿˜äº†æ¢¯åº¦æ¸…é›¶
        w.grad.data.zero_()
        b.grad.data.zero_()
    train_l = loss(net(features, w, b), labels)
    print('epoch %d, loss %f' % (epoch + 1, train_l.mean().item()))
# -

print(true_w, '\n', w)
print(true_b, '\n', b)


# # 3.3 çº¿æ€§å›å½’çš„ç®€æ´å®ç°

# +
import torch
from torch import nn
import numpy as np
torch.manual_seed(1)

print(torch.__version__)
torch.set_default_tensor_type('torch.FloatTensor')
# -

# ## 3.3.1 ç”Ÿæˆæ•°æ®é›†

num_inputs = 2
num_examples = 1000
true_w = [2, -3.4]
true_b = 4.2
features = torch.tensor(np.random.normal(0, 1, (num_examples, num_inputs)), dtype=torch.float)
labels = true_w[0] * features[:, 0] + true_w[1] * features[:, 1] + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)

# ## 3.3.2 è¯»å–æ•°æ®

# +
import torch.utils.data as Data

batch_size = 10

# å°†è®­ç»ƒæ•°æ®çš„ç‰¹å¾å’Œæ ‡ç­¾ç»„åˆ
dataset = Data.TensorDataset(features, labels)

# æŠŠ dataset æ”¾å…¥ DataLoader
data_iter = Data.DataLoader(
    dataset=dataset,      # torch TensorDataset format
    batch_size=batch_size,      # mini batch size
    shuffle=True,               # è¦ä¸è¦æ‰“ä¹±æ•°æ® (æ‰“ä¹±æ¯”è¾ƒå¥½)
    num_workers=2,              # å¤šçº¿ç¨‹æ¥è¯»æ•°æ®
)
# -

for X, y in data_iter:
    print(X, '\n', y)
    break


# ## 3.3.3 å®šä¹‰æ¨¡å‹

# +
class LinearNet(nn.Module):
    def __init__(self, n_feature):
        super(LinearNet, self).__init__()
        self.linear = nn.Linear(in_features=n_feature, out_features=1,bias=True)

    def forward(self, x):
        y = self.linear(x)
        return y
    
net = LinearNet(num_inputs)
print(net) # ä½¿ç”¨printå¯ä»¥æ‰“å°å‡ºç½‘ç»œçš„ç»“æ„

# +
# å†™æ³•ä¸€
net = nn.Sequential(
    nn.Linear(num_inputs, 1)
    # æ­¤å¤„è¿˜å¯ä»¥ä¼ å…¥å…¶ä»–å±‚
    )

# å†™æ³•äºŒ
net = nn.Sequential()
net.add_module('linear', nn.Linear(num_inputs, 1))
# net.add_module ......

# å†™æ³•ä¸‰
from collections import OrderedDict
net = nn.Sequential(OrderedDict([
          ('linear', nn.Linear(num_inputs, 1))
          # ......
        ]))

print(net)
print(net[0])
# -

for param in net.parameters():
    print(param)

# ## 3.3.4 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

# +
from torch.nn import init

init.normal_(net[0].weight, mean=0.0, std=0.01)
init.constant_(net[0].bias, val=0.0)  # ä¹Ÿå¯ä»¥ç›´æ¥ä¿®æ”¹biasçš„data: net[0].bias.data.fill_(0)
# -

for param in net.parameters():
    print(param)

# ## 3.3.5 å®šä¹‰æŸå¤±å‡½æ•°

loss = nn.MSELoss()

# ## 3.3.6 å®šä¹‰ä¼˜åŒ–ç®—æ³•

# +
import torch.optim as optim

optimizer = optim.SGD(net.parameters(), lr=0.03)
print(optimizer)

# +
# ä¸ºä¸åŒå­ç½‘ç»œè®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡
# optimizer =optim.SGD([
#                 # å¦‚æœå¯¹æŸä¸ªå‚æ•°ä¸æŒ‡å®šå­¦ä¹ ç‡ï¼Œå°±ä½¿ç”¨æœ€å¤–å±‚çš„é»˜è®¤å­¦ä¹ ç‡
#                 {'params': net.subnet1.parameters()}, # lr=0.03
#                 {'params': net.subnet2.parameters(), 'lr': 0.01}
#             ], lr=0.03)

# +
# # è°ƒæ•´å­¦ä¹ ç‡
# for param_group in optimizer.param_groups:
#     param_group['lr'] *= 0.1 # å­¦ä¹ ç‡ä¸ºä¹‹å‰çš„0.1å€
# -

# ## 3.3.7 è®­ç»ƒæ¨¡å‹

num_epochs = 3
for epoch in range(1, num_epochs + 1):
    for X, y in data_iter:
        output = net(X)
        l = loss(output, y.view(-1, 1))
        optimizer.zero_grad() # æ¢¯åº¦æ¸…é›¶ï¼Œç­‰ä»·äºnet.zero_grad()
        l.backward()
        optimizer.step()
    print('epoch %d, loss: %f' % (epoch, l.item()))

dense = net[0]
print(true_w, dense.weight.data)
print(true_b, dense.bias.data)


# # 3.5 å›¾åƒåˆ†ç±»æ•°æ®é›†ï¼ˆFashion-MNISTï¼‰

# +
import torch
import torchvision
import torchvision.transforms as transforms
import matplotlib.pyplot as plt
import time
import sys
sys.path.append("..")  # ä¸ºäº†å¯¼å…¥ä¸Šå±‚ç›®å½•çš„d2lzh_pytorch
import d2lzh_pytorch as d2l

print(torch.__version__)
print(torchvision.__version__)
# -

# ## 3.5.1 è·å–æ•°æ®é›†

# +
mnist_train = torchvision.datasets.FashionMNIST(
    root='~/Datasets/FashionMNIST',
    train=True,
    download=True,
    transform=transforms.ToTensor())

mnist_test = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST',
                                               train=False,
                                               download=True,
                                               transform=transforms.ToTensor())
# -

print(type(mnist_train))
print(len(mnist_train), len(mnist_test))

feature, label = mnist_train[0]
print(feature.shape, feature.dtype)  # Channel x Height X Width
print(label)

mnist_PIL = torchvision.datasets.FashionMNIST(root='~/Datasets/FashionMNIST',
                                              train=True,
                                              download=True)
PIL_feature, label = mnist_PIL[0]
print(PIL_feature)


# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def get_fashion_mnist_labels(labels):
    text_labels = [
        't-shirt', 'trouser', 'pullover', 'dress', 'coat', 'sandal', 'shirt',
        'sneaker', 'bag', 'ankle boot'
    ]
    return [text_labels[int(i)] for i in labels]


# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzhåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def show_fashion_mnist(images, labels):
    d2l.use_svg_display()
    # è¿™é‡Œçš„_è¡¨ç¤ºæˆ‘ä»¬å¿½ç•¥ï¼ˆä¸ä½¿ç”¨ï¼‰çš„å˜é‡
    _, figs = plt.subplots(1, len(images), figsize=(12, 12))
    for f, img, lbl in zip(figs, images, labels):
        f.imshow(img.view((28, 28)).numpy())
        f.set_title(lbl)
        f.axes.get_xaxis().set_visible(False)
        f.axes.get_yaxis().set_visible(False)
    plt.show()


X, y = [], []
for i in range(10):
    X.append(mnist_train[i][0])
    y.append(mnist_train[i][1])
show_fashion_mnist(X, get_fashion_mnist_labels(y))

# ## 3.5.2 è¯»å–å°æ‰¹é‡

# +
batch_size = 256
if sys.platform.startswith('win'):
    num_workers = 0  # 0è¡¨ç¤ºä¸ç”¨é¢å¤–çš„è¿›ç¨‹æ¥åŠ é€Ÿè¯»å–æ•°æ®
else:
    num_workers = 4
    
train_iter = torch.utils.data.DataLoader(mnist_train,
                                         batch_size=batch_size,
                                         shuffle=True,
                                         num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(mnist_test,
                                        batch_size=batch_size,
                                        shuffle=False,
                                        num_workers=num_workers)
# -

start = time.time()
for X, y in train_iter:
    continue
print('%.2f sec' % (time.time() - start))


# # 3.6 softmaxå›å½’çš„ä»é›¶å¼€å§‹å®ç°

# +
import torch
import torchvision
import numpy as np
import sys
sys.path.append("..") # ä¸ºäº†å¯¼å…¥ä¸Šå±‚ç›®å½•çš„d2lzh_pytorch
import d2lzh_pytorch as d2l

print(torch.__version__)
print(torchvision.__version__)
# -

# ## 3.6.1 è·å–å’Œè¯»å–æ•°æ®

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# ## 3.6.2 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

# +
num_inputs = 784
num_outputs = 10

W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)
b = torch.zeros(num_outputs, dtype=torch.float)
# -

W.requires_grad_(requires_grad=True)
b.requires_grad_(requires_grad=True) 

X = torch.tensor([[1, 2, 3], 
                  [4, 5, 6]])
print(X.sum(dim=0, keepdim=True))
print(X.sum(dim=1, keepdim=True))


# ## 3.6.3 å®ç°softmaxè¿ç®—

def softmax(X):
    X_exp = X.exp()
    partition = X_exp.sum(dim=1, keepdim=True)
    return X_exp / partition  # è¿™é‡Œåº”ç”¨äº†å¹¿æ’­æœºåˆ¶


X = torch.rand((2, 5))
X_prob = softmax(X)
print(X_prob, X_prob.sum(dim=1))


# ## 3.6.4 å®šä¹‰æ¨¡å‹

def net(X):
    return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)


# ## 3.6.5 å®šä¹‰æŸå¤±å‡½æ•°

y_hat = torch.tensor([[0.1, 0.3, 0.6], 
                      [0.3, 0.2, 0.5]])
y = torch.LongTensor([0, 2])
y_hat.gather(1, y.view(-1, 1))


def cross_entropy(y_hat, y):
    return - torch.log(y_hat.gather(1, y.view(-1, 1)))


# ## 3.6.6 è®¡ç®—åˆ†ç±»å‡†ç¡®ç‡

def accuracy(y_hat, y):
    return (y_hat.argmax(dim=1) == y).float().mean().item()


print(accuracy(y_hat, y))


# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨ã€‚è¯¥å‡½æ•°å°†è¢«é€æ­¥æ”¹è¿›ï¼šå®ƒçš„å®Œæ•´å®ç°å°†åœ¨â€œå›¾åƒå¢å¹¿â€ä¸€èŠ‚ä¸­æè¿°
def evaluate_accuracy(data_iter, net):
    acc_sum, n = 0.0, 0
    for X, y in data_iter:
        acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
        n += y.shape[0]
    return acc_sum / n


print(evaluate_accuracy(test_iter, net))

# ## 3.6.7 è®­ç»ƒæ¨¡å‹

# +
num_epochs, lr = 5, 0.1

# æœ¬å‡½æ•°å·²ä¿å­˜åœ¨d2lzh_pytorchåŒ…ä¸­æ–¹ä¾¿ä»¥åä½¿ç”¨
def train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size,
              params=None, lr=None, optimizer=None):
    for epoch in range(num_epochs):
        train_l_sum, train_acc_sum, n = 0.0, 0.0, 0
        for X, y in train_iter:
            y_hat = net(X)
            l = loss(y_hat, y).sum()
            
            # æ¢¯åº¦æ¸…é›¶
            if optimizer is not None:
                optimizer.zero_grad()
            elif params is not None and params[0].grad is not None:
                for param in params:
                    param.grad.data.zero_()
            
            l.backward()
            if optimizer is None:
                d2l.sgd(params, lr, batch_size)
            else:
                optimizer.step()  # â€œsoftmaxå›å½’çš„ç®€æ´å®ç°â€ä¸€èŠ‚å°†ç”¨åˆ°
            
            train_l_sum += l.item()
            train_acc_sum += (y_hat.argmax(dim=1) == y).sum().item()
            n += y.shape[0]
        test_acc = evaluate_accuracy(test_iter, net)
        print(('epoch %d, loss %.4f, train acc %.3f, test acc %.3f'
              % (epoch + 1, train_l_sum / n, train_acc_sum / n, test_acc)))

train_ch3(net, train_iter, test_iter, cross_entropy, num_epochs, batch_size, [W, b], lr)
# -

# ## 3.6.8 é¢„æµ‹

# +
X, y = iter(test_iter).next()

true_labels = d2l.get_fashion_mnist_labels(y.numpy())
pred_labels = d2l.get_fashion_mnist_labels(net(X).argmax(dim=1).numpy())
titles = [true + '\n' + pred for true, pred in zip(true_labels, pred_labels)]

d2l.show_fashion_mnist(X[0:9], titles[0:9])
# -


# # 3.7 softmaxå›å½’çš„ç®€æ´å®ç°

# +
import torch
from torch import nn
from torch.nn import init
import numpy as np
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

# ## 3.7.1 è·å–å’Œè¯»å–æ•°æ®

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# ## 3.7.2 å®šä¹‰å’Œåˆå§‹åŒ–æ¨¡å‹

# +
num_inputs = 784
num_outputs = 10

# class LinearNet(nn.Module):
#     def __init__(self, num_inputs, num_outputs):
#         super(LinearNet, self).__init__()
#         self.linear = nn.Linear(num_inputs, num_outputs)
#     def forward(self, x): # x shape: (batch, 1, 28, 28)
#         y = self.linear(x.view(x.shape[0], -1))
#         return y
    
# net = LinearNet(num_inputs, num_outputs)

class FlattenLayer(nn.Module):
    def __init__(self):
        super(FlattenLayer, self).__init__()
    def forward(self, x): # x shape: (batch, *, *, ...)
        return x.view(x.shape[0], -1)

from collections import OrderedDict
net = nn.Sequential(
        # FlattenLayer(),
        # nn.Linear(num_inputs, num_outputs)
        OrderedDict([
          ('flatten', FlattenLayer()),
          ('linear', nn.Linear(num_inputs, num_outputs))])
        )
# -

init.normal_(net.linear.weight, mean=0, std=0.01)
init.constant_(net.linear.bias, val=0) 

# ## 3.7.3 softmaxå’Œäº¤å‰ç†µæŸå¤±å‡½æ•°

loss = nn.CrossEntropyLoss()

# ## 3.7.4 å®šä¹‰ä¼˜åŒ–ç®—æ³•

optimizer = torch.optim.SGD(net.parameters(), lr=0.1)

# ## 3.7.5 è®­ç»ƒæ¨¡å‹

num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)


# # 3.8 å¤šå±‚æ„ŸçŸ¥æœº

# +
# %matplotlib inline
import torch
import numpy as np
import matplotlib.pylab as plt
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

from jupyterthemes import jtplot
jtplot.style()


# ## 3.8.2 æ¿€æ´»å‡½æ•°

def xyplot(x_vals, y_vals, name):
    d2l.set_figsize(figsize=(5, 2.5))
    d2l.plt.plot(x_vals.detach().numpy(), y_vals.detach().numpy())
    d2l.plt.xlabel('x')
    d2l.plt.ylabel(name + '(x)')


# ### 3.8.2.1 ReLUå‡½æ•°

x = torch.arange(-8.0, 8.0, 0.1, requires_grad=True)
y = x.relu()
xyplot(x, y, 'relu')

y.sum().backward()
xyplot(x, x.grad, 'grad of relu')

# ### 3.8.2.2 sigmoidå‡½æ•°

y = x.sigmoid()
xyplot(x, y, 'sigmoid')

x.grad.zero_()
y.sum().backward()
xyplot(x, x.grad, 'grad of sigmoid')

# ### 3.8.2.3 tanhå‡½æ•°

y = x.tanh()
xyplot(x, y, 'tanh')

x.grad.zero_()
y.sum().backward()
xyplot(x, x.grad, 'grad of tanh')


# # 3.9 å¤šå±‚æ„ŸçŸ¥æœºçš„ä»é›¶å¼€å§‹å®ç°

# +
import torch
import numpy as np
import sys
sys.path.append("..") # ä¸ºäº†å¯¼å…¥ä¸Šå±‚ç›®å½•çš„d2lzh_pytorch
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

# ## 3.9.1 è·å–å’Œè¯»å–æ•°æ®

batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)

# ## 3.9.2 å®šä¹‰æ¨¡å‹å‚æ•°

# +
num_inputs, num_outputs, num_hiddens = 784, 10, 256

W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float)
b1 = torch.zeros(num_hiddens, dtype=torch.float)

W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float)
b2 = torch.zeros(num_outputs, dtype=torch.float)

params = [W1, b1, W2, b2]
for param in params:
    param.requires_grad_(requires_grad=True)


# -

# ## 3.9.3 å®šä¹‰æ¿€æ´»å‡½æ•°

def relu(X):
    return torch.max(input=X, other=torch.tensor(0.0))


# ## 3.9.4 å®šä¹‰æ¨¡å‹

def net(X):
    X = X.view((-1, num_inputs))
    H = relu(torch.matmul(X, W1) + b1)
    return torch.matmul(H, W2) + b2


# ## 3.9.5 å®šä¹‰æŸå¤±å‡½æ•°

loss = torch.nn.CrossEntropyLoss()

# ## 3.9.6 è®­ç»ƒæ¨¡å‹

num_epochs, lr = 5, 100.0
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)




# # 3.10 å¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°

# +
import torch
from torch import nn
from torch.nn import init
import numpy as np
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

# ## 3.10.1 å®šä¹‰æ¨¡å‹

# +
num_inputs, num_outputs, num_hiddens = 784, 10, 256
    
net = nn.Sequential(
        d2l.FlattenLayer(),
        nn.Linear(num_inputs, num_hiddens),
        nn.ReLU(),
        nn.Linear(num_hiddens, num_outputs), 
        )
    
for params in net.parameters():
    init.normal_(params, mean=0, std=0.01)
# -

# ## 3.10.2 è¯»å–æ•°æ®å¹¶è®­ç»ƒæ¨¡å‹

# +
batch_size = 256
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
loss = torch.nn.CrossEntropyLoss()

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)

num_epochs = 5
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)
# -


# # 3.11 æ¨¡å‹é€‰æ‹©ã€æ¬ æ‹Ÿåˆå’Œè¿‡æ‹Ÿåˆ
# ## 3.11.4 å¤šé¡¹å¼å‡½æ•°æ‹Ÿåˆå®éªŒ

# +
# %matplotlib inline
import torch
import numpy as np
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

# ### 3.11.4.1 ç”Ÿæˆæ•°æ®é›†

n_train, n_test, true_w, true_b = 100, 100, [1.2, -3.4, 5.6], 5
features = torch.randn((n_train + n_test, 1))
poly_features = torch.cat((features, torch.pow(features, 2), torch.pow(features, 3)), 1) 
labels = (true_w[0] * poly_features[:, 0] + true_w[1] * poly_features[:, 1]
          + true_w[2] * poly_features[:, 2] + true_b)
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)

features[:2], poly_features[:2], labels[:2]


# ### 3.11.4.2 å®šä¹‰ã€è®­ç»ƒå’Œæµ‹è¯•æ¨¡å‹

def semilogy(x_vals, y_vals, x_label, y_label, x2_vals=None, y2_vals=None,
             legend=None, figsize=(3.5, 2.5)):
    d2l.set_figsize(figsize)
    d2l.plt.xlabel(x_label)
    d2l.plt.ylabel(y_label)
    d2l.plt.semilogy(x_vals, y_vals)
    if x2_vals and y2_vals:
        d2l.plt.semilogy(x2_vals, y2_vals, linestyle=':')
        d2l.plt.legend(legend)


# +
num_epochs, loss = 100, torch.nn.MSELoss()

def fit_and_plot(train_features, test_features, train_labels, test_labels):
    net = torch.nn.Linear(train_features.shape[-1], 1)
    # é€šè¿‡Linearæ–‡æ¡£å¯çŸ¥ï¼Œpytorchå·²ç»å°†å‚æ•°åˆå§‹åŒ–äº†ï¼Œæ‰€ä»¥æˆ‘ä»¬è¿™é‡Œå°±ä¸æ‰‹åŠ¨åˆå§‹åŒ–äº†
    
    batch_size = min(10, train_labels.shape[0])    
    dataset = torch.utils.data.TensorDataset(train_features, train_labels)
    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)
    
    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            l = loss(net(X), y.view(-1, 1))
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
        train_labels = train_labels.view(-1, 1)
        test_labels = test_labels.view(-1, 1)
        train_ls.append(loss(net(train_features), train_labels).item())
        test_ls.append(loss(net(test_features), test_labels).item())
    print(('final epoch: train loss', train_ls[-1], 'test loss', test_ls[-1]))
    semilogy(list(range(1, num_epochs + 1)), train_ls, 'epochs', 'loss',
             list(range(1, num_epochs + 1)), test_ls, ['train', 'test'])
    print(('weight:', net.weight.data,
          '\nbias:', net.bias.data))


# -

# ### 3.11.4.3 ä¸‰é˜¶å¤šé¡¹å¼å‡½æ•°æ‹Ÿåˆï¼ˆæ­£å¸¸ï¼‰

fit_and_plot(poly_features[:n_train, :], poly_features[n_train:, :], labels[:n_train], labels[n_train:])

# ### 3.11.4.4 çº¿æ€§å‡½æ•°æ‹Ÿåˆï¼ˆæ¬ æ‹Ÿåˆï¼‰

fit_and_plot(features[:n_train, :], features[n_train:, :], labels[:n_train], labels[n_train:])

# ### 3.11.4.5 è®­ç»ƒæ ·æœ¬ä¸è¶³ï¼ˆè¿‡æ‹Ÿåˆï¼‰

fit_and_plot(poly_features[0:2, :], poly_features[n_train:, :], labels[0:2], labels[n_train:])


# # 3.12 æƒé‡è¡°å‡

# +
# %matplotlib inline
import torch
import torch.nn as nn
import numpy as np
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
# -

# ## 3.12.2 é«˜ç»´çº¿æ€§å›å½’å®éªŒ

# +
n_train, n_test, num_inputs = 20, 100, 200
true_w, true_b = torch.ones(num_inputs, 1) * 0.01, 0.05

features = torch.randn((n_train + n_test, num_inputs))
labels = torch.matmul(features, true_w) + true_b
labels += torch.tensor(np.random.normal(0, 0.01, size=labels.size()), dtype=torch.float)
train_features, test_features = features[:n_train, :], features[n_train:, :]
train_labels, test_labels = labels[:n_train], labels[n_train:]


# -

# ## 3.12.3 ä»é›¶å¼€å§‹å®ç°
# ### 3.12.3.1 åˆå§‹åŒ–æ¨¡å‹å‚æ•°

def init_params():
    w = torch.randn((num_inputs, 1), requires_grad=True)
    b = torch.zeros(1, requires_grad=True)
    return [w, b]


# ### 3.12.3.2 å®šä¹‰$L_2$èŒƒæ•°æƒ©ç½šé¡¹

def l2_penalty(w):
    return (w**2).sum() / 2


# ### 3.12.3.3 å®šä¹‰è®­ç»ƒå’Œæµ‹è¯•

# +
batch_size, num_epochs, lr = 1, 100, 0.003
net, loss = d2l.linreg, d2l.squared_loss

dataset = torch.utils.data.TensorDataset(train_features, train_labels)
train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)

def fit_and_plot(lambd):
    w, b = init_params()
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            # æ·»åŠ äº†L2èŒƒæ•°æƒ©ç½šé¡¹
            l = loss(net(X, w, b), y) + lambd * l2_penalty(w)
            l = l.sum()
            
            if w.grad is not None:
                w.grad.data.zero_()
                b.grad.data.zero_()
            l.backward()
            d2l.sgd([w, b], lr, batch_size)
        train_ls.append(loss(net(train_features, w, b), train_labels).mean().item())
        test_ls.append(loss(net(test_features, w, b), test_labels).mean().item())
    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
                 range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('L2 norm of w:', w.norm().item())


# -

# ### 3.12.3.4 è§‚å¯Ÿè¿‡æ‹Ÿåˆ

fit_and_plot(lambd=0)

# ### 3.12.3.5 ä½¿ç”¨æƒé‡è¡°å‡

fit_and_plot(lambd=3)


# ## 3.12.4 ç®€æ´å®ç°

def fit_and_plot_pytorch(wd):
    # å¯¹æƒé‡å‚æ•°è¡°å‡ã€‚æƒé‡åç§°ä¸€èˆ¬æ˜¯ä»¥weightç»“å°¾
    net = nn.Linear(num_inputs, 1)
    nn.init.normal_(net.weight, mean=0, std=1)
    nn.init.normal_(net.bias, mean=0, std=1)
    optimizer_w = torch.optim.SGD(params=[net.weight], lr=lr, weight_decay=wd) # å¯¹æƒé‡å‚æ•°è¡°å‡
    optimizer_b = torch.optim.SGD(params=[net.bias], lr=lr)  # ä¸å¯¹åå·®å‚æ•°è¡°å‡
    
    train_ls, test_ls = [], []
    for _ in range(num_epochs):
        for X, y in train_iter:
            l = loss(net(X), y).mean()
            optimizer_w.zero_grad()
            optimizer_b.zero_grad()
            
            l.backward()
            
            # å¯¹ä¸¤ä¸ªoptimizerå®ä¾‹åˆ†åˆ«è°ƒç”¨stepå‡½æ•°ï¼Œä»è€Œåˆ†åˆ«æ›´æ–°æƒé‡å’Œåå·®
            optimizer_w.step()
            optimizer_b.step()
        train_ls.append(loss(net(train_features), train_labels).mean().item())
        test_ls.append(loss(net(test_features), test_labels).mean().item())
    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'loss',
                 range(1, num_epochs + 1), test_ls, ['train', 'test'])
    print('L2 norm of w:', net.weight.data.norm().item())


fit_and_plot_pytorch(0)

fit_and_plot_pytorch(3)


# +
# %matplotlib inline
import torch
import torch.nn as nn
import numpy as np
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)


# -

def dropout(X, drop_prob):
    X = X.float()
    assert 0 <= drop_prob <= 1
    keep_prob = 1 - drop_prob
    # è¿™ç§æƒ…å†µä¸‹æŠŠå…¨éƒ¨å…ƒç´ éƒ½ä¸¢å¼ƒ
    if keep_prob == 0:
        return torch.zeros_like(X)
    mask = (torch.randn(X.shape) < keep_prob).float()
    
    return mask * X / keep_prob


X = torch.arange(16).view(2, 8)
dropout(X, 0)

dropout(X, 0.5)

dropout(X, 1.0)

# +
num_inputs, num_outputs, num_hiddens1, num_hiddens2 = 784, 10, 256, 256

W1 = torch.tensor(np.random.normal(0, 0.01, size=(num_inputs, num_hiddens1)), dtype=torch.float, requires_grad=True)
b1 = torch.zeros(num_hiddens1, requires_grad=True)
W2 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens1, num_hiddens2)), dtype=torch.float, requires_grad=True)
b2 = torch.zeros(num_hiddens2, requires_grad=True)
W3 = torch.tensor(np.random.normal(0, 0.01, size=(num_hiddens2, num_outputs)), dtype=torch.float, requires_grad=True)
b3 = torch.zeros(num_outputs, requires_grad=True)

params = [W1, b1, W2, b2, W3, b3]

# +
drop_prob1, drop_prob2 = 0.2, 0.5

def net(X, is_training=True):
    X = X.view(-1, num_inputs)
    H1 = (torch.matmul(X, W1) + b1).relu()
    if is_training:  # åªåœ¨è®­ç»ƒæ¨¡å‹æ—¶ä½¿ç”¨ä¸¢å¼ƒæ³•
        H1 = dropout(H1, drop_prob1)  # åœ¨ç¬¬ä¸€å±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚
    H2 = (torch.matmul(H1, W2) + b2).relu()
    if is_training:
        H2 = dropout(H2, drop_prob2)  # åœ¨ç¬¬äºŒå±‚å…¨è¿æ¥åæ·»åŠ ä¸¢å¼ƒå±‚
    return torch.matmul(H2, W3) + b3


# +
# def evaluate_accuracy(data_iter, net):
#     acc_sum, n = 0.0, 0
#     for X, y in data_iter:
#         if isinstance(net, torch.nn.Module):
#             net.eval() # è¯„ä¼°æ¨¡å¼, è¿™ä¼šå…³é—­dropout
#             acc_sum += (net(X).argmax(dim=1) == y).float().sum().item()
#             net.train() # æ”¹å›è®­ç»ƒæ¨¡å¼
#         else: # è‡ªå®šä¹‰çš„æ¨¡å‹
#             if('is_training' in net.__code__.co_varnames): # å¦‚æœæœ‰is_trainingè¿™ä¸ªå‚æ•°
#                 # å°†is_trainingè®¾ç½®æˆFalse
#                 acc_sum += (net(X, is_training=False).argmax(dim=1) == y).float().sum().item() 
#             else:
#                 acc_sum += (net(X).argmax(dim=1) == y).float().sum().item() 
#         n += y.shape[0]
#     return acc_sum / n
# -

num_epochs, lr, batch_size = 5, 100.0, 256 # è¿™é‡Œçš„å­¦ä¹ ç‡è®¾ç½®çš„å¾ˆå¤§ï¼ŒåŸå› åŒ3.9.6èŠ‚ã€‚
loss = torch.nn.CrossEntropyLoss()
train_iter, test_iter = d2l.load_data_fashion_mnist(batch_size)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, params, lr)

# +
net = nn.Sequential(
        d2l.FlattenLayer(),
        nn.Linear(num_inputs, num_hiddens1),
        nn.ReLU(),
        nn.Dropout(drop_prob1),
        nn.Linear(num_hiddens1, num_hiddens2), 
        nn.ReLU(),
        nn.Dropout(drop_prob2),
        nn.Linear(num_hiddens2, 10)
        )

for param in net.parameters():
    nn.init.normal_(param, mean=0, std=0.01)
# -

optimizer = torch.optim.SGD(net.parameters(), lr=0.5)
d2l.train_ch3(net, train_iter, test_iter, loss, num_epochs, batch_size, None, None, optimizer)


# # 3.16 å®æˆ˜Kaggleæ¯”èµ›ï¼šæˆ¿ä»·é¢„æµ‹

from jupyterthemes import jtplot
jtplot.style()

# +
# å¦‚æœæ²¡æœ‰å®‰è£…pandasï¼Œåˆ™åæ³¨é‡Šä¸‹é¢ä¸€è¡Œ
# # !pip install pandas

# %matplotlib inline
import torch
import torch.nn as nn
import numpy as np
import pandas as pd
import sys
sys.path.append("..") 
import d2lzh_pytorch as d2l

print(torch.__version__)
torch.set_default_tensor_type(torch.FloatTensor)
# -

# ## 3.16.2 è·å–å’Œè¯»å–æ•°æ®é›†

train_data = pd.read_csv('../../data/kaggle_house/train.csv')
test_data = pd.read_csv('../../data/kaggle_house/test.csv')

train_data.shape

test_data.shape

train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]]

all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))

# ## 3.16.3 é¢„å¤„ç†æ•°æ®

numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))
# æ ‡å‡†åŒ–åï¼Œæ¯ä¸ªç‰¹å¾çš„å‡å€¼å˜ä¸º0ï¼Œæ‰€ä»¥å¯ä»¥ç›´æ¥ç”¨0æ¥æ›¿æ¢ç¼ºå¤±å€¼
all_features = all_features.fillna(0)

# dummy_na=Trueå°†ç¼ºå¤±å€¼ä¹Ÿå½“ä½œåˆæ³•çš„ç‰¹å¾å€¼å¹¶ä¸ºå…¶åˆ›å»ºæŒ‡ç¤ºç‰¹å¾
all_features = pd.get_dummies(all_features, dummy_na=True)
all_features.shape

n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float)
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float)
train_labels = torch.tensor(train_data.SalePrice.values, dtype=torch.float).view(-1, 1)

# ## 3.16.4 è®­ç»ƒæ¨¡å‹

# +
loss = torch.nn.MSELoss()

def get_net(feature_num):
    net = nn.Linear(feature_num, 1)
    for param in net.parameters():
        nn.init.normal_(param, mean=0, std=0.01)
    return net


# -

def log_rmse(net, features, labels):
    with torch.no_grad():
        # å°†å°äº1çš„å€¼è®¾æˆ1ï¼Œä½¿å¾—å–å¯¹æ•°æ—¶æ•°å€¼æ›´ç¨³å®š
        clipped_preds = torch.max(net(features), torch.tensor(1.0))
        rmse = torch.sqrt(2 * loss(clipped_preds.log(), labels.log()).mean())
    return rmse.item()


def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []
    dataset = torch.utils.data.TensorDataset(train_features, train_labels)
    train_iter = torch.utils.data.DataLoader(dataset, batch_size, shuffle=True)
    # è¿™é‡Œä½¿ç”¨äº†Adamä¼˜åŒ–ç®—æ³•
    optimizer = torch.optim.Adam(params=net.parameters(),
                                 lr=learning_rate,
                                 weight_decay=weight_decay)
    net = net.float()
    for epoch in range(num_epochs):
        for X, y in train_iter:
            l = loss(net(X.float()), y.float())
            optimizer.zero_grad()
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net, train_features, train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls


# ## 3.16.5 $K$æŠ˜äº¤å‰éªŒè¯

def get_k_fold_data(k, i, X, y):
    # è¿”å›ç¬¬iæŠ˜äº¤å‰éªŒè¯æ—¶æ‰€éœ€è¦çš„è®­ç»ƒå’ŒéªŒè¯æ•°æ®
    assert k > 1
    fold_size = X.shape[0] // k
    X_train, y_train = None, None
    for j in range(k):
        idx = slice(j * fold_size, (j + 1) * fold_size)
        X_part, y_part = X[idx, :], y[idx]
        if j == i:
            X_valid, y_valid = X_part, y_part
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            X_train = torch.cat((X_train, X_part), dim=0)
            y_train = torch.cat((y_train, y_part), dim=0)
    return X_train, y_train, X_valid, y_valid


def k_fold(k, X_train, y_train, num_epochs,
           learning_rate, weight_decay, batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)
        net = get_net(X_train.shape[1])
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
                                   weight_decay, batch_size)
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]
        if i == 0:
            d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse',
                         range(1, num_epochs + 1), valid_ls,
                         ['train', 'valid'])
        print('fold %d, train rmse %f, valid rmse %f' % (i, train_ls[-1], valid_ls[-1]))
    return train_l_sum / k, valid_l_sum / k


# ## 3.16.6 æ¨¡å‹é€‰æ‹©

k, num_epochs, lr, weight_decay, batch_size = 5, 100, 5, 0, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
                          weight_decay, batch_size)
print(('%d-fold validation: avg train rmse %f, avg valid rmse %f' %
       (k, train_l, valid_l)))


# ## 3.16.7 é¢„æµ‹å¹¶åœ¨Kaggleæäº¤ç»“æœ

def train_and_pred(train_features, test_features, train_labels, test_data,
                   num_epochs, lr, weight_decay, batch_size):
    net = get_net(train_features.shape[1])
    train_ls, _ = train(net, train_features, train_labels, None, None,
                        num_epochs, lr, weight_decay, batch_size)
    d2l.semilogy(range(1, num_epochs + 1), train_ls, 'epochs', 'rmse')
    print('train rmse %f' % train_ls[-1])
    preds = net(test_features).detach().numpy()
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    submission.to_csv('./submission.csv', index=False)


train_and_pred(train_features, test_features, train_labels, test_data, num_epochs, lr, weight_decay, batch_size)

