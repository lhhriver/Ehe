# 常用算法优缺点

## 朴素贝叶斯

> **适用数据类型**：标称型数据

主要用于自然语言处理

> **优点**

1. 朴素贝叶斯模型发源于古典数学理论，**有稳定的分类效率**。
2. 对**小规模的数据**表现很好，**能个处理多分类任务**，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去**增量训练**。
3. **对缺失数据不太敏感**，算法也比较**简单**，常用于**文本分类**。

> **缺点**

1. 理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,**假设属性之间相互独立**，这个假设在实际应用中往往是不成立的，**在属性个数比较多或者属性之间相关性较大时，分类效果不好**。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
2. 需要知道先验概率，且先验概率很多时候取决于假设，**假设的模型可以有很多种**，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。
3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。
4. 对输入数据的表达形式很敏感。



> 在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。其中:
>
> - GaussianNB就是先验为**高斯分布**的朴素贝叶斯。
> - MultinomialNB就是先验为**多项式分布**的朴素贝叶斯。
> - 而BernoulliNB就是先验为**伯努利分布**的朴素贝叶斯。

这三个类适用的分类场景各不相同，一般来说，如果样本特征的分布**大部分是连续值**，使用GaussianNB会比较好。如果如果样本特征的分大部分是**多元离散值**，使用MultinomialNB比较合适。而如果样本特征是**二元离散值或者很稀疏的多元离散值**，应该使用BernoulliNB。



## 感知机

感知机算法是一个简单易懂的算法，自己编程实现也不太难。前面提到它是很多算法的鼻祖，比如支持向量机算法，神经网络与深度学习。因此虽然它现在已经不是一个在实践中广泛运用的算法，还是值得好好的去研究一下。感知机算法对偶形式为什么在实际运用中比原始形式快，也值得好好去体会。

**使用感知机一个最大的前提，就是数据是线性可分的**。这严重限制了感知机的使用场景。它的分类竞争对手在面对不可分的情况时，比如**支持向量机可以通过核技巧来让数据在高维可分，神经网络可以通过激活函数和增加隐藏层来让数据可分**。



## 决策树

决策树算法作为一个大类别的分类回归算法的优缺点。

> 优点

1. 简单直观，生成的决策树很**直观**，对有**明确业务逻辑的数据分类效果好**，分类结果可解释性强，容易提取出规则。
2. 基本**不需要预处理**，不需要提前归一化，处理缺失值。
3. 使用决策树预测的代价是$O(log2m)$。 $m$为样本数。
4. 既可以处理**离散值**也可以处理**连续值**。很多算法只是专注于离散值或者连续值。
5. 可以处理**多维度**输出的分类问题。
6. 相比于神经网络之类的黑盒分类模型，决策树在逻辑上可以得到**很好的解释**。
7. 可以交叉验证的剪枝来选择模型，从而提高泛化能力。
8. 对于异常点的**容错能力**好，健壮性高。



> 缺点

1. 决策树算法非常**容易过拟合，导致泛化能力不强**。可以通过设置节点最少样本数量和限制决策树深度来改进。
2. 决策树会因为**样本发生一点点的改动，就会导致树结构的剧烈改变**。这个可以通过集成学习之类的方法解决。
3. 寻找最优的决策树是一个**NP难的问题**，我们一般是通过启发式方法，容易陷入局部最优。可以通过集成学习之类的方法来改善。
4. 有些比较**复杂的关系，决策树很难学习**，比如异或。这个就没有办法了，一般这种关系可以换神经网络分类方法来解决。
5. 如果某些**特征的样本比例过大，生成决策树容易偏向于这些特征**。这个可以通过调节样本权重来改善。



## 逻辑回归

逻辑回归尤其是二元逻辑回归是非常常见的模型，**训练速度很快**，虽然使用起来没有支持向量机（SVM）那么占主流，但是解决普通的分类问题是足够了，训练速度也比起SVM要快不少。如果你要理解机器学习分类算法，那么第一个应该学习的分类算法个人觉得应该是逻辑回归。理解了逻辑回归，其他的分类算法再学习起来应该没有那么难了。

> 优点：

（1）训练**速度较快**，分类的时候，计算量仅仅只和特征的数目相关，只需要代入样本的数据就可以拿到结果；**适合样本量大的情况**。

（2）简单易理解，模型的**可解释性非常好**，从特征的权重可以看到不同的特征对最后结果的影响；

（3）**适合二分类问题，不需要缩放输入特征**；

（4）内存资源占用小，因为只需要存储各个维度的特征值；



> 缺点：

（1）**不能用Logistic回归去解决非线性问题**，因为Logistic的决策面是线性的；

（2）对多重共线性数据较为敏感；

（3）**很难处理数据不平衡的问题**；

（4）**准确率并不是很高**，因为形式非常的简单(非常类似线性模型)，很难去拟合数据的真实分布；

（5）逻辑回归本身**无法筛选特征**，有时会用gbdt来筛选特征，然后再上逻辑回归。



## SVM

> 特点 :

1. 支持向量是SVM的训练结果，在SVM分类决策中**起决定作用的是支持向量**。
2. SVM的目标是对特征空间划分得到最优超平面，SVM方法**核心是最大化分类边界**。
3. SVM方法的理论基础是非线性映射，SVM利用**内积核函数代替向高维空间的非线性映射**。
4. SVM是一种有坚实理论基础的新颖的适用**小样本学习方法**。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。
5. SVM的最终决策函数只由少数的支持向量所确定，**计算的复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”**。
6. 少数支持向量决定了最终结果，这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且**具有较好的“鲁棒性”**。这种鲁棒性主要体现在： 
	- ① 增、删非支持向量样本对模型没有影响;
	- ② 支持向量样本集具有一定的鲁棒性;
	- ③ 有些成功的应用中，SVM方法对核的选取不敏感
7.  SVM学习问题可以表示为**凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值**。而其他分类方法（如基于规则的分类器和人工神经网络）都采用一种基于**贪心学习的策略**来搜索假设空间，这种方法一般只能获得局部最优解。
8. SVM通过最大化决策边界的边缘来控制模型的能力。尽管如此，用户必须提供其他参数，如使用核函数类型和引入松弛变量等。
9. SVM**在小样本训练集上能够得到比其它算法好很多的结果**。SVM优化目标是结构化风险最小，而不是经验风险最小，**避免了过拟合问题**，通过margin的概念，得到对数据分布的结构化描述，减低了对数据规模和数据分布的要求，**有优秀的泛化能力**。
10.  它是一个凸优化问题，因此局部最优解一定是全局最优解的优点。

	

> 优点: 

1. SVM是一种有坚实理论基础的新颖的**适用小样本学习方法**。它基本上不涉及概率测度及大数定律等，也简化了通常的分类和回归等问题。
2.  计算的**复杂性取决于支持向量的数目，而不是样本空间的维数，这在某种意义上避免了“维数灾难”**。
3. 少数支持向量决定了最终结果，**对异常值不敏感**, 这不但可以帮助我们抓住关键样本、“剔除”大量冗余样本,而且注定了该方法不但算法简单，而且具**有较好的`鲁棒性`**。
4. SVM学习问题可以表示为**凸优化问题，因此可以利用已知的有效算法发现目标函数的全局最小值, 有优秀的泛化能力**。



> 缺点:

1. **对大规模训练样本难以实施**

	> SVM的空间消耗主要是存储训练样本和核矩阵，由于SVM是借助二次规划来求解支持向量，而求解二次规划将涉及m阶矩阵的计算（m为样本的个数），当m数目很大时该矩阵的存储和计算将耗费大量的机器内存和运算时间。
	>  如果数据量很大，SVM的训练时间就会比较长，效率并不是很高，如垃圾邮件的分类检测，没有使用SVM分类器，而是使用简单的朴素贝叶斯分类器，或者是使用逻辑回归模型分类。

2. **解决多分类问题困难**

	> 经典的支持向量机算法只给出了二类分类的算法，而在实际应用中，一般要解决多类的分类问题。可以通过多个二类支持向量机的组合来解决。主要有一对多组合模式、一对一组合模式和SVM决策树；再就是通过构造多个分类器的组合来解决。主要原理是克服SVM固有的缺点，结合其他算法的优势，解决多类问题的分类精度。如：与粗糙集理论结合，形成一种优势互补的多类问题的组合分类器。

3. **对参数和核函数选择敏感** 

	> 支持向量机性能的优劣主要取决于核函数的选取，所以对于一个实际问题而言，如何根据实际的数据模型选择合适的核函数从而构造SVM算法。目前比较成熟的核函数及其参数的选择都是人为的，根据经验来选取的，带有一定的随意性。在不同的问题领域，核函数应当具有不同的形式和参数，所以在选取时候应该将领域知识引入进来，但是目前还没有好的方法来解决核函数的选取问题。


## 谱聚类算法总结

谱聚类算法是一个使用起来简单，但是讲清楚却不是那么容易的算法，它需要你有一定的数学基础。如果你掌握了谱聚类，相信你会对矩阵分析，图论有更深入的理解。同时对降维里的主成分分析也会加深理解。

> 优点

1. 谱聚类只需要数据之间的相似度矩阵，因此对于处理**稀疏数据**的聚类很有效。这点传统聚类算法比如K-Means很难做到 
2. 由于使用了降维，因此在处理高维数据聚类时的复杂度比传统聚类算法好。

> 缺点

1. 如果最终聚类的维度非常高，则由于降维的幅度不够，谱聚类的运行速度和最后的聚类效果均不好。 
2. 聚类效果依赖于相似矩阵，不同的相似矩阵得到的最终聚类效果可能很不同。



## 随机森林

RF作为一个可以高度并行化的算法，RF在大数据时候大有可为。 



> RF的主要优点有：

 1） 训练可以**高度并行化**，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。

 2） 由于可以随机选择决策树节点划分特征，这样**在样本特征维度很高的时候，仍然能高效的训练模型**。

 3） 在训练后，可以给出各个**特征对于输出的重要性**。

 4） 由于采用了随机采样，训练出的模型的**方差小，泛化能力强**。

 5） 相对于Boosting系列的Adaboost和GBDT， **RF实现比较简单**。

 6） 对部分特征**缺失不敏感**。



> RF的主要缺点有：

 1）在某些噪音比较大的样本集上，RF模型容易陷入过拟合。

 2)  **取值划分比较多的特征**容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。



## 线性回归

**LinearRegression**：一般来说，只要我们觉得数据有线性关系，LinearRegression类是我们的首先。如果发现拟合或者预测的不好，再考虑用其他的线性回归库。如果是学习线性回归，推荐先从这个类开始第一步的研究。

> 优点：实现简单、计算简单
>
> 缺点：和SVM相比容易欠拟合
>
> 适用场景：样本量大的回归

**Ridge**：一般来说，只要我们觉得数据有线性关系，用LinearRegression类拟合的不是特别好，需要正则化，可以考虑用Ridge类。但是这个类最大的缺点是每次我们要自己指定一个超参数$\alpha$，然后自己评估$\alpha$的好坏，比较麻烦，一般我都用下一节讲到的RidgeCV类来跑Ridge回归，不推荐直接用这个Ridge类，除非你只是为了学习Ridge回归。

**RidgeCV**：类对超参数$\alpha$使用了交叉验证，来帮忙我们选择一个合适的$\alpha$。在初始化RidgeCV类时候，我们可以传一组备选的$\alpha$值，10个，100个都可以。RidgeCV类会帮我们选择一个合适的$\alpha$。免去了我们自己去一轮轮筛选$\alpha$的苦恼。

如果输入特征的维度很高，而且是稀疏线性关系的话，RidgeCV类就不合适了。这时应该主要考虑下面几节要讲到的Lasso回归类家族。

**Lasso**：一般来说，对于高维的特征数据，尤其线性关系是稀疏的，我们会采用Lasso回归。或者是要在一堆特征里面找出主要的特征，那么Lasso回归更是首选了。但是Lasso类需要自己对$\alpha$调优，所以不是Lasso回归的首选，一般用到的是下一节要讲的LassoCV类。

**LassoCV**：是进行Lasso回归的首选。当我们面临在一堆高位特征中找出主要特征时，LassoCV类更是必选。当面对稀疏线性关系时，LassoCV也很好用。



## 最大熵模型

最大熵模型在分类方法里算是比较优的模型，但是由于它的约束函数的数目一般来说会随着样本量的增大而增大，导致样本量很大的时候，对偶函数优化求解的迭代过程非常慢，scikit-learn甚至都没有最大熵模型对应的类库。但是理解它仍然很有意义，尤其是它和很多分类方法都有千丝万缕的联系。　



> 最大熵模型的优点

1. 最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型,作为经典的分类模型时**准确率较高**。
2. 可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度。

> 最大熵模型的缺点

1. 由于约束函数数量和样本数目有关系，导致迭代过程计算量巨大，实际应用比较难。



## 最小二乘法

从上面可以看出，最小二乘法适用简洁高效，比梯度下降这样的迭代法似乎方便很多。但是这里我们就聊聊最小二乘法的局限性。

**首先**，最小二乘法需要计算$X^TX$的逆矩阵，有可能它的**逆矩阵不存在**，这样就没有办法直接用最小二乘法了，此时梯度下降法仍然可以使用。当然，我们可以通过对样本数据进行整理，去掉冗余特征。让$X^TX$的行列式不为0，然后继续使用最小二乘法。

**第二**，当样本**特征n非常的大**的时候，计算$X^TX$的逆矩阵是一个非常耗时的工作（n x n的矩阵求逆），甚至不可行。此时以梯度下降为代表的迭代法仍然可以使用。那这个n到底多大就不适合最小二乘法呢？如果你没有很多的分布式大数据计算资源，建议超过10000个特征就用迭代法吧。或者通过主成分分析降低特征的维度后再用最小二乘法。

**第三**，如果**拟合函数不是线性**的，这时无法使用最小二乘法，需要通过一些技巧转化为线性才能使用，此时梯度下降仍然可以用。

**第四**，讲一些特殊情况。当样本量m很少，小于特征数n的时候，这时拟合方程是欠定的，常用的优化方法都无法去拟合数据。当样本量m等于特征数n的时候，用方程组求解就可以了。**当m大于n时，拟合方程是超定的，也就是我们常用与最小二乘法的场景了**。



## Adaboost

到这里Adaboost就写完了，前面有一个没有提到，就是弱学习器的类型。理论上任何学习器都可以用于Adaboost。但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。


>Adaboost的主要优点有：

1. Adaboost作为分类器时，分类精度很高。
2. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
3. 作为简单的二元分类器时，构造简单，结果可理解。
4. 不容易发生过拟合。




>Adaboost的主要缺点有：

1. **对异常样本敏感**，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。



## DBSCAN

和传统的K-Means算法相比，DBSCAN最大的不同就是**不需要输入类别数k**，当然它最大的优势是**可以发现任意形状的聚类簇**，而不是像K-Means，一般仅仅使用于凸的样本集聚类。同时它在聚类的同时还可以找出异常点，这点和BIRCH算法类似。

那么我们什么时候需要用DBSCAN来聚类呢？一般来说，如果数据集是稠密的，并且数据集不是凸的，那么用DBSCAN会比K-Means聚类效果好很多。如果数据集不是稠密的，则不推荐用DBSCAN来聚类。

> 优点

DBSCAN的主要优点有：

1. 可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。
2. 可以在聚类的同时发现异常点，对数据集中的**异常点不敏感**。
3. 聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

> 缺点

DBSCAN的主要缺点有：

1. 如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。
2. 如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。
3. 调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对**距离阈值**ϵ，**邻域样本数阈值**MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。



## GBDT

GBDT终于讲完了，GDBT本身并不复杂，不过要吃透的话需要对集成学习的原理，决策树原理和各种损失函树有一定的了解。由于GBDT的卓越性能，只要是研究机器学习都应该掌握这个算法，包括背后的原理和应用调参方法。目前GBDT的算法比较好的库是xgboost。当然scikit-learn也可以。

> GBDT的优点

1. 可以灵活处理各种类型的数据，包括连续值和离散值。
2. 在相对少的调参时间情况下，预测的准确率也可以比较高。这个是相对SVM来说的。
3.  使用一些健壮的损失函数，对异常值的鲁棒性非常强。比如 Huber损失函数和Quantile损失函数。



> GBDT的缺点

1. 由于弱学习器之间存在依赖关系，难以并行训练数据。不过可以通过自采样的SGBT来达到部分并行。



## K-Means

初学者很容易把K-Means和KNN搞混，两者其实差别还是很大的。

K-Means是无监督学习的聚类算法，没有样本输出；而KNN是监督学习的分类算法，有对应的类别输出。KNN基本不需要训练，对测试集里面的点，只需要找到在训练集中最近的k个点，用这最近的k个点的类别来决定测试点的类别。而K-Means则有明显的训练过程，找到k个类别的最佳质心，从而决定样本的簇类别。

当然，两者也有一些相似点，两个算法都包含一个过程，即找出和某一个点最近的点。两者都利用了最近邻(nearest neighbors)的思想。



> 优点

K-Means的主要优点有：

1. 原理比较简单，实现也是很容易，收敛速度快。
2. 聚类效果较优。
3. 算法的可解释度比较强。
4. 主要需要调参的参数仅仅是簇数k。

> 缺点

K-Means的主要缺点有：

1. **K值**的选取不好把握。
2. 对于不是**凸数据集**比较难收敛。
3. 如果各隐含类别的**数据不平衡**，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳。
4. 采用迭代方法，得到的结果只是**局部最优**。
5. 对**噪音和异常点**比较的敏感。



## KNN

> 优点：

1. 理论成熟，思想简单，既可以用来做分类也可以用来做回归。
2. 可用于非线性分类。
3. 训练时间复杂度比支持向量机之类的算法低，仅为O(n)。
4. 和朴素贝叶斯之类的算法比，对数据没有假设，准确度高，对异常点不敏感。
5. 由于KNN方法主要靠周围有限的邻近的样本，而不是靠判别类域的方法来确定所属类别的，因此对于类域的交叉或重叠较多的待分样本集来说（**分类边界不明显**），KNN方法较其他方法更为适合。
6. 该算法比较适用于**样本容量比较大**的类域的自动分类，而那些样本容量较小的类域采用这种算法比较容易产生误分。



> 缺点：

1. 时间复杂度、空间复杂度高，计算量大，尤其是特征数非常多的时候。
2. **样本不平衡**的时候，对稀有类别的预测准确率低。
3. KD树，球树之类的模型建立需要**大量的内存**。
4. 使用懒散学习方法，基本上不学习，导致预测时速度比起逻辑回归之类的算法慢。
5. 相比决策树模型，KNN模型可解释性不强。
6. **容易过拟合**，预测结果对近邻点十分敏感，如果近邻点是噪声点的话，预测就会出错。因此，k值过小容易导致KNN算法的过拟合。



## PCA

这里对PCA算法做一个总结。作为一个非监督学习的降维方法，它只需要特征值分解，就可以对数据进行压缩，去噪。因此在实际场景应用很广泛。为了克服PCA的一些缺点，出现了很多PCA的变种，比如第六节的为解决**非线性降维**的KPCA，还有解决**内存限制**的增量PCA方法Incremental PCA，以及解决**稀疏数据**降维的PCA方法Sparse PCA等。

> PCA算法的主要优点有：

1. 仅仅需要以**方差衡量信息量**，不受数据集以外的因素影响。　
2. **各主成分之间正交**，可消除原始数据成分间的相互影响的因素。
3. 计算方法简单，主要运算是特征值分解，易于实现。



> PCA算法的主要缺点有：

1. 主成分各个特征维度的含义具有一定的模糊性，不如原始样本特征的解释性强。
2. 方差小的非主成分也可能含有对样本差异的重要信息，因降维丢弃可能对后续数据处理有影响。



# 算法选型

## 速度问题

- 数据量太大--抽样
- 样本特征太多--降为
- 样本值的量级太大--归一化
- 算法本身问题

## 分类



## 回归