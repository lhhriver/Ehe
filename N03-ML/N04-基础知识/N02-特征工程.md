# 数据类型

## 数据类型分类

如果从宏观角度分析，数据类型分为定性和定量两种。（表现形式：文字型/数值型）

- **定性**：变量是品质特征，如性别分男和女，是一种特质。
- **定量**：变量是数值，可以量化，如身高体重等。定量又可以分为**离散型**和**连续型**，离散型一般为计数结果，如贷款违约次数，连续型一般为测试结果，如身高体重的测量。



# 数据的分类

## 按数据的计量层次分类

一般可以将数据类型的度量分为四种：定类，定序，定距，和定比，**这四种类型是从低到高的递进关系，高级的类型可以用低级类型的分析方法来分析，而反过来却不行**，理解下面这些类型对于后面学习统计分析方法尤为重要。

### 定类

> 数据最低级，将数据按类别属性进行分类，类别间是平等并列的关系。如，某商场将顾客喜爱的顔色分为红色、白色、黄色等。
>
> 定类数据之间是平行并列关系，不能进行排序。

1. 根据定性的原则区分总体各个案类别的变量。定类变量的值只能把研究对象分类，也即只能决定研究对象是同类抑或不同类。
2. 例如性别区分为男性和女性两类；出生地区分为农村、城市、城镇三类；头发颜色分为黑，宗，白，黄；婚姻状况区分为未婚、已婚、分居、离婚、丧偶等类。
3. 这些变量的值，只能区别异同，属于定类层次。
4. 设计定类变量的各个类别时，要注意两个原则。一个是类与类之间要互相排斥，也即每个研究对象只能归入一类；另一个是所有研究对象均有归属，不可遗漏。例如性别分为男女两类，它既概括了人的性别的全部类别，同时类别之间又具有排斥性。

### 定序变量

> 定序数据。数据的中间级，不仅可将数据分成不同类别，且各类别间可通过排序来比较优势。如人的受教育程度可分为：小学、初中、高中、大学、硕士、博士。
>
> 定序数据可以比较顺序。

1. 区别同一类别个案中等级次序的变量。定序变量能决定次序，也即变量的值能把研究对象排列高低或大小，具有＞与＜的数学特质。它是比定类变量层次更高的变量，因此也具有定类变量的特质，即区分类别（＝，≠）。
2. 例如文化程度可以分为大学、高中、初中、小学、文盲；工厂规模可以分为大、中、小；年龄可以分为老、中、青。
3. 这些变量的值，既可以区分异同，也可以区别研究对象的高低或大小。但是，各个定序变量的值之间没有确切的间隔距离。比如大学究竟比高中高出多少，大学与高中之间的距离和初中与小学之间的距离是否相等，通常是没有确切的尺度来测量的。
4. 定序变量在各个案上所取的变量值只具有大于或小于的性质，只能排列出它们的顺序，而不能反映出大于或小于的数量或距离。

### 定距变量

> 定距数据。是具有一定单位的实际测量值。不仅可知两不同变量值之间存在差异，还可通过加、减运算准确计算出各变量值间的实际差距。如考试成绩、智商、摄食温度。
>
> 是对事物类别或次序之间距离的测度。通常使用自然或物理单位作为计量尺度。例：30°C和20℃之间相差10℃，-30°C和-20℃之间也是相差10℃。再比如，1等星比2等星亮10倍，0等星比1等星亮10倍，-1等星又比0等星亮10倍。定距数据可以进行加、减运算，不能进行乘、除运算。其原因为定距尺度中没有绝对零点（定距尺度中的“0”是作为比较的标准，不表示没有）。

1. 区别同一类别个案中等级次序及其距离的变量。它除了包括定序变量的特性外，还能确切测量同一类别各个案高低、大小次序之间的距离，因而具有加与减的数学特质。但是，定距变量没有一个真正的零点。
2. 例如，摄氏温度这一定距变量说明，摄氏40度比30度高10度，摄氏30度比20度又高10度，它们之间高出的距离相等，而摄氏零度并不是没有温度。又比如调查数个地区的工人占全部劳动人口的比率时，发现甲、乙，丙、丁、戊五个地区的比率分别是2％、10％、35％、20％、10％。甲区与丙区相差33％，丙区与丁区相差15％。这也是一个定距变量。
3. 定距变量各类别之间的距离，只能用加减而不能用乘除或倍数的形式来说明它们之间的关系。

### 定比变量

> 它与定距数据的唯一区别是：在定比数据中存在绝对零点，而定距数据中不存在绝对零点（零点是人为制定的）。如收入、产量均为定比数据。定比变量除了具有定距变量的特性外，还**具有一个真正的零点**，因而它具有乘与除（×、÷）的数学特质。要求达到定比变量这一测量层次。
>
>   定比变量是最高测量层次的变量。当前的社会学研究所应用的统计方法还很少。
>
>    一般来说，定比数据不可能取负值。一般也不会取零值，因为要么就是不存在了，要么就是极限情况。如，绝对零度只能无限接近，不可能完全达到。如果一个物体的体积为零，那么它要么不存在，要么是数学中的抽象概念，比如，几何中的点、线、面的体积都为零。而一个人的年龄为0时呢？作为社会学意义上的人，可以认为它是极限（开始），所以，年龄在社会意义上被认为是定比数据。
>
>  但从生物学意义上，人的年龄则是定距数据，因为年龄的绝对零点不便测算，考虑到婴儿在母体期间从何时起算年龄没有一定论。

1. 也是区别同一类别个案中等级次序及其距离的变量。定比变量除了具有定距变量的特性外，还具有一个真正的零点，因而它具有乘与除（×、÷）的数学特质。
2. 例如年龄和收入这两个变量，固然是定距变量，同时又是定比变量，因为其零点是绝对的，可以作乘除的运算。如A月收入是60元，而B是30元，我们可以算出前者是后者的两倍。
3. 智力商数这个变量是定距变量，但不是定比变量，因为其0分只具有相对的意义，不是绝对的或固定的，不能说某人的智商是0分就是没有智力；同时，由于其零点是不固定的，即使A是140分而B是70分，我们也不能说前者的智力是后者的两倍，只能说两者相差70分。
4. 因为0值是不固定的，如果将其向上移高20分，则A的智商变为120分而B变成50分，两者的相差仍是70分，但A却是B的2.4倍，而不是原先的2倍了。摄氏温度这一变量也如此。
5. 定比变量是最高测量层次的变量。



## 按数据来源分类

按数据的来源可将数据分为：第一手数据和第二手数据。

## 按时间状况分类

1. 时间序列数据。

指在不同的时间上搜集到的数据，反映现象随时间变化的发展情况。

2. 截面型数据。

指在相同或近似相同的时间点上搜集到的数据，描述现象在某一时刻的变化情况。



#  什么是特征工程

有这么一句话在业界广泛流传，**数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已**。那特征工程到底是什么呢？顾名思义，其本质是一项工程活动，目的是最大限度地从原始数据中提取特征以供算法和模型使用。

特征工程主要分为三部分：
1. **数据预处理** 对应的sklearn包：[sklearn-Processing data](http://scikit-learn.org/stable/modules/preprocessing.html#non-linear-transformation)
2. **特征选择** 对应的sklearn包： [sklearn-Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html)
3. **降维** 对应的sklearn包： [sklearn-Dimensionality reduction](http://scikit-learn.org/stable/modules/decomposition.html#decompositions)



本文中使用sklearn中的IRIS（鸢尾花）数据集来对特征处理功能进行说明，首先导入IRIS数据集的代码如下：


```python
import numpy as np
import pandas as pd
```


```python
from sklearn.datasets import load_iris

iris = load_iris()
```


```python
# 特征矩阵
iris.data
```


```python
# 目标向量
iris.target
```

# 特征选择

## 特征的来源

在做数据分析的时候，特征的来源一般有两块：  
- 一块是业务已经整理好各种特征数据，我们需要去找出适合我们问题需要的特征；
- 另一块是我们从业务特征中自己去寻找高级数据特征。

我们就针对这两部分来分别讨论。

## 选择合适的特征

第一步是找到该领域懂业务的专家，让他们给一些建议。比如我们需要解决一个药品疗效的分类问题，那么先找到领域专家，向他们咨询哪些因素（特征）会对该药品的疗效产生影响，较大影响的和较小影响的都要。这些特征就是我们的特征的第一候选集。

这个特征集合有时候也可能很大，在尝试降维之前，我们有必要用特征工程的方法去选择出较重要的特征结合，这些方法不会用到领域知识，而仅仅是统计学的方法。

最简单的方法就是**方差筛选**。方差越大的特征，那么我们可以认为它是比较有用的。如果方差较小，比如小于1，那么这个特征可能对我们的算法作用没有那么大。最极端的，如果某个特征方差为0，即所有的样本该特征的取值都是一样的，那么它对我们的模型训练没有任何作用，可以直接舍弃。在实际应用中，我们会指定一个方差的阈值，当方差小于这个阈值的特征会被我们筛掉。sklearn中的VarianceThreshold类可以很方便的完成这个工作。

特征选择方法有很多，一般分为三类：
1. **过滤法**：它按照特征的发散性或者相关性指标对各个特征进行评分，设定评分阈值或者待选择阈值的个数，选择合适特征。上面我们提到的方差筛选就是过滤法的一种。
2. **包装法**：根据目标函数，通常是预测效果评分，每次选择部分特征，或者排除部分特征。
3. **嵌入法**：它先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据权值系数从大到小来选择特征。类似于过滤法，但是它是通过机器学习训练来确定特征的优劣，而不是直接从特征的一些统计学指标来确定特征的优劣。

### 过滤法选择特征

#### 特征方差

第一个使用特征方差来过滤选择特征。


```python
from sklearn.feature_selection import VarianceThreshold

from sklearn.datasets import load_iris

iris = load_iris()
# 方差选择法，返回值为特征选择后的数据
# 参数threshold为方差的阈值
VarianceThreshold(threshold=0.2).fit_transform(iris.data)
```

#### 相关系数

第二个可以使用的是相关系数。这个主要用于输出连续值的监督学习算法中。我们分别计算所有训练集中各个特征与输出值之间的相关系数，设定一个阈值，选择相关系数较大的部分特征。


```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

# 选择K个最好的特征，返回选择特征后的数据
# 第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
# 参数k为选择的特征个数

SelectKBest(lambda X, Y: np.array(map(lambda x: pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```

#### 假设检验

第三个可以使用的是假设检验，比如卡方检验。卡方检验可以检验某个特征分布和输出值分布之间的相关性。个人觉得它比粗暴的方差法好用。

在sklearn中，可以使用chi2这个类来做卡方检验得到所有特征的卡方值与显著性水平P临界值，我们可以给定卡方值阈值， 选择卡方值较大的部分特征。


```python
from sklearn.feature_selection import SelectKBest
from sklearn.feature_selection import chi2

from sklearn.datasets import load_iris

# 导入IRIS数据集
iris = load_iris()

# 选择K个最好的特征，返回选择特征后的数据
SelectKBest(chi2, k=3).fit_transform(iris.data, iris.target)
```



除了卡方检验，我们还可以使用F检验和t检验，它们都是使用假设检验的方法，只是使用的统计分布不是卡方分布，而是F分布和t分布而已。在sklearn中，有F检验的函数f_classif和f_regression，分别在分类和回归特征选择时使用。



#### 信息熵

第四个是互信息，即从信息熵的角度分析各个特征和输出值之间的关系评分。 在决策树算法中我们讲到过互信息（信息增益）。互信息值越大，说明该特征和输出值之间的相关性越大，越需要保留。 

在sklearn中，可以使用mutual_info_classif(分类)和mutual_info_regression(回归)来计算各个输入特征和输出值之间的互信息。

以上就是过滤法的主要方法，个人经验是，在没有什么思路的时候，可以优先使用卡方检验和互信息来做特征选择。


```python
from sklearn.feature_selection import mutual_info_classif

# 互信息相关性：
mutual_info = mutual_info_classif(iris.data, iris.target, discrete_features= False)
mutual_info
```


```python
import matplotlib.pyplot as plt
import seaborn as sns
%matplotlib inline

X = iris.data
y = iris.target

new_y = [y[i:i+1] for i in range(0, len(y), 1)]
data = np.hstack((X, new_y))
data_df = pd.DataFrame(data)

#0到3表示特征，4表示目标变量,画图查看相关性，如下图所示
sns.heatmap(data_df.corr(), annot= True, fmt= '.2f')
```


```python

```

***
***

### 包装法选择特征

包装法的解决思路没有过滤法这么直接，它会选择一个目标函数来一步步的筛选特征。

最常用的包装法是**递归消除特征法**(recursive feature elimination,以下简称RFE)。递归消除特征法使用一个机器学习模型来进行多轮训练，每轮训练后，消除若干权值系数的对应的特征，再基于新的特征集进行下一轮训练。在sklearn中，可以使用RFE函数来选择特征。

我们下面以经典的SVM-RFE算法来讨论这个特征选择的思路。 这个算法以支持向量机来做RFE的机器学习模型选择特征。 它在第一轮训练的时候，会选择所有的特征来训练，得到了分类的超平面$w \dot x+b=0$后，如果有n个特征，那么RFE-SVM会选择出$w$中分量的平方值$w^2_i$最小的那个序号$i$对应的特征，将其排除，在第二类的时候，特征数就剩下$n-1$个了，我们继续用这$n-1$个特征和输出值来训练SVM，同样的，去掉$w^2_i$最小的那个序号$i$对应的特征。 以此类推，直到剩下的特征数满足我们的需求为止。


```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

# 递归特征消除法，返回特征选择后的数据
# 参数estimator为基模型
# 参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=3).fit_transform(iris.data, iris.target)
```


### 嵌入法选择特征

嵌入法也是用机器学习的方法来选择特征，但是它和RFE的区别是它不是通过不停的筛掉特征来进行训练，而是使用的都是特征全集。 
在sklearn中，使用SelectFromModel函数来选择特征。

#### 基于**惩罚项**的特征选择法

- 最常用的是使用L1正则化和L2正则化来选择特征。  
- 正则化惩罚项越大，那么模型的系数就会越小。  
- 当正则化惩罚项大到一定的程度的时候，部分特征系数会变成0，当正则化惩罚项继续增大到一定程度时，所有的特征系数都会趋于0。但是我们会发现一部分特征系数会更容易先变成0，这部分系数就是可以筛掉的。也就是说，我们选择特征系数较大的特征。  
- 常用的L1正则化和L2正则化来选择特征的基学习器是逻辑回归。

使用带惩罚项的基模型，除了筛选出特征外，同时也进行了降维。使用feature_selection库的SelectFromModel类结合带L1惩罚项的逻辑回归模型，来选择特征的代码如下：


```python
from sklearn.feature_selection import SelectFromModel
from sklearn.linear_model import LogisticRegression

# 带L1惩罚项的逻辑回归作为基模型的特征选择
SelectFromModel(LogisticRegression(penalty="l1", 
                                   C=0.1, 
                                   multi_class='auto')).fit_transform(iris.data, iris.target)
```

#### 基于**树模型**的特征选择法

是不是所有的机器学习方法都可以作为嵌入法的基学习器呢？也不是，一般来说，可以得到**特征系数**coef或者可以得到**特征重要度**(feature importances)的算法才可以做为嵌入法的基学习器。


树模型中GBDT可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：


```python
from sklearn.feature_selection import SelectFromModel
from sklearn.ensemble import GradientBoostingClassifier

# GBDT作为基模型的特征选择
SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
```



**总结**

类 | 所属方式 | 说明
:- | :-: | -: |
VarianceThreshold|Filter|方差选择法
SelectKBest|Filter|可选关联系数、卡方校验、最大信息系数作为得分计算的方法
RFE|Wrapper|递归地训练基模型，将权值系数较小的特征从特征集合中消除
SelectFromModel	|Embedded|训练基模型，选择权值系数较高的特征

## 寻找高级特征

在我们拿到已有的特征后，我们还可以根据需要寻找到更多的高级特征。比如有车的路程特征和时间间隔特征，我们就可以得到车的平均速度这个二级特征。根据车的速度特征，我们就可以得到车的加速度这个三级特征，根据车的加速度特征，我们就可以得到车的加加速度这个四级特征。也就是说，高级特征可以一直寻找下去。

在Kaggle之类的算法竞赛中，高分团队主要使用的方法除了集成学习算法，剩下的主要就是在高级特征上面做文章。所以寻找高级特征是模型优化的必要步骤之一。当然，在第一次建立模型的时候，我们可以先不寻找高级特征，得到以后基准模型后，再寻找高级特征进行优化。

寻找高级特征最常用的方法有：
- 若干项特征加和： 
   - 我们假设你希望根据每日销售额得到一周销售额的特征。你可以将最近的7天的销售额相加得到。
- 若干项特征之差： 
   - 假设你已经拥有每周销售额以及每月销售额两项特征，可以求一周前一月内的销售额。
- 若干项特征乘积： 
   - 假设你有商品价格和商品销量的特征，那么就可以得到销售额的特征。
- 若干项特征除商： 
   - 假设你有每个用户的销售额和购买的商品件数，那么就是得到该用户平均每件商品的销售额。

当然，寻找高级特征的方法远不止于此，它需要你根据你的业务和模型需要而得，而不是随便的两两组合形成高级特征，这样容易导致特征爆炸，反而没有办法得到较好的模型。个人经验是，聚类的时候高级特征尽量少一点，分类回归的时候高级特征适度的多一点。



# 特征表达

特征表达部分，即如果对某一个特征的具体表现形式做处理。主要包括缺失值处理，特殊的特征处理比如时间和地理位置处理，离散特征的连续化和离散化处理，连续特征的离散化处理几个方面。

## 缺失值处理

特征有缺失值是非常常见的，大部分机器学习模型在拟合前需要所有的特征都有值，不能是空或者NULL。那么如果有缺失值我们需要怎么处理呢？

首先我们会看是该特征是连续值还是离散值。  
- 如果是连续值，那么一般有两种选择
   - 一是选择所有有该特征值的样本，然后取平均值，来填充缺失值。
   - 另一种是取中位数来填充缺失值。
- 如果是离散值，则一般会选择所有有该特征值的样本中最频繁出现的类别值，来填充缺失值

在sklearn中，可以使用preprocessing.Imputer来选择这三种不同的处理逻辑做预处理。


```python
import numpy as np
from sklearn.preprocessing import Imputer

# 缺失值计算，返回值为计算缺失值后的数据
# 参数missing_value为缺失值的表示形式，默认为NaN
# 参数strategy为缺失值填充方式，默认为mean（均值）
imp = Imputer(missing_values='NaN',strategy='mean')

imp.fit_transform(np.vstack((np.array([np.nan, np.nan, np.nan, np.nan]), iris.data[0:5,:])))
```

## 数据变换

### 多项式变换（对行向量处理）
常见的数据变换有基于多项式的、基于指数函数的、基于对数函数的。4个特征，度为2的多项式转换公式如下：
$$
\begin{aligned}&\left(x_{1}^{\prime}, x_{2}^{\prime}, x_{3}^{\prime}, x_{4}^{\prime}, x_{5}^{\prime}, x_{6}^{\prime}, x_{7}^{\prime}, x_{7}^{\prime}, x_{3}^{\prime}, x_{10}^{\prime}, x_{11}^{\prime}, x_{12}^{\prime}, x_{13}^{\prime}, x_{14}^{\prime} x_{15}^{\prime}\right) & \\
=&\left(1, x_{1}, x_{2}, x_{3}, x_{4}, x_{1}^{2}, x_{1} * x_{2}, x_{1} * x_{4}, x_{2}^{2}, x_{2} * x_{3}, x_{2} * x_{4}, x_{2}^{2}, x_{3} * x_{4}, x_{4}^{2}\right) \end{aligned}
$$
使用preproccessing库的PolynomialFeatures类对数据进行多项式转换的代码如下：


```python
from sklearn.preprocessing import PolynomialFeatures

# 多项式转换
# 参数degree为度，默认值为2
PolynomialFeatures().fit_transform(iris.data)
```

### 自定义变换
基于单变元函数的数据变换可以使用一个统一的方式完成，使用preproccessing库的FunctionTransformer对数据进行对数函数转换的代码如下：


```python
from numpy import log1p
from sklearn.preprocessing import FunctionTransformer

# 自定义转换函数为对数函数的数据变换
# 第一个参数是单变元函数
FunctionTransformer(log1p).fit_transform(iris.data)
```

### 总结
类 | 功能 | 说明
:-: | :-: | :- 
StandardScaler | 无量纲化 | 标准化，基于特征矩阵的列，将特征值转换至服从标准正态分布
MinMaxScaler | 无量纲化 | 区间缩放，基于最大最小值，将特征值转换到[0, 1]区间上
Normalizer | 归一化 | 基于特征矩阵的行，将样本向量转换为“单位向量”
Binarizer | 二值化 | 基于给定阈值，将定量特征按阈值划分
OneHotEncoder | 哑编码 | 将定性数据编码为定量数据
Imputer | 缺失值计算 | 计算缺失值，缺失值可填充为均值等
PolynomialFeatures | 多项式数据转换 | 多项式数据转换
FunctionTransformer | 自定义单元数据转换 | 使用单变元的函数来转换数据



## 特殊的特征处理

有些特征的默认取值比较特殊，一般需要做了处理后才能用于算法。比如日期时间，比如显示20180519，这样的值一般没办法直接使用。那么一般需要如何变换呢？

### 时间特征

对于时间原始特征，处理方法有很多，这里只举例几种有代表性的方法。　  


- 第一种是使用连续的时间差值法，即计算出所有样本的时间到某一个未来时间之间的数值差距，这样这个差距是UTC的时间差，从而将时间特征转化为连续值。 
- 第二种方法是根据时间所在的年，月，日，星期几，小时数，将一个时间特征转化为若干个离散特征，这种方法在分析具有明显时间趋势的问题比较好用。 
- 第三种是权重法，即根据时间的新旧得到一个权重值。比如对于商品，三个月前购买的设置一个较低的权重，最近三天购买的设置一个中等的权重，在三个月内但是三天前的设置一个较大的权重。当然，还有其他的设置权重的方法，这个要根据要解决的问题来灵活确定。

### 地理特征

对地理特征，比如“广州市天河区XX街道XX号”，这样的特征我们应该如何使用呢？处理成离散值和连续值都是可以的。 

如果是处理成离散值，则需要转化为多个离散特征，比如城市名特征，区县特征，街道特征等。  但是如果我们需要判断用户分布区域，则一般处理成连续值会比较好，这时可以将地址处理成经度和纬度的连续特征。

## 离散特征的连续化处理

有很多机器学习算法只能处理连续值特征，不能处理离散值特征，比如线性回归，逻辑回归等。那么想使用逻辑回归，线性回归时这些值只能丢弃吗？当然不是。我们可以将离散特征连续化处理。

### 独热编码

最常见的离散特征连续化的处理方法是独热编码one-hot encoding。 

处理方法其实比较简单，比如某特征的取值是高，中和低，那么我们就可以创建三个取值为0或者1的特征，将高编码为1,0,0这样三个特征，中编码为0,1,0这样三个特征，低编码为0,0,1这样三个特征。也就是说，之前的一个特征被我们转化为了三个特征。 

sklearn的OneHotEncoder可以帮我们做这个处理。


```python
from sklearn.preprocessing import OneHotEncoder

# 哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
enc = OneHotEncoder(categories='auto')

enc.fit_transform(iris.target.reshape((-1, 1))).toarray()
```

### 特征嵌入

第二个方法是特征嵌入embedding。 


这个一般用于深度学习中。比如对于用户的ID这个特征，如果要使用独热编码，则维度会爆炸，如果使用特征嵌入就维度低很多了。  


对于每个要嵌入的特征，我们会有一个特征嵌入矩阵，这个矩阵的行很大，对应我们该特征的数目。比如用户ID，如果有100万个，那么嵌入的特征矩阵的行就是100万。但是列一般比较小，比如可以取20。这样每个用户ID就转化为了一个20维的特征向量。进而参与深度学习模型。  


在tensorflow中，我们可以先随机初始化一个特征嵌入矩阵，对于每个用户，可以用tf.nn.embedding_lookup找到该用户的特征嵌入向量。特征嵌入矩阵会在反向传播的迭代中优化。


此外，在自然语言处理中，我们也可以用word2vec将词转化为词向量，进而可以进行一些连续值的后继处理。

## 离散特征的离散化处理

 离散特征有时间也不能直接使用，需要先进行转化。比如最常见的，如果特征的取值是高，中和低，那么就算你需要的是离散值，也是没法直接使用的。

 对于原始的离散值特征，最常用的方法也是独热编码，方法在第三节已经讲到。

 第二种方法是虚拟编码dummy coding，它和独热编码类似，但是它的特点是，如果我们的特征有N个取值，它只需要N-1个新的0,1特征来代替，而独热编码会用N个新特征代替。比如一个特征的取值是高，中和低，那么我们只需要两位编码，比如只编码中和低，如果是1，0则是中，0,1则是低。0,0则是高了。目前虚拟编码使用的没有独热编码广，因此一般有需要的话还是使用独热编码比较好。

 此外，有时候我们可以对特征进行研究后做一个更好的处理。比如，我们研究商品的销量对应的特征。里面有一个原始特征是季节春夏秋冬。我们可以将其转化为淡季和旺季这样的二值特征，方便建模。当然有时候转化为三值特征或者四值特征也是可以的。

对于分类问题的特征输出，我们一般需要用sklearn的LabelEncoder将其转化为0,1,2，...这样的类别标签值。

## 连续特征的离散化处理

 对于连续特征，有时候我们也可以将其做离散化处理。这样特征变得高维稀疏，方便一些算法的处理。

 对常用的方法是根据阈值进行分组，比如我们根据连续值特征的分位数，将该特征分为高，中和低三个特征。将分位数从0-0.3的设置为高，0.3-0.7的设置为中，0.7-1的设置为高。

 当然还有高级一些的方法。比如使用GBDT。在LR+GBDT的经典模型中，就是使用GDBT来先将连续值转化为离散值。那么如何转化呢？ 

 比如我们用训练集的所有连续值和标签输出来训练GBDT，最后得到的GBDT模型有两颗决策树，第一颗决策树有三个叶子节点，第二颗决策树有4个叶子节点。如果某一个样本在第一颗决策树会落在第二个叶子节点，在第二颗决策树落在第4颗叶子节点，那么它的编码就是0,1,0,0,0,0,1，一共七个离散特征，其中会有两个取值为1的位置，分别对应每颗决策树中样本落点的位置。

 在sklearn中，我们可以用GradientBoostingClassifier的apply方法很方便的得到样本离散化后的特征，然后使用独热编码即可。 

### 示例


```python
from sklearn.datasets import make_classification
from sklearn.model_selection import train_test_split
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.preprocessing import OneHotEncoder

X, y = make_classification(n_samples=10)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5)

gbc = GradientBoostingClassifier(n_estimators=2)
one_hot = OneHotEncoder(categories='auto')

gbc.fit(X_train, y_train)
X_train_new = one_hot.fit_transform(gbc.apply(X_train)[:, :, 0])
print(X_train_new.todense())
```

***
***
***

# 特征预处理

本文我们来讨论特征预处理的相关问题。主要包括特征的归一化和标准化，异常特征样本清洗与样本数据不平衡问题的处理。

## 特征的标准化和归一化

由于标准化和归一化这两个词经常混用，所以本文不再区别标准化和归一化，而通过具体的标准化和归一化方法来区别具体的预处理操作。

### z-score标准化

z-score标准化：这是最常见的特征预处理方式，基本所有的线性模型在拟合的时候都会做 z-score标准化。  

将服从正态分布的特征值转换成标准正态分布，标准化需要计算特征的均值和标准差，公式表达为：
$$
x^{\prime}=\frac{x-\overline{X}}{S}
$$


在sklearn中，我们可以用StandardScaler来做z-score标准化。  


```python
from sklearn.preprocessing import StandardScaler

# 标准化，返回值为标准化后的数据
StandardScaler().fit_transform(iris.data)
```

### max-min标准化

max-min标准化：也称为离差标准化，预处理后使特征值映射到[0,1]之间。  

具体的方法是求出样本特征x的最大值max和最小值min，然后用(x-min)/(max-min)来代替原特征。  
$$
x^{\prime}=\frac{x-Min}{Max-Min}
$$


如果我们希望将数据映射到任意一个区间[a,b]，而不是[0,1]，那么也很简单 
$$
x^{\prime}=\frac{(x-Min)(b-a)}{(Max-Min)+a}
$$


在sklearn中，我们可以用MinMaxScaler来做max-min标准化。这种方法的问题就是如果测试集或者预测数据里的特征有小于min，或者大于max的数据，会导致max和min发生变化，需要重新计算。    


所以实际算法中， 除非你对特征的取值区间有需求，否则max-min标准化没有 z-score标准化好用。


```python
from sklearn.preprocessing import MinMaxScaler

# 区间缩放，返回值为缩放到[0, 1]区间的数据
MinMaxScaler().fit_transform(iris.data)
```

**在什么时候使用标准化比较好，什么时候区间缩放比较好呢？**
- 在后续的分类、聚类算法中，需要使用距离来度量相似性的时候、或者使用PCA、LDA这些需要用到协方差分析进行降维的时候，同时数据分布可以近似为正太分布，标准化方法(Z-score standardization)表现更好。  
- 在不涉及距离度量、协方差计算、数据不符合正太分布的时候，可以使用区间缩放法或其他归一化方法。比如图像处理中，将RGB图像转换为灰度图像后将其值限定在[0 255]的范围。

### L1/L2范数标准化

**L1/L2范数标准化**：如果我们只是为了统一量纲，样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”,那么通过L2范数整体标准化也是可以的，具体方法是求出每个样本特征向量$\vec{x}$的L2范数$||\vec{x}||_2$,然后用$\vec{x}/||\vec{x}||_2$代替原样本特征即可。当然L1范数标准化也是可以的，即用$\vec{x}/||\vec{x}||_1$代替原样本特征。  

规则为l2的归一化公式如下：
$$
x^{\prime}=\frac{x}{\sqrt{\sum_{j}^{m} x[j]^{2}}}
$$


通常情况下，范数标准化首选L2范数标准化。在sklearn中，我们可以用Normalizer来做L1/L2范数标准化。


```python
from sklearn.preprocessing import Normalizer

# 归一化，返回值为归一化后的数据
Normalizer().fit_transform(iris.data)
```



### 二值化



### 对定量特征二值化（对列向量处理）

**定性与定量区别**

**定性**：博主很胖，博主很瘦

**定量**：博主有80kg，博主有60kg

一般定性都会有相关的描述词，定量的描述都是可以用数字来量化处理
定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：
$$
x^{\prime}=\left\{\begin{array}{l}{1, x>\text { threshold }} \\ {0, x \leq \text { threshold }}\end{array}\right.
$$
使用preproccessing库的Binarizer类对数据进行二值化的代码如下：


```python
from sklearn.preprocessing import Binarizer

# 二值化，阈值设置为3，返回值为二值化后的数据
Binarizer(threshold=3).fit_transform(iris.data)
```

### 中心化

此外，经常我们还会用到中心化，主要是在PCA降维的时候，此时我们求出特征x的平均值mean后，用x-mean代替原特征，也就是特征的均值变成了0, 但是方差并不改变。这个很好理解，因为PCA就是依赖方差来降维的。

虽然大部分机器学习模型都需要做标准化和归一化，也有不少模型可以不做做标准化和归一化，主要是基于概率分布的模型，比如决策树大家族的CART，随机森林等。当然此时使用标准化也是可以的，大多数情况下对模型的泛化能力也有改进。

## 降维
- 当特征选择完成后，可以直接训练模型了，但是可能由于特征矩阵过大，导致计算量大，训练时间长的问题，因此降低特征矩阵维度也是必不可少的。常见的降维方法除了以上提到的基于L1惩罚项的模型以外，另外还有主成分分析法（PCA）和线性判别分析（LDA），线性判别分析本身也是一个分类模型。  
- PCA和LDA有很多的相似点，其本质是要将原始的样本映射到维度更低的样本空间中，但是PCA和LDA的映射目标不一样：**PCA是为了让映射后的样本具有最大的发散性；而LDA是为了让映射后的样本有最好的分类性能**。  
- 所以说PCA是一种无监督的降维方法，而LDA是一种有监督的降维方法。

### 主成分分析法（PCA）
使用decomposition库的PCA类选择特征的代码如下：


```python
from sklearn.decomposition import PCA

# 主成分分析法，返回降维后的数据
# 参数n_components为主成分数目
PCA(n_components=2).fit_transform(iris.data)
```

### 线性判别分析法（LDA）
使用LDA进行降维的代码如下：


```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA

# 线性判别分析法，返回降维后的数据
# 参数n_components为降维后的维数
LDA(n_components=2).fit_transform(iris.data, iris.target)
```



## 异常特征样本清洗

我们在实际项目中拿到的数据往往有不少异常数据，有时候不筛选出这些异常数据很可能让我们后面的数据分析模型有很大的偏差。那么如果我们没有专业知识，如何筛选出这些异常特征样本呢？常用的方法有两种。

### 聚类

第一种是聚类，比如我们可以用KMeans聚类将训练样本分成若干个簇，如果某一个簇里的样本数很少，而且簇质心和其他所有的簇都很远，那么这个簇里面的样本极有可能是异常特征样本了。我们可以将其从训练集过滤掉。

### 异常点检测

第二种是异常点检测方法，主要是使用iForest或者one class SVM，使用异常点检测的机器学习算法来过滤所有的异常点。

当然，某些筛选出来的异常样本是否真的是不需要的异常特征样本，最好找懂业务的再确认一下，防止我们将正常的样本过滤掉了。

## 处理不平衡数据

### 权重法

- 权重法是比较简单的方法，我们可以对训练集里的每个类别加一个权重class weight。如果该类别的样本数多，那么它的权重就低，反之则权重就高。如果更细致点，我们还可以对每个样本加权重sample weight，思路和类别权重也是一样，即样本数多的类别样本权重低，反之样本权重高。
- sklearn中，绝大多数分类算法都有class weight和 sample weight可以使用。
- 如果权重法做了以后发现预测效果还不好，可以考虑采样法。



### 采样法

采样法常用的也有两种思路:  

1. 一种是对类别样本数多的样本做子采样, 比如训练集里A类别样本占90%，B类别样本占10%。那么我们可以对A类的样本子采样，直到子采样得到的A类样本数和B类别现有样本一致为止，这样我们就只用子采样得到的A类样本数和B类现有样本一起做训练集拟合模型。  
2. 第二种思路是对类别样本数少的样本做过采样, 还是上面的例子，我们对B类别的样本做过采样，直到过采样得到的B类别样本数加上B类别原来样本一起和A类样本数一致，最后再去拟合模型。

[参考博客](https://blog.csdn.net/qq_27802435/article/details/81201357)


```python
from sklearn.datasets import make_classification
from collections import Counter
X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,
                           n_redundant=0, n_repeated=0, n_classes=3,
                           n_clusters_per_class=1,
                           weights=[0.01, 0.05, 0.94],
                           class_sep=0.8, random_state=0)
Counter(y)
```

#### 下采样

原型生成方法将减少数据集的样本数量, 剩下的样本是由原始数据集生成的, 而不是直接来源于原始数据集。
ClusterCentroids函数实现了上述功能: 每一个类别的样本都会用K-Means算法的中心点来进行合成, 而不是随机从原始样本进行抽取。


```python
from imblearn.under_sampling import ClusterCentroids

cc = ClusterCentroids(random_state=0)
X_resampled, y_resampled = cc.fit_sample(X, y)

sorted(Counter(y_resampled).items())
```



RandomUnderSampler函数是一种快速并十分简单的方式来平衡各个类别的数据: 随机选取数据的子集.


```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0)
X_resampled, y_resampled = rus.fit_sample(X, y)

sorted(Counter(y_resampled).items())
```



#### 过采样


```python
from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_sample(X, y)

sorted(Counter(y_resampled).items())
```



### SMOTE算法

上述两种常用的采样法很简单，但是都有个问题，就是采样后改变了训练集的分布，可能导致泛化能力差。   


所以有的算法就通过其他方法来避免这个问题，比如SMOTE算法通过人工合成的方法来生成少类别的样本。方法也很简单，对于某一个缺少样本的类别，它会随机找出几个该类别的样本，再找出最靠近这些样本的若干个该类别样本，组成一个候选合成集合，然后在这个集合中不停的选择距离较近的两个样本，在这两个样本之间，比如中点，构造一个新的该类别样本。  


举个例子，比如该类别的候选合成集合有两个样本$(x_1,y),(x_2,y)$,那么SMOTE采样后，可以得到一个新的训练样本$(\frac{x_1+x_2}{2},y)$,通过这种方法，我们可以得到不改变训练集分布的新样本，让训练集中各个类别的样本数趋于平衡。   


我们可以用imbalance-learn这个Python库中的SMOTEENN类来做SMOTE采样。

- SMOTE: 对于少数类样本a, 随机选择一个最近邻的样本b, 然后从a与b的连线上随机选取一个点c作为新的少数类样本;
- ADASYN: 关注的是在那些基于K最近邻分类器被错误分类的原始样本附近生成新的少数类样本


```python
from imblearn.over_sampling import SMOTE, ADASYN
 
X_resampled_smote, y_resampled_smote = SMOTE().fit_sample(X, y)
 
sorted(Counter(y_resampled_smote).items())
```


```python
X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_sample(X, y)
 
sorted(Counter(y_resampled_adasyn).items())
```



# 解答

## 简单说说特征工程



## 为什么一些机器学习模型需要对数据进行归一化？



## 谈谈深度学习中的归一化问题



## 标准化与归一化的区别?

简单来说，标准化是依照特征矩阵的列处理数据，其通过求z-score的方法，将样本的特征值转换到同一量纲下。归一化是依照特征矩阵的行处理数据，其目的在于样本向量在点乘运算或其他核函数计算相似性时，拥有统一的标准，也就是说都转化为“单位向量”。规则为L2的归一化公式如下：

特征向量的缺失值处理：

1. 缺失值较多，直接将该特征舍弃掉，否则可能反倒会带入较大的noise，对结果造成不良影响。
2. 缺失值较少，其余的特征缺失值都在10%以内，我们可以采取很多的方式来处理:
    - 把NaN直接作为一个特征，假设用0表示；
    - 用均值填充；
    - 用随机森林等算法预测填充



## 如何进行特征选择？

特征选择是一个重要的数据预处理过程，主要有两个原因：一是减少特征数量、降维，使模型泛化能力更强，减少过拟合;二是增强对特征和特征值之间的理解。

常见的特征选择方式：

1. 去除方差较小的特征。
2. 正则化。1正则化能够生成稀疏的模型。L2正则化的表现更加稳定，由于有用的特征往往对应系数非零。
3. 随机森林，对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。一般不需要feature engineering、调参等繁琐的步骤。它的两个主要问题，1是重要的特征有可能得分很低（关联特征问题），2是这种方法对特征变量类别多的特征越有利（偏向问题）。
4. 稳定性选择。是一种基于二次抽样和选择算法相结合较新的方法，选择算法可以是回归、SVM或其他类似的方法。它的主要思想是在不同的数据子集和特征子集上运行特征选择算法，不断的重复，最终汇总特征选择结果，比如可以统计某个特征被认为是重要特征的频率（被选为重要特征的次数除以它所在的子集被测试的次数）。理想情况下，重要特征的得分会接近100%。稍微弱一点的特征得分会是非0的数，而最无用的特征得分将会接近于0。



## 你知道有哪些数据处理和特征工程的处理？

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/特征工程-20201215-223657-786625.png)



## 数据不平衡问题。

这主要是由于数据分布不平衡造成的。解决方法如下：

1. 采样，对小样本加噪声采样，对大样本进行下采样
2. 进行特殊的加权，如在Adaboost中或者SVM中
3. 采用对不平衡数据集不敏感的算法
4. 改变评价标准：用AUC/ROC来进行评价
5. 采用Bagging/Boosting/Ensemble等方法
6. 考虑数据的先验分布



## 机器学习中，有哪些特征选择的工程方法？

数据和特征决定了机器学习的上限，而模型和算法只是逼近这个上限而已。

1. 计算每一个特征与响应变量的相关性：工程上常用的手段有计算皮尔逊系数和互信息系数，皮尔逊系数只能衡量线性相关性而互信息系数能够很好地度量各种相关性，但是计算相对复杂一些，好在很多toolkit里边都包含了这个工具（如sklearn的MINE），得到相关性之后就可以排序选择特征了；
2. 构建单个特征的模型，通过模型的准确性为特征排序，借此来选择特征；
3. 通过L1正则项来选择特征：L1正则方法具有稀疏解的特性，因此天然具备特征选择的特性，但是要注意，L1没有选到的特征不代表不重要，原因是两个具有高相关性的特征可能只保留了一个，如果要确定哪个特征重要应再通过L2正则方法交叉检验*；
4. 训练能够对特征打分的预选模型：RandomForest和Logistic Regression等都能对模型的特征打分，通过打分获得相关性后再训练最终模型；
5. 通过特征组合后再来选择特征：如对用户id和用户特征最组合来获得较大的特征集再来选择特征，这种做法在推荐系统和广告系统中比较常见，这也是所谓亿级甚至十亿级特征的主要来源，原因是用户数据比较稀疏，组合特征能够同时兼顾全局模型和个性化模型，这个问题有机会可以展开讲。
6. 通过深度学习来进行特征选择：目前这种手段正在随着深度学习的流行而成为一种手段，尤其是在计算机视觉领域，原因是深度学习具有自动学习特征的能力，这也是深度学习又叫unsupervised feature learning的原因。从深度学习模型中选择某一神经层的特征后就可以用来进行最终目标模型的训练了。



## 对于维度极低的特征，选择线性还是非线性分类器？

答案：非线性分类器，低维空间可能很多特征都跑到一起了，导致线性不可分。

1. 如果特征的数量很大，跟样本数量差不多，这时候选用LR或者是Linear Kernel的SVM。
2. 如果特征的数量比较小，样本数量一般，不算大也不算小，选用SVM+Gaussian Kernel。
3. 如果特征的数量比较小，而样本数量很多，需要手工添加一些特征变成第一种情况。