最大熵模型(maximum entropy model， MaxEnt)也是很典型的分类算法了，它和逻辑回归类似，都是属于**对数线性分类模型**。在损失函数优化的过程中，使用了和支持向量机类似的**凸优化技术**。而对**熵**的使用，让我们想起了决策树算法中的ID3和C4.5算法。理解了最大熵模型，对逻辑回归，支持向量机以及决策树算法都会加深理解。本文就对最大熵模型的原理做一个小结。

最大熵模型通俗的解释就是**按照模型熵最大的原则来选择模型**。这种选择模型的方法和我们在日常生活中做的策略是一致的,很多时候我们做的决策和选择其实都依照了最大熵的原则。比如有一个六个面的骰子,如果要我们猜测每个面的概率是多少？一般在没有任何提示信息的情况下都会选1/6。而$P\left(X=x_{i}\right)=\frac{1}{6}, x_{i} \subseteq\left\{x_{1}, x_{2} \ldots x_{6}\right\}$就是熵最大的模型，没有做任何无关的猜测，保持的最大不确定性。

# 熵和条件熵的回顾

`熵`度量了事物的不确定性，越不确定的事物，它的熵就越大。具体的，随机变量X的熵的表达式如下：
$$
H(X) = -\sum\limits_xp(x) logp(x)
$$

其中$n$代表$X$的$n$种不同的离散取值。而$p(x)$代表了$X$取值为$x$的概率，$log$为以2或者$e$为底的对数。

熟悉了一个变量$X$的熵，很容易推广到多个个变量的`联合熵`，这里给出两个变量X和Y的联合熵表达式：
$$
H(X,Y) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(x_i,y_i)
$$


有了联合熵，又可以得到`条件熵`的表达式$H(Y|X)$，条件熵类似于条件概率,它度量了我们的$Y$在知道$X$以后剩下的不确定性。表达式如下：
$$
H(Y|X) = -\sum\limits_{i=1}^{n}p(x_i,y_i)logp(y_i|x_i) = \sum\limits_{j=1}^{n}p(x_j)H(Y|x_j)
$$

用下面这个图很容易明白他们的关系。左边的椭圆代表$H(X)$, 右边的椭圆代表$H(Y)$, 中间重合的部分就是我们的`互信息`或者`信息增益`$I(X,Y)$, 左边的椭圆去掉重合部分就是$H(X|Y)$, 右边的椭圆去掉重合部分就是$H(Y|X)$。两个椭圆的并就是$H(X,Y)$。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/最大熵-20201215-223659-640760.png)

# 最大熵模型的定义

最大熵模型假设分类模型是一个条件概率分布$P(Y|X)$,  $X$为特征，$Y$为输出。

给定一个训练集${(x^{(1)},y^{(1)}), (x^{(2)},y^{(2)}), ... ，(x^{(m)},y^{(m)})}$, 其中$x$为$n$维特征向量，$y$为类别输出。我们的目标就是用最大熵模型选择一个最好的分类类型。

在给定训练集的情况下，我们可以得到`总体联合分布`$P(X,Y)$的`经验分布`$\tilde{P}(X,Y)$ 和`边缘分布`$P(X)$的`经验分布`$\tilde{P}(X)$。$\tilde{P}(X,Y)$即为训练集中$X$,$Y$同时出现的次数除以样本总数$m$，$\tilde{P}(X)$即为训练集中$X$出现的次数除以样本总数$m$。

> 最大熵模型的核心是将最大熵思想应用在分类问题上，那么最简单地应用就是默认对于任意的$x$，其类别的条件分布$P(y|x)$为均匀分布，显然，这是很不靠谱的。那么，如果已有一些观测到的样本，该如何利用呢？通俗地说，我们希望从中观察获取到一些规律/特点/性质，这些能够帮助我们分类。那么这些规律/特点/性质可以称之为**特征**。很多时候，一提起特征想到的就是$f(x)$，即对$x$进行一定的变换，使得模型学习的更容易。仔细思考，可以发现本质上其实就是对$x$进行一些变换，使得变换后和$y$的取值具有更强的相关性，即$x$的某一类取值变换后和$y$的某个值是很相关的，这句话的形式化表示就是$f(x,y)=1$。这么看来，$f(x)$是对样本的变换，而我们观察发现一些$x$值经过$f(x)$变换后总是对应$y$的某个分类，为了表示这句话，于是有了**特征函数** $f(x,y)$ 。

用特征函数$f(x,y)$描述输入$x$和输出$y$之间的关系。定义为：
$$
f(x,y)= \begin{cases} 1& {x与y满足某个关系}\\ 0& {否则} \end{cases}
$$


可以认为只要出现在训练集中出现的$(x^{(i)},y^{(i)})$, 其$f(x^{(i)},y^{(i)}) = 1$. 同一个训练样本可以有多个约束特征函数。

如何将$f(x,y)$和最大熵模型结合，如果没法和最大熵思想结合，显然是没什么用。对任何分类问题，理想中我们都希望知$P(x,y)$，但这是不可能的，甚至是根据样本对其进行估计都是很难的，一般来说，$\tilde{P}(X,Y)$和$P(x,y)$差别还是很大的。但是虽然无法准确估计每个$(x,y)$的概率，却可以较准确地估计样本空间中某个子集的概率，这是因为我们得到的样本虽然不足以对整个联合分布进行估计，但对其中一些项还是能够较准确地估计的，而$f(x,y)$的作用其实就是人为地去认定，那些估计是比较准确地。结合公式来看，特征函数$f(x,y)$关于经验分布$\tilde{P}(x, y)$的期望值，用$E_{\tilde{P}}(f)$表示为:
$$
\sum_{x, y} P(x, y) f(x, y) \approx  E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f(x, y)
$$
如果$f(x,y)=1$ ,其实就意味着满足该规律的样本在观测到的所有样本中是是占一定比例的，并且该比例足以让我们确信存在$f(x,y)$这样一个事实，意味着人为地认定这些样本没有过多或者过少地出现，换句话说，对于满足事实$f(x,y)$的所有$(x,y)$，他们的$\tilde{P}(x, y)$我们认为是近似于 $P(x,y)$的。而如果$f(x,y)=0$，那么即使$\tilde{P}(x, y)$估计的很差，也不影响下面这个事实：
$$
E_{\tilde{P}}(f)=\sum_{x, y} \tilde{P}(x, y) f(x, y) \approx \sum_{x, y} P(x, y) f(x, y)
$$
我们没法很好的估计$P(x,y)$，但我们可以很好的估计一个关于$P(x,y)$的式子，很好的估计$P(x,y)$的一部分。并且因为是求和，相当于估计$(x,y)$的一个子集，即使子集中的某一项估计的误差比较大，但分担到子集整体上，误差也还是比较小的，当然，这些也都是相对而言的，只能说在一般而言，我们对$\sum_{x, y} P(x, y) f(x, y)$进行估计要比对$P(x,y)$进行估计靠谱得多。

到这里，知道了$E_{\tilde{P}}(f)$的意思，也就不难理解整个约束条件的意思了。其实约束条件就是希望模型假设满足
$$
P(x,y) = P(x)P(y|x)
$$
虽然$\tilde{P}(x)$可以估计$P(x)$, 但$P(x,y)$不好估计，于是改为对$P(x,y) = P(x)P(y|x)$进行估计，对应的，后边的也要改为相应的形式，即
$$
E_{P}(f)=\sum_{x, y} \tilde{P}(x) P(y|x) f(x, y)
$$
然后依旧保持等式关系，就得到了约束条件
$$
E_{\tilde{P}}(f)=E_{P}(f)
$$
上式就是最大熵模型学习的约束条件，假如我们有$M$个特征函数$f_i(x,y) (i=1,2...,M)$就有$M$个约束条件。可以理解为我们如果训练集里有$m$个样本，就有和这$m$个样本对应的$M$个约束条件。



**最大熵模型的定义**如下：

> 假设满足所有约束条件的模型集合为：
>
> $$
> E_{\tilde{P}}(f_i) = E_{P}(f_i) (i=1,2,...M)
> $$
>
>
> 定义在条件概率分布$P(Y|X)$上的条件熵为：
> $$
> H(P) = -\sum\limits_{x,y}\tilde{P}(x)P(y|x)logP(y|x)
> $$
>
> 我们的目标是得到使$H(P)$最大的时候对应的$P(y|x)$, 这里可以对$H(P)$加了个负号求极小值，这样做的目的是为了使$−H(P)$为凸函数，方便使用凸优化的方法来求极值。



# 最大熵模型损失函数的优化

在上一节我们已经得到了最大熵模型的函数$H(P)$。它的损失函数$−H(P)$定义为:

$$
\underbrace{ min }_{P} -H(P) = \sum\limits_{x,y}\tilde{P}(x)P(y|x)logP(y|x)
$$

约束条件为：

$$
E_{\tilde{P}}(f_i) - E_{P}(f_i) = 0 (i=1,2,...M)
$$

$$
\sum\limits_yP(y|x) = 1
$$


由于它是一个凸函数，同时对应的约束条件为仿射函数，根据凸优化理论，这个优化问题可以用拉格朗日函数将其转化为无约束优化函数，此时损失函数对应的拉格朗日函数$L(P,w)$定义为：

$$
L(P,w) \equiv \sum\limits_{x,y}\tilde{P}(x)P(y|x)logP(y|x) + w_0(1 - \sum\limits_yP(y|x)) + \sum\limits_{i=1}^{M}w_i(E_{\tilde{P}}(f_i) - E_{P}(f_i))
$$
其中$w_i(i=1,2,...m)$为拉格朗日乘子。如果大家也学习过支持向量机，就会发现这里用到的凸优化理论是一样的，接着用到了拉格朗日对偶也一样。

我们的拉格朗日函数，即为凸优化的原始问题： 
$$
\underbrace{ min }_{P} \underbrace{ max }_{w}L(P, w)
$$


其对应的拉格朗日对偶问题为： 
$$
\underbrace{ max}_{w} \underbrace{ min }_{P}L(P, w)
$$


由于原始问题满足凸优化理论中的KKT条件，因此原始问题的解和对偶问题的解是一致的。这样我们的损失函数的优化变成了拉格朗日对偶问题的优化。

求解对偶问题的第一步就是求$\underbrace{ min }_{P}L(P, w)$, 这可以通过求导得到。这样得到的$\underbrace{ min }_{P}L(P, w)$是关于$w$的函数。记为：
$$
\psi(w) = \underbrace{ min }_{P}L(P, w) = L(P_w, w)
$$
$\psi(w)$即为对偶函数，将其解记为：
​
$$
P_w = arg \underbrace{ min }_{P}L(P, w) = P_w(y|x)
$$
具体的是求$L(P,w)$关于$P(y|x)$的偏导数:

$$
\frac{\partial L(P, w)}{\partial P(y|x)} = \sum\limits_{x,y}\tilde{P}(x)(logP(y|x) +1) -  \sum\limits_yw_0 - \sum\limits_{x,y}(\tilde{P}(x)\sum\limits_{i=1}^{M}w_if_i(x,y)) \\
= \sum\limits_{x,y}\tilde{P}(x)(logP(y|x) +1- w_0 -\sum\limits_{i=1}^{M}w_if_i(x,y))
$$


令偏导数为0，可以解出$P(y|x)$关于$w$的表达式如下：
$$
P(y|x) = exp(\sum\limits_{i=1}^{M}w_if_i(x,y) +w_0 -1) = \frac{exp(\sum\limits_{i=1}^{M}w_if_i(x,y))}{exp(1-w_0)}
$$
由于$\sum\limits_yP(y|x) = 1$，可以得到$P_w(y|x)$的表达式如下：
$$
P_w(y|x) = \frac{exp(\sum\limits_{i=1}^{M}w_if_i(x,y))}{\sum\limits_yexp(\sum\limits_{i=1}^{M}w_if_i(x,y))}
$$

这样我们就得出了$P(y|x)$和$w$的关系，从而可以把对偶函数$\psi(w)$里面的所有的$P(y|x)$替换成用$w$表示，这样对偶函数$\psi(w)$就是全部用$w$表示了。接着我们对$\psi(w)$求极大化，就可以得到极大化时对应的$w$向量的取值，带入$P(y|x)$和$w$的关系式， 从而也可以得到$P(y|x)$的最终结果。

对$\psi(w)$求极大化，由于它是连续可导的，所以优化方法有很多种，比如梯度下降法，牛顿法，拟牛顿法都可以。对于最大熵模型还有一种专用的优化方法，叫做改进的迭代尺度法(improved iterative scaling, IIS)。

IIS也是启发式方法，它假设当前的参数向量是$w$,我们希望找到一个新的参数向量$w+\delta$,使得对偶函数$\psi(w)$增大。如果能找到这样的方法，就可以重复使用这种方法，直到找到对偶函数的最大值。

IIS使用的方法是找到$\psi(w + \delta) - \psi(w)$的一个下界$B(w|\delta)$，通过对$B(w|\delta)$极小化来得到对应的$\delta$的值，进而来迭代求解$w$。对于$B(w|\delta)$，它的极小化是通过对$\delta$求偏导数而得到的。

由于IIS一般只用于最大熵模型，适用范围不广泛，这里就不详述算法过程了，感兴趣的朋友可以直接参考IIS的论文The improved iterative scaling algorithm: A gentle introduction。



# 最大熵模型小结

最大熵模型在分类方法里算是比较优的模型，但是由于它的**约束函数的数目一般来说会随着样本量的增大而增大**，导致样本量很大的时候，对偶函数优化求解的迭代过程非常慢，scikit-learn甚至都没有最大熵模型对应的类库。但是理解它仍然很有意义，尤其是它和很多分类方法都有千丝万缕的联系。　

**优点：** 

a) 最大熵统计模型获得的是所有满足约束条件的模型中信息熵极大的模型,作为经典的分类模型时准确率较高。

b) 可以灵活地设置约束条件，通过约束条件的多少可以调节模型对未知数据的适应度和对已知数据的拟合程度。



**缺点：**  

a) 由于约束函数数量和样本数目有关系，导致迭代过程计算量巨大，实际应用比较难。



# 最大熵模型与逻辑回归

假设模型为二分类，定义特征函数，其中$g(x)$为提取出每个$x$的特征, 输出$x$特征向量:
$$
f(x,y) = \left\{\begin{array}{ll}
g(x), &y  =1 \\
0  , &y=0
\end{array}\right.
$$
当$y=1$时，将以上特征函数代入到之前求出的最大嫡模型中:
$$
\begin{align}
P(y=1 \mid x)&=\frac{\exp \left(w_{i} g(x)\right)}{\exp \left(w_{i} g(x)\right)+\exp \left(w_{i} * 0\right)}\\
&=\frac{1}{1+\exp \left(-w_{i} g(x)\right)}
\end{align}
$$
当$y=0$时，将以上特征函数代入到之前求出的最大嫡模型中:
$$
\begin{align}
P(y=0 \mid x) &= \frac{\exp \left(w_{i} 0\right)}{\exp \left(w_{i} g(x)\right)+\exp \left(w_{i} * 0\right)} \\
 &= \frac{1}{\exp \left(w_{i} g(x)\right)+1} \\
 &= 1-p(y=1 \mid x)
\end{align}
$$
我们发现当类标签只有两个的时候，最大熵模型就是 logistic 回归模型。**逻辑回归其实就是最大熵模型在$y=1$时抽取$x$的特征的一种情况**，上一篇文章中，我们用极大似然估计来求参数$w_i$,其实这样求出的模型就求了$max(P_w(y|x))$  ,所以就是求熵最大的模型。

我的理解是最大熵模型是一种很抽象的描述，在不同的场景和业务中我们可能需要定义不同的特征函数求出对应的权重值，来解决不同的问题。



# 资料

1. https://www.zhihu.com/question/24094554
2. https://zhuanlan.zhihu.com/p/36012167
3. https://www.cnblogs.com/pinard/p/6093948.html