在所有的机器学习分类算法中，朴素贝叶斯和其他绝大多数的分类算法都不同。对于大多数的分类算法，比如决策树、KNN、逻辑回归，支持向量机等，他们都是**判别方法**，也就是直接学习出特征输出Y和特征X之间的关系，要么是决策函数$Y=f(X)$,要么是条件分布$P(Y|X)$。

但是朴素贝叶斯却是**生成方法**，也就是直接找出特征输出Y和特征X的联合分布$P(X,Y)$, 然后用$P(Y|X) = P(X,Y)/P(X)$得出。

朴素贝叶斯很直观，计算量也不大，在很多领域有广泛的应用。

# 朴素贝叶斯相关的统计学知识

在了解朴素贝叶斯的算法之前，我们需要对相关必须的统计学知识做一个回顾。

**贝叶斯学派**很古老，但是从诞生到一百年前一直不是主流。主流是**频率学派**，频率学派的权威皮尔逊和费歇尔都对贝叶斯学派不屑一顾，但是贝叶斯学派硬是凭借在现代特定领域的出色应用表现为自己赢得了半壁江山。

贝叶斯学派的思想可以概括为**先验概率+数据=后验概率**。也就是说我们在实际问题中需要得到的后验概率，可以通过先验概率和数据一起综合得到。数据大家好理解，被频率学派攻击的是先验概率，一般来说先验概率就是我们对于数据所在领域的历史经验，但是这个经验常常难以量化或者模型化，于是贝叶斯学派大胆的假设先验分布的模型，比如正态分布，beta分布等。这个假设一般没有特定的依据，因此一直被频率学派认为很荒谬。虽然难以从严密的数学逻辑里推出贝叶斯学派的逻辑，但是在很多实际应用中，贝叶斯理论很好用，比如垃圾邮件分类，文本分类。　

我们先看看**条件独立公式**，如果X和Y相互独立，则有：
$$
P(X,Y) =P(X)P(Y)
$$


**条件概率公式**：
$$
P(Y|X) = \frac{P(X,Y)}{P(X)}
$$

$$
P(X|Y) = \frac{P(X,Y)}{P(Y)}
$$


贝叶斯公式的常规形式:
$$
P(Y|X) = \frac{P(X|Y)P(Y)}{P(X)}
$$
**全概率公式**
$$
P(X) = \sum\limits_{k}P(X|Y =Y_k)P(Y_k)
$$

其中　$\sum\limits_{k}P(Y_k)=1$  

从上面的公式很容易得出**贝叶斯公式**：
$$
P(Y_k|X) = \frac{P(X|Y_k)P(Y_k)}{\sum\limits_{k}P(X|Y =Y_k)P(Y_k)}
$$

# 朴素贝叶斯的模型

从统计学知识回到我们的数据分析。假如我们的分类模型样本是：
$$
(x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)}, y_1), (x_1^{(2)}, x_2^{(2)}, ...x_n^{(2)},y_2), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_n)
$$
即我们有$m$个样本，每个样本有$n$个特征，特征输出有$K$个类别，定义为${C_1,C_2,...,C_K}$。

从样本我们可以学习得到朴素贝叶斯的**先验分布**$P(Y=C_k)(k=1,2,...K)$,

接着学习到**条件概率分布**$P(X=x|Y=C_k) = P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)$,

然后我们就可以用贝叶斯公式得到X和Y的**联合分布**P(X,Y)了。联合分布P(X,Y)定义为：
$$
\begin{align} P(X,Y=C_k)  &= P(X=x|Y=C_k)P(Y=C_k) \\&= P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)P(Y=C_k) \end{align}
$$
从上面的式子可以看出$P(Y=C_k)$比较容易通过最大似然法求出，得到的$P(Y=C_k)$就是类别$C_k$在训练集里面出现的频数。但是$P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k)$很难求出, 这是一个超级复杂的有n个维度的条件分布。朴素贝叶斯模型在这里做了一个大胆的假设，**即X的n个维度之间相互独立**，这样就可以得出：

$$
\begin{align} & P(X_1=x_1, X_2=x_2,...X_n=x_n|Y=C_k) \\
=& P(X_1=x_1|Y=C_k)P(X_2=x_2|Y=C_k)...P(X_n=x_n|Y=C_k)
\end{align}
$$
从上式可以看出，这个很难的条件分布大大的简化了，但是这也可能带来预测的不准确性。你会说如果我的特征之间非常不独立怎么办？**如果真是非常不独立的话，那就尽量不要使用朴素贝叶斯模型了**，考虑使用其他的分类方法比较好。但是一般情况下，样本的特征之间独立这个条件的确是弱成立的，尤其是数据量非常大的时候。虽然我们牺牲了准确性，但是得到的好处是模型的条件分布的计算大大简化了，这就是贝叶斯模型的选择。

最后回到我们要解决的问题，我们的问题是给定测试集的一个新样本特征$(x_1^{(test)}, x_2^{(test)}, ...x_n^{(test)})$，我们如何判断它属于哪个类型？

既然是贝叶斯模型，当然是后验概率最大化来判断分类了。我们只要计算出所有的$K$个条件概率$P(Y=C_k|X=X^{(test)})$,然后找出最大的条件概率对应的类别，这就是朴素贝叶斯的预测了。



# 朴素贝叶斯的推断过程

上节我们已经对朴素贝叶斯的模型也预测方法做了一个大概的解释，这里我们对朴素贝叶斯的推断过程做一个完整的诠释过程。

我们预测的类别$C_{result}$是使$P(Y=C_k|X=X^{(test)})$最大化的类别，数学表达式为：

$$
\begin{align} 
C_{result}  & = \underbrace{argmax}_{C_k}P(Y=C_k|X=X^{(test)}) \\
& = \underbrace{argmax}_{C_k} \frac{P(X=X^{(test)}|Y=C_k)P(Y=C_k)}{P(X=X^{(test)})}
\end{align}
$$

由于对于所有的类别计算$P(Y=C_k|X=X^{(test)})$时，上式的分母是一样的，都是$P(X=X^{(test)})$，因此，我们的预测公式可以简化为：

$$
C_{result}  = \underbrace{argmax}_{C_k}P(X=X^{(test)}|Y=C_k)P(Y=C_k)
$$

接着我们利用朴素贝叶斯的独立性假设，就可以得到通常意义上的朴素贝叶斯推断公式:

$$
C_{result}  = \underbrace{argmax}_{C_k}P(Y=C_k)\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)
$$

# 案例说明

## 离散

数据如下：

|  帅  | 性格 | 身高 |  上进  | 家否 |
| :--: | :--: | :--: | :----: | :--: |
|  帅  | 不好 |  矮  | 不上进 | 不嫁 |
| 不帅 |  好  |  矮  |  上进  | 不嫁 |
|  帅  |  好  |  矮  |  上进  |  嫁  |
| 不帅 |  好  |  高  |  上进  |  嫁  |
|  帅  | 不好 |  矮  |  上进  | 不嫁 |
| 不帅 | 不好 |  矮  | 不上进 | 不嫁 |
|  帅  |  好  |  高  | 不上进 |  嫁  |
| 不帅 |  好  |  高  |  上进  |  嫁  |
|  帅  |  好  |  高  |  上进  |  嫁  |
| 不帅 | 不好 |  高  |  上进  |  嫁  |
|  帅  |  好  |  矮  | 不上进 | 不嫁 |
|  帅  |  好  |  矮  |  上进  | 不嫁 |



男生的四个特点分别是：不帅，性格不好，身高矮，不上进，请判断女生是嫁还是不嫁?

该问题转换为数学问题就是比较：
$$
\begin{align}
&p(嫁|不帅，性格不好，身高矮，不上进) \\
&p(不嫁|不帅，性格不好，身高矮，不上进)
\end{align}
$$


由贝叶斯公式得：
$$
p(嫁|不帅，性格不好，身高矮，不上进) = \frac{p(不帅，性格不好，身高矮，不上进|嫁) * p(嫁)}{p(不帅，性格不好，身高矮，不上进)}
$$
假设各个特征相互独立，即：
$$
p(嫁|不帅，性格不好，身高矮，不上进) = \frac{p(不帅|嫁)*p(性格不好|嫁)*p(身高矮|嫁)*p(不上进|嫁) * p(嫁)}{p(不帅，性格不好，身高矮，不上进)}
$$


首先我们整理训练数据中：

- 嫁的样本数总共有6个, 则 $p(嫁)=\frac{6}{12}=\frac{1}{2}$
- 不帅, 也嫁了的样本数总共有6个, 则 $p(不帅|嫁)=\frac{3}{6}=\frac{1}{2}$
- 性格不好, 也嫁了的样本数总共有1个, 则 $p(性格不好|嫁)=\frac{1}{6}$
- 身高矮, 也嫁了的样本数总共有1个, 则 $p(身高矮|嫁)=\frac{1}{6}$
- 不上进, 也嫁了的样本数总共有1个, 则 $p(不上进|嫁)=\frac{1}{6}$

$$
\begin{align}
p(嫁|不帅，性格不好，身高矮，不上进) = & \frac{\frac{1}{2} * \frac{1}{6} * \frac{1}{6} * \frac{1}{6} * \frac{1}{2}}{p(不帅，性格不好，身高矮，不上进)} \\
= & \frac{\frac{1}{864}}{p(不帅，性格不好，身高矮，不上进)}
\end{align}
$$



`同理`：

- 不嫁的样本数总共有6个, 则 $p(不嫁)=\frac{6}{12}=\frac{1}{2}$
- 不帅, 就不嫁的样本数总共有1个, 则 $p(不帅 \mid 不嫁)=\frac{1}{6}$
- 性格不好, 就不嫁的样本数总共有3个, 则 $p(性格不好 \mid 不嫁)=\frac{3}{6}=\frac{1}{2}$
- 矮, 就不嫁的样本数总共有6个, 则 $p(身高矮|不嫁)=\frac{6}{6}=1$;
- 不上进, 就不嫁的样本数总共有3个, 则 $p(不上进|不嫁)=\frac{3}{6}=\frac{1}{2}$

$$
\begin{align}
p(不嫁|不帅，性格不好，身高矮，不上进) = & \frac{\frac{1}{2} * \frac{1}{6} * \frac{1}{2} * 1 * \frac{1}{2}}{p(不帅，性格不好，身高矮，不上进)} \\
= & \frac{\frac{1}{48}}{p(不帅，性格不好，身高矮，不上进)}
\end{align}
$$

由于分母都相同, 且分子 $\frac{1}{864}<\frac{1}{48},$ 所以走后得出的结论是该女生不嫁给这个男生。



## 连续

**连续型实例**，训练数据如下：

| 性别 | 身高(英尺) | 体重(磅) | 脚的尺寸(英寸) |
| :--: | :--------: | :------: | :------------: |
|  男  |     6      |   180    |       12       |
|  男  |    5.92    |   190    |       11       |
|  男  |    5.58    |   170    |       12       |
|  男  |    5.92    |   165    |       10       |
|  女  |     5      |   100    |       6        |
|  女  |    5.5     |   150    |       8        |
|  女  |    5.42    |   130    |       7        |
|  女  |    5.75    |   150    |       9        |



通过以上人体测量特征，包括身高、体重、脚的尺寸，判断一个人是男性还是女性。

**测试样本**：

|  性别  | 身高(英尺) | 体重(磅) | 脚的尺寸(英寸) |
| :----: | :--------: | :------: | :------------: |
| sample |     6      |   130    |       8        |



首先，假设人的身高，体重，脚的尺寸都满足高斯分布，分别计算各个特征的均值和方差，得到下表：

| 性别 | 均值(身高) | 方差(身高) | 均值(体重) | 方差(体重) | 均值(脚的尺寸) | 方差(脚的尺寸) |
| :--: | ---------: | :--------- | ---------: | :--------- | -------------: | -------------- |
| 男性 |      5.855 | 0.035033   |     176.25 | 122.92     |          11.25 | 0.91667        |
| 女性 |     5.4175 | 0.097225   |      132.5 | 558.33     |            7.5 | 1.6667         |



其次，我们认为先验概率是男性或者是女性是等概率的，即 $p(male)=p(female)=0.5$，或者通过统计样本中男女比例来作为先验概率也可以，本例得到的结果是一样的。

判断该条测试样本属于男性还是女性，就等价于比较是男性的后验概率和女性的后验概率哪个大。
$$
\begin{aligned}
& posterior(male)=\frac{p(male) * p(height \mid male) * p(weight \mid male) * p(footsize \mid male)}{evidence}
\end{aligned}
$$

$$
\begin{aligned}
posterior(female)=\frac{p(female) * p(height \mid female) * p(weight \mid female) * p(footsize \mid female)}{evidence}
\end{aligned}
$$


分母是个常数，只需要比较分子就行，这里给出分母的值：
$$
\begin{align}
evidence &=  p(male) * p(height \mid male) * p(weight \mid male) * p(foot size \mid male) \\
& + p(female) * p(height \mid female) * p(weight \mid female) * p(footsize \mid female)
\end{align}
$$


计算男性的后验概率： $p(male)=0.5$，其中

$p(height \mid male)=\frac{1}{\sqrt{2 \pi \sigma^{2}}} e^{-\frac{(6-\mu)^{2}}{2 \sigma^{2}}} \approx 1.5789$,  其中 $ \mu=5.855, \sigma^{2}=3.5033 e^{-2}$

这里大于1是因为是概率密度函数，而不是概率分布函数，所以大于1也是合理的。
$$
p(weight \mid \text {male})=5.9881 e^{-6}
$$

$$
p(footsize|male) =1.3112 e^{-3}
$$

$$
posterior(male) =\frac{6.1984 e^{-9}}{\text {evidence}}
$$



计算女性的后验概率： $p(female)=0.5$
$$
p(height|female) =2.2346 e^{-1}
$$

$$
p(weight|f emale) =1.6789 e^{-2}
$$

$$
p(footsize|female) =2.8669 e^{-1}
$$

$$
posterior(female) =\frac{5.3778 e^{-4}}{\text {evidence}}
$$

由于女性的后验概率分子较大，所以我们预测这个样本的为女性。



# 朴素贝叶斯的参数估计

在上一节中，我们知道只要求出$P(Y=C_k)$和$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,...n)$，我们通过比较就可以得到朴素贝叶斯的推断结果。这一节我们就讨论怎么通过训练集计算这两个概率。

对于$P(Y=C_k)$,比较简单，通过极大似然估计我们很容易得到$P(Y=C_k)$为样本类别$C_k$出现的频率，即样本类别$C_k$出现的次数$m_k$除以样本总数$m$。

对于$P(X_j=X_j^{(test)}|Y=C_k)(j=1,2,...n)$, 这个取决于我们的先验条件：

**a)** 如果我们的$X_j$是**离散**的值，那么我们可以假设$X_j$符合**多项式分布**，这样得到$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中，特征$X_j^{(test)}$出现的频率。即：
$$
P(X_j=X_j^{(test)}|Y=C_k) = \frac{m_{kj^{test}}}{m_k}
$$

其中$m_k$为样本类别$C_k$总的特征计数，而$m_{kj^{test}}$为类别为$C_k$的样本中，第$j$维特征$X_j^{(test)}$出现的计数。

某些时候，可能某些类别在样本中没有出现，这样可能导致$P(X_j=X_j^{(test)}|Y=C_k)$为0，这样会影响后验的估计，为了解决这种情况，我们引入了**拉普拉斯平滑**，即此时有：

$$
P(X_j=X_j^{(test)}|Y=C_k) = \frac{m_{kj^{test}} + \lambda}{m_k + O_j\lambda}
$$
其中$\lambda$为一个大于0的常数，通常取为1。$O_j$为第$j$个特征的取值个数。

**b)** 如果我们我们的$X_j$是**非常稀疏**的离散值，即各个特征出现概率很低，这时我们可以假设$X_j$符合**伯努利分布**，即特征$X_j$出现记为1，不出现记为0。即只要$X_j$出现即可，我们不关注$X_j$的次数。这样得到$P(X_j=X_j^{(test)}|Y=C_k)$是在样本类别$C_k$中，$X_j^{(test)}$出现的频率。此时有：
$$
P(X_j=X_j^{(test)}|Y=C_k) = P(X_j|Y=C_k)X_j^{(test)} + (1 - P(X_j|Y=C_k))(1-X_j^{(test)})
$$

其中，$X_j^{(test)}$取值为0和1。

**c)** 如果我们我们的$X_j$是**连续**值，我们通常取$X_j$的先验概率为**正态分布**，即在样本类别$C_k$中，$X_j$的值符合正态分布。这样$P(X_j=X_j^{(test)}|Y=C_k)$的概率分布是：
$$
P(X_j=X_j^{(test)}|Y=C_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}exp\Bigg{(}-\frac{(X_j^{(test)} - \mu_k)^2}{2\sigma_k^2}\Bigg{)}
$$

其中$\mu_k$和$\sigma_k^2$是正态分布的期望和方差，可以通过极大似然估计求得。$\mu_k$为在样本类别$C_k$中，所有$X_j$的平均值。$\sigma_k^2$为在样本类别$C_k$中，所有$X_j$的方差。对于一个连续的样本值，带入正态分布的公式，就可以求出概率分布了。

# 朴素贝叶斯算法过程

我们假设训练集为$m$个样本$n$个维度，如下：

$$
(x_1^{(0)}, x_2^{(0)}, ...x_n^{(0)}, y_0), (x_1^{(1)}, x_2^{(1)}, ...x_n^{(1)},y_1), ... (x_1^{(m)}, x_2^{(m)}, ...x_n^{(m)}, y_n)
$$

共有$K$个特征输出类别，分别为${C_1,C_2,...,C_K}$, 每个特征输出类别的样本个数为${m_1,m_2,...,m_K}$,

在第$k$个类别中，如果是离散特征，则特征$X_j$各个类别取值为$m_{jl}$。其中 $l$ 取值为$1,2,...S_j$，$S_j$为特征$j$不同的取值数。

输出为实例$X^{(test)}$的分类。



**算法流程如下：**

**1)** 如果没有$Y$的先验概率，则计算$Y$的$K$个先验概率：$P(Y=C_k) = (m_k+\lambda)/(m+K\lambda)$，否则$P(Y=C_k)$为输入的先验概率。

**2)** 分别计算第 $k$ 个类别的第 $j$ 维特征的第 $l$ 个取值条件概率：$P(X_j=x_{jl}|Y=C_k)$ 

1. 如果是离散值：
$$
P(X_j=x_{jl}|Y=C_k) = \frac{m_{kjl} + \lambda}{m_k + S_j\lambda}
$$
$\lambda$可以取值为1，或者其他大于0的数字。
2. 如果是稀疏二项离散值：
$$
P(X_j=x_{jl}|Y=C_k) = P(j|Y=C_k)x_{jl} + (1 - P(j|Y=C_k)(1-x_{jl})
$$
 此时$l$只有两种取值。
3. 如果是连续值不需要计算各个 $l$ 的取值概率，直接求正态分布的参数:

$$
P(X_j=x_j|Y=C_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}exp\Bigg{(}-\frac{(x_j - \mu_k)^2}{2\sigma_k^2}\Bigg{)}
$$
需要求出$\mu_k$和$\sigma_k^2$。 $\mu_k$为在样本类别$C_k$中，所有$X_j$的平均值。$\sigma_k^2$为在样本类别$C_k$中，所有$X_j$的方差。

**3）**对于实例$X^{(test)}$，分别计算：
$$
P(Y=C_k)\prod_{j=1}^{n}P(X_j=x_j^{(test)}|Y=C_k)
$$

**4）**确定实例$X^{(test)}$的分类$C_{result}$
$$
C_{result}  = \underbrace{argmax}_{C_k}P(Y=C_k)\prod_{j=1}^{n}P(X_j=X_j^{(test)}|Y=C_k)
$$

从上面的计算可以看出，没有复杂的求导和矩阵运算，因此效率很高。

# 朴素贝叶斯算法小结

 朴素贝叶斯算法的主要原理基本已经做了总结，这里对朴素贝叶斯的优缺点做一个总结。

> **优点**

1. 朴素贝叶斯模型发源于古典数学理论，**有稳定的分类效率**。
2. 对小规模的数据表现很好，**能个处理多分类任务**，适合增量式训练，尤其是数据量超出内存时，我们可以一批批的去**增量训练**。
3. **对缺失数据不太敏感**，算法也比较**简单**，常用于**文本分类**。



> **缺点**

1.  理论上，朴素贝叶斯模型与其他分类方法相比具有最小的误差率。但是实际上并非总是如此，这是因为朴素贝叶斯模型给定输出类别的情况下,**假设属性之间相互独立**，这个假设在实际应用中往往是不成立的，在属性个数比较多或者属性之间相关性较大时，分类效果不好。而在属性相关性较小时，朴素贝叶斯性能最为良好。对于这一点，有半朴素贝叶斯之类的算法通过考虑部分关联性适度改进。
2. 需要知道先验概率，且**先验概率很多时候取决于假设**，假设的模型可以有很多种，因此在某些时候会由于假设的先验模型的原因导致预测效果不佳。
3. 由于我们是通过先验和数据来决定后验的概率从而决定分类，所以分类决策存在一定的错误率。
4. 对输入数据的表达形式很敏感。



# scikit-learn 朴素贝叶斯类库使用小结

## scikit-learn 朴素贝叶斯类库概述

朴素贝叶斯是一类比较简单的算法，scikit-learn中朴素贝叶斯类库的使用也比较简单。相对于决策树，KNN之类的算法，朴素贝叶斯需要关注的参数是比较少的，这样也比较容易掌握。在scikit-learn中，一共有3个朴素贝叶斯的分类算法类。分别是GaussianNB，MultinomialNB和BernoulliNB。其中:
- GaussianNB就是先验为**高斯分布**的朴素贝叶斯。
- MultinomialNB就是先验为**多项式分布**的朴素贝叶斯。
- 而BernoulliNB就是先验为**伯努利分布**的朴素贝叶斯。

这三个类适用的分类场景各不相同，一般来说，如果样本特征的分布**大部分是连续值**，使用GaussianNB会比较好。如果如果样本特征的分大部分是**多元离散值**，使用MultinomialNB比较合适。而如果样本特征是**二元离散值或者很稀疏的多元离散值**，应该使用BernoulliNB。

## GaussianNB类使用总结

GaussianNB假设特征的先验概率为正态分布，即如下式：

$$
P(X_j=x_j|Y=C_k) = \frac{1}{\sqrt{2\pi\sigma_k^2}}exp\Bigg{(}-\frac{(x_j - \mu_k)^2}{2\sigma_k^2}\Bigg{)}
$$

其中$C_k$为$Y$的第$k$类类别。$\mu_k$和$\sigma_k^2$为需要从训练集估计的值。

GaussianNB会根据训练集求出$\mu_k$和$\sigma_k^2$。 $\mu_k$为在样本类别$C_k$中，所有$X_j$的平均值。$\sigma_k^2$为在样本类别$C_k$中，所有$X_j$的方差。

GaussianNB类的主要参数仅有一个，即**先验概率priors** ，对应Y的各个类别的先验概率$P(Y=C_k)$。这个值默认不给出，如果不给出此时$P(Y=C_k) = m_k/m$。其中m为训练集样本总数量，$m_k$为输出为第$k$类别的训练集样本数。如果给出的话就以priors 为准。

在使用GaussianNB的fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。

- predict方法就是我们最常用的预测方法，直接给出测试集的预测类别输出。
- predict_proba则不同，它会给出测试集样本在各个类别上预测的概率。容易理解，predict_proba预测出的各个类别概率里的最大值对应的类别，也就是predict方法得到类别。
- predict_log_proba和predict_proba类似，它会给出测试集样本在各个类别上预测的概率的一个对数转化。转化后predict_log_proba预测出的各个类别对数概率里的最大值对应的类别，也就是predict方法得到类别。

> **实例**


```python
import numpy as np
X = np.array([[-1, -1], [-2, -1], [-3, -2], [1, 1], [2, 1], [3, 2]])
Y = np.array([1, 1, 1, 2, 2, 2])
from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
#拟合数据
clf.fit(X, Y)
print("==Predict result by predict==")
print((clf.predict([[-0.8, -1]])))
print("==Predict result by predict_proba==")
print((clf.predict_proba([[-0.8, -1]])))
print("==Predict result by predict_log_proba==")
print((clf.predict_log_proba([[-0.8, -1]])))
```

    ==Predict result by predict==
    [1]
    ==Predict result by predict_proba==
    [[9.99999949e-01 5.05653254e-08]]
    ==Predict result by predict_log_proba==
    [[-5.05653266e-08 -1.67999998e+01]]


从上面的结果可以看出，测试样本[-0.8,-1]的类别预测为类别1。具体的测试样本[-0.8,-1]被预测为1的概率为9.99999949e-01 ，远远大于预测为2的概率5.05653254e-08。这也是为什么最终的预测结果为1的原因了。

此外，GaussianNB一个重要的功能是有 partial_fit方法，这个方法的一般用在如果训练集数据量非常大，一次不能全部载入内存的时候。这时我们可以把训练集分成若干等分，重复调用partial_fit来一步步的学习训练集，非常方便。后面讲到的MultinomialNB和BernoulliNB也有类似的功能。

## MultinomialNB类使用总结

MultinomialNB假设特征的先验概率为多项式分布，即如下式：

$$
P(X_j=x_{jl}|Y=C_k) = \frac{x_{jl} + \lambda}{m_k + n\lambda}
$$

其中，$P(X_j=x_{jl}|Y=C_k)$是第k个类别的第$j$维特征的第$l$个个取值条件概率。$m_k$是训练集中输出为第$k$类的样本个数。$\lambda$为一个大于0的常数，常常取为1，即**拉普拉斯平滑**。也可以取其他值。

MultinomialNB参数比GaussianNB多，但是一共也只有仅仅3个。
- 其中，参数**alpha**即为上面的常数$\lambda$，如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数。
- 布尔参数**fit_prior**表示是否要考虑先验概率，如果是false,则所有的样本类别输出都有相同的类别先验概率。
- 否则可以自己用第三个参数**class_prior**输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为$P(Y=C_k) = m_k/m$。其中m为训练集样本总数量，$m_k$为输出为第k类别的训练集样本数。总结如下：

| **fit_prior** | **class_prior**    | **最终先验概率**       |
| ------------- | ------------------ | ---------------------- |
| false         | 填或者不填没有意义 | $P(Y=C_k)=1/k$         |
| true          | 不填               | $P(Y=C_k)=m_k/m$       |
| true          | 填                 | $P(Y=C_k)=class_prior$ |



在使用MultinomialNB的fit方法或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。由于方法和GaussianNB完全一样，这里就不累述了。

## BernoulliNB类使用总结

BernoulliNB假设特征的先验概率为二元伯努利分布，即如下式：

$$
P(X_j=x_{jl}|Y=C_k) = P(j|Y=C_k)x_{jl} + (1 - P(j|Y=C_k)(1-x_{jl})
$$

此时$l$只有两种取值。$x_{jl}$只能取值0或者1。

BernoulliNB一共有4个参数，其中3个参数的名字和意义和MultinomialNB完全相同。唯一增加的一个参数是**binarize**。这个参数主要是用来帮BernoulliNB处理二项分布的，可以是数值或者不输入。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类。

在使用BernoulliNB的fit或者partial_fit方法拟合数据后，我们可以进行预测。此时预测有三种方法，包括predict，predict_log_proba和predict_proba。由于方法和GaussianNB完全一样，这里就不累述了。



# 答题

1. 怎么利用贝叶斯方法，实现”拼写检查”的功能

2. **为什么朴素贝叶斯如此“朴素”？**

因为它假定所有的特征在数据集中的作用是同样重要和独立的。正如我们所知，这个假设在现实世界中是很不真实的，因此，说朴素贝叶斯真的很“朴素”。



3. **简单说说贝叶斯定理**

