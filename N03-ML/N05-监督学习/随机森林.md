在集成学习原理小结中，我们讲到了集成学习有两个流派，一个是boosting派系，它的特点是各个弱学习器之间有依赖关系。另一种是bagging流派，它的特点是各个弱学习器之间没有依赖关系，可以并行拟合。本文就对集成学习中Bagging与随机森林算法做一个总结。

随机森林是集成学习中可以和梯度提升树GBDT分庭抗礼的算法，尤其是它可以很方便的并行训练，在如今大数据大样本的的时代很有诱惑力。

# bagging的原理

在集成学习原理小结中，我们给Bagging画了下面一张原理图。

![](./images/随机森林/随机森林-20201215-223659-735046-1703346738791-7.png)



从上图可以看出，Bagging的弱学习器之间的确没有boosting那样的联系。它的特点在"随机采样"。那么什么是随机采样？

**随机采样**(bootsrap)就是从我们的训练集里面采集**固定个数**的样本，但是每采集一个样本后，都将样本**放回**。也就是说，之前采集到的样本在放回后有可能继续被采集到。

对于我们的Bagging算法，一般会随机采集和训练集样本数$m$一样个数的样本。这样得到的采样集和训练集样本的个数相同，但是样本内容不同。如果我们对有$m$个样本训练集做$T$次的随机采样，则由于随机性，$T$个采样集各不相同。

注意到这和GBDT的子采样是不同的。**GBDT的子采样是无放回采样，而Bagging的子采样是放回采样**。

对于一个样本，它在某一次含m个样本的训练集的随机采样中，每次被采集到的概率是$\frac{1}{m}$。不被采集到的概率为$1-\frac{1}{m}$。如果m次采样都没有被采集中的概率是$(1-\frac{1}{m})^m$。当$m \to \infty$时，$(1-\frac{1}{m})^m \to \frac{1}{e} \simeq 0.368$。也就是说，在bagging的每轮随机采样中，训练集中大约有36.8%的数据没有被采样集采集中。

对于这部分大约36.8%的没有被采样到的数据，我们常常称之为**袋外数据**(Out Of Bag, 简称OOB)。这些数据没有参与训练集模型的拟合，因此可以用来检测模型的泛化能力。

bagging对于弱学习器没有限制，这和Adaboost一样。但是最常用的一般也是决策树和神经网络。

bagging的**集合策略**也比较简单，对于**分类**问题，通常使用简单投票法，得到最多票数的类别或者类别之一为最终的模型输出。对于**回归**问题，通常使用简单平均法，对T个弱学习器得到的回归结果进行算术平均得到最终的模型输出。

由于Bagging算法每次都进行采样来训练模型，因此**泛化能力很强**，对于降低模型的方差很有作用。当然对于训练集的拟合程度就会差一些，也就是模型的**偏差**会大一些。



# bagging算法流程

上一节我们对bagging算法的原理做了总结，这里就对bagging算法的流程做一个总结。相对于Boosting系列的Adaboost和GBDT，bagging算法要简单的多。

**输入**为样本集$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱学习器算法, 弱分类器迭代次数T。

**输出**为最终的强分类器$f(x)$ 。

1. 对于t=1,2...,T: 
	- 对训练集进行第$t$次随机采样，共采集$m$次，得到包含$m$个样本的采样集$D_t$ 
	- 用采样集$D_t$训练第$t$个弱学习器$G_t(x)$ 
2. 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。



# 随机森林算法

理解了bagging算法，随机森林(Random Forest, 以下简称RF)就好理解了。它是Bagging算法的进化版，也就是说，它的思想仍然是bagging,但是进行了独有的改进。我们现在就来看看RF算法改进了什么。　

首先，RF使用了**CART决策树**作为弱学习器，这让我们想到了梯度提升树GBDT。

第二，在使用决策树的基础上，RF对决策树的建立做了改进，对于普通的决策树，我们会在节点上所有的n个样本特征中选择一个最优的特征来做决策树的左右子树划分，但是RF通过**随机选择节点上的一部分样本特征**，这个数字小于n，假设为$n_{sub}$，**然后在这些随机选择的$n_{sub}$个样本特征中，选择一个最优的特征来做决策树的左右子树划分**。这样进一步增强了模型的泛化能力。

如果$n_{sub}=n$，则此时RF的CART决策树和普通的CART决策树没有区别。$n_{sub}$越小，则模型越健壮，当然此时对于训练集的拟合程度会变差。也就是说**$n_{sub}$越小，模型的方差会减小，但是偏差会增大**。在实际案例中，一般会通过交叉验证调参获取一个合适的$n_{sub}$的值。

除了上面两点，RF和普通的bagging算法没有什么不同， 下面简单总结下RF的算法。

**输入**为样本集$D=\{(x_,y_1),(x_2,y_2), ...(x_m,y_m)\}$，弱分类器迭代次数$T$。

**输出**为最终的强分类器$f(x)$。 

1. 对于$t=1, 2..., T$: 

	**a)** 对训练集进行第t次随机采样，共采集m次，得到包含m个样本的采样集$D_t$。 
	
	**b)** 用采样集$D_t$训练第t个决策树模型$G_t(x)$，在训练决策树模型的节点的时候， 在节点上所有的样本特征中**选择一部分样本特征**， 在这些随机选择的部分样本特征中选择一个最优的特征来做决策树的左右子树划分。 
2. 如果是分类算法预测，则$T$个弱学习器投出最多票数的类别或者类别之一为最终类别。如果是回归算法，$T$个弱学习器得到的回归结果进行算术平均得到的值为最终的模型输出。



# 随机森林的推广

 由于RF在实际应用中的良好特性，基于RF，有很多变种算法，应用也很广泛，不光可以用于分类回归，还可以用于特征转换，异常点检测等。下面对于这些RF家族的算法中有代表性的做一个总结。

## extra trees

extra trees是RF的一个变种, 原理几乎和RF一模一样，仅有区别有：

1. 对于每个决策树的训练集，RF采用的是随机采样bootstrap来选择采样集作为每个决策树的训练集，而extra trees一般**不采用随机采样，即每个决策树采用原始训练集**。
2. 在选定了划分特征后，RF的决策树会基于基尼系数，均方差之类的原则，选择一个最优的特征值划分点，这和传统的决策树相同。但是extra trees比较的激进，他会**随机的选择一个特征值来划分决策树**。

从第二点可以看出，由于随机选择了特征值的划分点位，而不是最优点位，这样会导致生成的决策树的规模一般会大于RF所生成的决策树。也就是说，**模型的方差相对于RF进一步减少，但是偏差相对于RF进一步增大**。在某些时候，extra trees的泛化能力比RF更好。



## Totally Random Trees Embedding

Totally Random Trees Embedding(以下简称 TRTE)是一种**非监督学习的数据转化方法**。它将低维的数据集映射到高维，从而让映射到高维的数据更好的运用于分类回归模型。我们知道，在支持向量机中运用了核方法来将低维的数据集映射到高维，此处TRTE提供了另外一种方法。

TRTE在数据转化的过程也使用了类似于RF的方法，建立$T$个决策树来拟合数据。当决策树建立完毕以后，数据集里的每个数据在$T$个决策树中叶子节点的位置也定下来了。

比如我们有3颗决策树，每个决策树有5个叶子节点，某个数据特征x划分到第一个决策树的第2个叶子节点，第二个决策树的第3个叶子节点，第三个决策树的第5个叶子节点。则x映射后的特征编码为(0,1,0,0,0,  0,0,1,0,0,  0,0,0,0,1), 有15维的高维特征。这里特征维度之间加上空格是为了强调三颗决策树各自的子编码。

映射到高维特征后，可以继续使用监督学习的各种分类回归算法了。



## Isolation Forest

 Isolation Forest（以下简称IForest）是一种**异常点检测**的方法。它也使用了类似于RF的方法来检测异常点。

对于在T个决策树的样本集，IForest也会对训练集进行随机采样, 但是采样个数不需要和RF一样，对于RF，需要采样到**采样集样本个数等于训练集个数**。但是IForest不需要采样这么多，一般来说，**采样个数要远远小于训练集个数**, 因为我们的目的是异常点检测，只需要部分的样本我们一般就可以将异常点区别出来了。

对于每一个决策树的建立， IForest采用**随机选择一个划分特征，对划分特征随机选择一个划分阈值**。这点也和RF不同。

另外，IForest一般会选择一个比较小的最大决策树深度max_depth, 原因同样本采集, 用少量的异常点检测一般不需要这么大规模的决策树。

对于`异常点的判断`，则是将测试样本点$x$拟合到$T$颗决策树。计算在每颗决策树上该样本的叶子节点的深度$h_t(x)$。从而可以计算出平均高度$h(x)$。此时我们用下面的公式计算样本点$x$的异常概率：
$$
s(x,m) = 2^{-\frac{h(x)}{c(m)}}
$$

其中，$m$为样本个数。$c(m)$的表达式为：
$$
c(m) =2\ln(m-1) + \xi - 2\frac{m-1}{m}, \; \xi为欧拉常数
$$

$s(x,m)$的取值范围是[0,1], 取值越接近于1，则是异常点的概率也越大。



# 随机森林小结

RF的算法原理也终于讲完了，作为一个可以高度并行化的算法，RF在大数据时候大有可为。 这里也对常规的随机森林算法的优缺点做一个总结。

> RF的主要优点有：

 1） 训练可以**高度并行化**，对于大数据时代的大样本训练速度有优势。个人觉得这是的最主要的优点。

 2） 由于可以随机选择决策树节点划分特征，这样**在样本特征维度很高的时候，仍然能高效的训练模型**。

 3） 在训练后，可以给出各个**特征对于输出的重要性**。

 4） 由于采用了随机采样，训练出的模型的**方差小，泛化能力强**。

 5） 相对于Boosting系列的Adaboost和GBDT， **RF实现比较简单**。

 6） 对部分特征**缺失不敏感**。

> RF的主要缺点有：
>
> 1. 在某些噪音比较大的样本集上，RF模型容易陷入过拟合。
>
>   2)  取值划分比较多的特征容易对RF的决策产生更大的影响，从而影响拟合的模型的效果。



# scikit-learn随机森林调参

 从实践的角度对RF做一个总结。重点讲述scikit-learn中RF的调参注意事项，以及和GBDT调参的异同点。

## scikit-learn随机森林类库概述

在scikit-learn中，RF的分类类是RandomForestClassifier，回归类是RandomForestRegressor。当然RF的变种Extra Trees也有， 分类类ExtraTreesClassifier，回归类ExtraTreesRegressor。由于RF和Extra Trees的区别较小，调参方法基本相同，本文只关注于RF的调参。

和GBDT的调参类似，RF需要调参的参数也包括两部分，第一部分是Bagging框架的参数，第二部分是CART决策树的参数。下面我们就对这些参数做一个介绍。

## RF框架参数

首先我们关注于RF的Bagging框架的参数。这里可以和GBDT对比来学习。在scikit-learn梯度提升树(GBDT)调参小结中我们对GBDT的框架参数做了介绍。GBDT的框架参数比较多，重要的有最大迭代器个数，步长和子采样比例，调参起来比较费力。但是RF则比较简单，这是因为bagging框架里的各个弱学习器之间是没有依赖关系的，这减小的调参的难度。换句话说，达到同样的调参效果，RF调参时间要比GBDT少一些。

下面我来看看RF重要的Bagging框架的参数，由于RandomForestClassifier和RandomForestRegressor参数绝大部分相同，这里会将它们一起讲，不同点会指出。

1. **n_estimators**: 也就是弱学习器的最大迭代次数，或者说最大的**弱学习器的个数**。一般来说n_estimators太小，容易欠拟合，n_estimators太大，计算量会太大，并且n_estimators到一定的数量后，再增大n_estimators获得的模型提升会很小，所以一般选择一个适中的数值。默认是100。
2. **oob_score **: 即是否采用**袋外样本来评估模型的好坏**。默认识False。个人推荐设置为True，因为袋外分数反应了一个模型拟合后的泛化能力。
3. **criterion**: 即CART树做划分时对特征的评价标准。分类模型和回归模型的损失函数是不一样的。
	- **分类RF**对应的CART分类树默认是基尼系数gini, 另一个可选择的标准是信息增益。
	- **回归RF**对应的CART回归树默认是均方差mse，另一个可以选择的标准是绝对值差mae。一般来说选择默认的标准就已经很好的。

 从上面可以看出， RF重要的框架参数比较少，主要需要关注的是 n_estimators，即RF最大的决策树个数。



## RF决策树参数

 下面我们再来看RF的决策树参数，它要调参的参数基本和GBDT相同，如下:

1. **max_features**: RF划分时考虑的**最大特征数**
	- 默认是**"auto"**,意味着划分时最多考虑$\sqrt{N}$个特征；
	- 如果是**"log2"**,意味着划分时最多考虑$log2N$个特征；
	- 如果是**"sqrt"**或者**"auto"**,意味着划分时最多考虑$\sqrt{N}$个特征。
	- 如果是**整数**，代表考虑的特征绝对数。
	- 如果是**浮点数**，代表考虑特征百分比，即考虑（百分比xN）取整后的特征数。其中N为样本总特征数。
	- 一般我们用默认的"auto"就可以了，如果特征数非常多，我们可以灵活使用刚才描述的其他取值来控制划分时考虑的最大特征数，以控制决策树的生成时间。
2. **max_depth**: 决策树**最大深度**
默认可以不输入，如果不输入的话，决策树在建立子树的时候不会限制子树的深度。**一般来说，数据少或者特征少的时候可以不管这个值。如果模型样本量多，特征也多的情况下，推荐限制这个最大深度，**具体的取值取决于数据的分布。常用的可以取值10-100之间。
3. **min_samples_split**: 内部节点再划分所需**最小样本数**  
 这个值限制了子树继续划分的条件，如果某节点的样本数少于min_samples_split，则不会继续再尝试选择最优特征来进行划分。 默认是2, **如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。**
4. **min_samples_leaf**: **叶子节点最少样本数**
这个值限制了叶子节点最少的样本数，如果某叶子节点数目小于样本数，则会和兄弟节点一起**被剪枝**。 默认是1,可以输入最少的样本数的整数，或者最少样本数占样本总数的百分比。**如果样本量不大，不需要管这个值。如果样本量数量级非常大，则推荐增大这个值。**
5. **min_weight_fraction_leaf**：**叶子节点最小的样本权重和**
这个值限制了叶子节点所有样本权重和的最小值，如果小于这个值，则会和兄弟节点一起被剪枝。 默认是0，就是不考虑权重问题。**一般来说，如果我们有较多样本有缺失值，或者分类树样本的分布类别偏差很大，就会引入样本权重，这时我们就要注意这个值了。**
6. **max_leaf_nodes**: **最大叶子节点数**
通过限制最大叶子节点数，可以防止过拟合，默认是"None"，即不限制最大的叶子节点数。如果加了限制，算法会建立在最大叶子节点数内最优的决策树。如果特征不多，可以不考虑这个值，但是**如果特征分成多的话，可以加以限制**，具体的值可以通过交叉验证得到。
7. **min_impurity_split**: **节点划分最小不纯度**  

这个值限制了决策树的增长，如果某节点的不纯度(基于基尼系数，均方差)小于这个阈值，则该节点不再生成子节点。即为叶子节点 。一般不推荐改动默认值1e-7。

上面决策树参数中最重要的包括**最大特征数max_features**， **最大深度max_depth**， **内部节点再划分所需最小样本数**min_samples_split和**叶子节点最少样本数**min_samples_leaf。



# RF调参实例


```python
import pandas as pd
import numpy as np
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import GridSearchCV
from sklearn import metrics

import matplotlib.pylab as plt
%matplotlib inline
```


```python
train = pd.read_csv('./train_modified.csv')
target = 'Disbursed'  # Disbursed的值就是二元分类的输出
IDcol = 'ID'
train['Disbursed'].value_counts()
```




    0    19680
    1      320
    Name: Disbursed, dtype: int64



可以看到类别输出如下，也就是类别0的占大多数。


```python
# 接着我们选择好样本特征和类别输出。

x_columns = [x for x in train.columns if x not in [target, IDcol]]
X = train[x_columns]
y = train['Disbursed']
```


```python
# 不管任何参数，都用默认的，我们拟合下数据看看：

rf0 = RandomForestClassifier(oob_score=True, random_state=10)
rf0.fit(X, y)
```



```python
RandomForestClassifier(
    bootstrap=True, 
    class_weight=None, 
    criterion='gini',                
    max_depth=None, 
    max_features='auto', 
    max_leaf_nodes=None,
    min_impurity_decrease=0.0, 
    min_impurity_split=None,
    min_samples_leaf=1, 
    min_samples_split=2,
    min_weight_fraction_leaf=0.0, 
    n_estimators=10, 
    n_jobs=None,
    oob_score=True, 
    random_state=10, 
    verbose=0, 
    warm_start=False)
```




```python
print(rf0.oob_score_)
y_predprob = rf0.predict_proba(X)[:, 1]
print("AUC Score (Train): %f" % metrics.roc_auc_score(y, y_predprob))
```

    0.98005
    AUC Score (Train): 0.999833


可见袋外分数已经很高，而且AUC分数也很高。相对于GBDT的默认参数输出，RF的默认参数拟合效果对本例要好一些。
***

首先对**n_estimators**进行网格搜索：


```python

param_test1 = {'n_estimators': range(10, 71, 10)}
gsearch1 = GridSearchCV(
    estimator=RandomForestClassifier(min_samples_split=100,
                                     min_samples_leaf=20,
                                     max_depth=8,
                                     max_features='sqrt',
                                     random_state=10),
    param_grid=param_test1,
    scoring='roc_auc',
    cv=5)

gsearch1.fit(X, y)
```




```python
GridSearchCV(cv=5, error_score='raise-deprecating',
       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=8, max_features='sqrt', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=20, min_samples_split=100,
            min_weight_fraction_leaf=0.0, n_estimators='warn', n_jobs=None,
            oob_score=False, random_state=10, verbose=0, warm_start=False),
       fit_params=None, iid='warn', n_jobs=None,
       param_grid={'n_estimators': range(10, 71, 10)},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring='roc_auc', verbose=0)
```




```python
gsearch1.cv_results_['mean_test_score']
```


    array([0.80680934, 0.81600252, 0.81818272, 0.81838438, 0.82034069,
           0.82113345, 0.8199191 ])




```python
gsearch1.cv_results_['params']
```


    [{'n_estimators': 10},
     {'n_estimators': 20},
     {'n_estimators': 30},
     {'n_estimators': 40},
     {'n_estimators': 50},
     {'n_estimators': 60},
     {'n_estimators': 70}]




```python
gsearch1.best_params_
```


    {'n_estimators': 60}




```python
gsearch1.best_score_
```


    0.8211334476626017



这样我们得到了最佳的弱学习器迭代次数，接着我们对决策树最大深度**max_depth**和内部节点再划分所需最小样本数**min_samples_split**进行网格搜索。


```python
param_test2 = {
    'max_depth': range(3, 14, 2),
    'min_samples_split': range(50, 201, 20)
}
gsearch2 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=60,
                                                         min_samples_leaf=20,
                                                         max_features='sqrt',
                                                         oob_score=True,
                                                         random_state=10),
                        param_grid=param_test2,
                        scoring='roc_auc',
                        iid=False,
                        cv=5)
gsearch2.fit(X, y)
```




```python
GridSearchCV(
    cv=5, 
    error_score='raise-deprecating',
    estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=None, max_features='sqrt', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=20, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=None,
            oob_score=True, random_state=10, verbose=0, warm_start=False),
       fit_params=None, iid=False, n_jobs=None,
       param_grid={'max_depth': range(3, 14, 2), 'min_samples_split': range(50, 201, 20)},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring='roc_auc', verbose=0)
```




```python
gsearch2.cv_results_['mean_test_score']
```


    array([0.79379248, 0.79338637, 0.79350308, 0.79366624, 0.79387425,
           0.79372817, 0.7937758 , 0.79349474, 0.80960366, 0.80919874,
           0.80887759, 0.80922971, 0.80822853, 0.80801019, 0.80792286,
           0.80771167, 0.81687627, 0.81872459, 0.81501127, 0.81475522,
           0.81557061, 0.81458651, 0.81601007, 0.81703824, 0.82090439,
           0.81907989, 0.82035736, 0.81889291, 0.81991314, 0.81787625,
           0.81897588, 0.81745943, 0.82395238, 0.82380272, 0.81952728,
           0.82253557, 0.81950267, 0.8188687 , 0.81910371, 0.81563969,
           0.82290754, 0.82176623, 0.82415365, 0.82420168, 0.8220854 ,
           0.81852491, 0.81954594, 0.82092345])




```python
gsearch2.cv_results_['params']
```


    [{'max_depth': 3, 'min_samples_split': 50},
     {'max_depth': 3, 'min_samples_split': 70},
     {'max_depth': 3, 'min_samples_split': 90},
     {'max_depth': 3, 'min_samples_split': 110},
     {'max_depth': 3, 'min_samples_split': 130},
     {'max_depth': 3, 'min_samples_split': 150},
     {'max_depth': 3, 'min_samples_split': 170},
     {'max_depth': 3, 'min_samples_split': 190},
     {'max_depth': 5, 'min_samples_split': 50},
     {'max_depth': 5, 'min_samples_split': 70},
     {'max_depth': 5, 'min_samples_split': 90},
     {'max_depth': 5, 'min_samples_split': 110},
     {'max_depth': 5, 'min_samples_split': 130},
     {'max_depth': 5, 'min_samples_split': 150},
     {'max_depth': 5, 'min_samples_split': 170},
     {'max_depth': 5, 'min_samples_split': 190},
     {'max_depth': 7, 'min_samples_split': 50},
     {'max_depth': 7, 'min_samples_split': 70},
     {'max_depth': 7, 'min_samples_split': 90},
     {'max_depth': 7, 'min_samples_split': 110},
     {'max_depth': 7, 'min_samples_split': 130},
     {'max_depth': 7, 'min_samples_split': 150},
     {'max_depth': 7, 'min_samples_split': 170},
     {'max_depth': 7, 'min_samples_split': 190},
     {'max_depth': 9, 'min_samples_split': 50},
     {'max_depth': 9, 'min_samples_split': 70},
     {'max_depth': 9, 'min_samples_split': 90},
     {'max_depth': 9, 'min_samples_split': 110},
     {'max_depth': 9, 'min_samples_split': 130},
     {'max_depth': 9, 'min_samples_split': 150},
     {'max_depth': 9, 'min_samples_split': 170},
     {'max_depth': 9, 'min_samples_split': 190},
     {'max_depth': 11, 'min_samples_split': 50},
     {'max_depth': 11, 'min_samples_split': 70},
     {'max_depth': 11, 'min_samples_split': 90},
     {'max_depth': 11, 'min_samples_split': 110},
     {'max_depth': 11, 'min_samples_split': 130},
     {'max_depth': 11, 'min_samples_split': 150},
     {'max_depth': 11, 'min_samples_split': 170},
     {'max_depth': 11, 'min_samples_split': 190},
     {'max_depth': 13, 'min_samples_split': 50},
     {'max_depth': 13, 'min_samples_split': 70},
     {'max_depth': 13, 'min_samples_split': 90},
     {'max_depth': 13, 'min_samples_split': 110},
     {'max_depth': 13, 'min_samples_split': 130},
     {'max_depth': 13, 'min_samples_split': 150},
     {'max_depth': 13, 'min_samples_split': 170},
     {'max_depth': 13, 'min_samples_split': 190}]




```python
gsearch2.best_params_
```


    {'max_depth': 13, 'min_samples_split': 110}




```python
gsearch2.best_score_
```


    0.8242016800050813



我们看看我们现在模型的袋外分数：


```python
rf1 = RandomForestClassifier(n_estimators=60,
                             max_depth=13,
                             min_samples_split=110,
                             min_samples_leaf=20,
                             max_features='sqrt',
                             oob_score=True,
                             random_state=10)

rf1.fit(X, y)
print(rf1.oob_score_)
```

    0.984


可见此时我们的袋外分数有一定的提高。也就是时候模型的泛化能力增强了。

对于内部节点**再划分所需最小样本数**min_samples_split，我们暂时不能一起定下来，因为这个还和决策树其他的参数存在关联。下面我们再对内部节点**再划分所需最小样本数**min_samples_split和**叶子节点最少样本数**min_samples_leaf一起调参。


```python
param_test3 = {
    'min_samples_split': range(80, 150, 20),
    'min_samples_leaf': range(10, 60, 10)
}

gsearch3 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=60,
                                                         max_depth=13,
                                                         max_features='sqrt',
                                                         oob_score=True,
                                                         random_state=10),
                        param_grid=param_test3,
                        scoring='roc_auc',
                        iid=False,
                        cv=5)
gsearch3.fit(X, y)
```




```python
GridSearchCV(cv=5, error_score='raise-deprecating',
       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=13, max_features='sqrt', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=1, min_samples_split=2,
            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=None,
            oob_score=True, random_state=10, verbose=0, warm_start=False),
       fit_params=None, iid=False, n_jobs=None,
       param_grid={'min_samples_split': range(80, 150, 20), 'min_samples_leaf': range(10, 60, 10)},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring='roc_auc', verbose=0)
```




```python
gsearch3.cv_results_['mean_test_score']
```


    array([0.8209294 , 0.81913348, 0.82048399, 0.8179751 , 0.8209429 ,
           0.82097426, 0.82486503, 0.82169239, 0.82352087, 0.82164475,
           0.82069876, 0.82141332, 0.82278249, 0.82141411, 0.82042881,
           0.82162093, 0.82224975, 0.82224975, 0.81890403, 0.81916643])




```python
gsearch3.cv_results_['params']
```


    [{'min_samples_leaf': 10, 'min_samples_split': 80},
     {'min_samples_leaf': 10, 'min_samples_split': 100},
     {'min_samples_leaf': 10, 'min_samples_split': 120},
     {'min_samples_leaf': 10, 'min_samples_split': 140},
     {'min_samples_leaf': 20, 'min_samples_split': 80},
     {'min_samples_leaf': 20, 'min_samples_split': 100},
     {'min_samples_leaf': 20, 'min_samples_split': 120},
     {'min_samples_leaf': 20, 'min_samples_split': 140},
     {'min_samples_leaf': 30, 'min_samples_split': 80},
     {'min_samples_leaf': 30, 'min_samples_split': 100},
     {'min_samples_leaf': 30, 'min_samples_split': 120},
     {'min_samples_leaf': 30, 'min_samples_split': 140},
     {'min_samples_leaf': 40, 'min_samples_split': 80},
     {'min_samples_leaf': 40, 'min_samples_split': 100},
     {'min_samples_leaf': 40, 'min_samples_split': 120},
     {'min_samples_leaf': 40, 'min_samples_split': 140},
     {'min_samples_leaf': 50, 'min_samples_split': 80},
     {'min_samples_leaf': 50, 'min_samples_split': 100},
     {'min_samples_leaf': 50, 'min_samples_split': 120},
     {'min_samples_leaf': 50, 'min_samples_split': 140}]




```python
gsearch3.best_params_
```


    {'min_samples_leaf': 20, 'min_samples_split': 120}




```python
gsearch3.best_score_
```


    0.8248650279471545



***

最后我们再对**最大特征数max_features**做调参:


```python
param_test4 = {'max_features': range(3, 11, 2)}
gsearch4 = GridSearchCV(estimator=RandomForestClassifier(n_estimators=60,
                                                         max_depth=13,
                                                         min_samples_split=120,
                                                         min_samples_leaf=20,
                                                         oob_score=True,
                                                         random_state=10),
                        param_grid=param_test4,
                        scoring='roc_auc',
                        iid=False,
                        cv=5)
gsearch4.fit(X, y)
```




```python
GridSearchCV(cv=5, error_score='raise-deprecating',
       estimator=RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
            max_depth=13, max_features='auto', max_leaf_nodes=None,
            min_impurity_decrease=0.0, min_impurity_split=None,
            min_samples_leaf=20, min_samples_split=120,
            min_weight_fraction_leaf=0.0, n_estimators=60, n_jobs=None,
            oob_score=True, random_state=10, verbose=0, warm_start=False),
       fit_params=None, iid=False, n_jobs=None,
       param_grid={'max_features': range(3, 11, 2)},
       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',
       scoring='roc_auc', verbose=0)
```




```python
gsearch4.cv_results_['mean_test_score']
```


    array([0.81981191, 0.8163868 , 0.82486503, 0.81703506])




```python
gsearch4.cv_results_['params']
```


    [{'max_features': 3},
     {'max_features': 5},
     {'max_features': 7},
     {'max_features': 9}]




```python
gsearch4.best_params_
```


    {'max_features': 7}




```python
gsearch4.best_score_
```


    0.8248650279471545



用我们搜索到的最佳参数，我们再看看最终的模型拟合：


```python
rf2 = RandomForestClassifier(n_estimators=60,
                             max_depth=13,
                             min_samples_split=120,
                             min_samples_leaf=20,
                             max_features=7,
                             oob_score=True,
                             random_state=10)
rf2.fit(X, y)
print(rf2.oob_score_)
```

    0.984


可见此时模型的袋外分数基本没有提高，主要原因是0.984已经是一个很高的袋外分数了，如果想进一步需要提高模型的泛化能力，我们需要更多的数据。



# 试题

1. 随机森林如何处理缺失值。

**方法一**（na.roughfix）简单粗暴，对于训练集,同一个class下的数据，如果是分类变量缺失，用众数补上，如果是连续型变量缺失，用中位数补。

**方法二**（rfImpute）这个方法计算量大，至于比方法一好坏？不好判断。先用na.roughfix补上缺失值，然后构建森林并计算proximity matrix，再回头看缺失值，如果是分类变量，则用没有阵进行加权平均的方法补缺失值。然后迭代4-6次，这个补缺失值的思想和KNN有些类似1缺失的观测实例的proximity中的权重进行投票。如果是连续型变量，则用proximity矩2。
2. 随机森林如何评估特征重要性。

衡量变量重要性的方法有两种，Decrease GINI 和 Decrease Accuracy：

1) **Decrease GINI**： 对于回归问题，直接使用argmax(Var-VarLeft-VarRight)作为评判标准，**即当前节点训练集的方差Var减去左节点的方差VarLeft和右节点的方差VarRight**。

2) **Decrease Accuracy**：对于一棵树Tb(x)，我们用OOB样本可以得到测试误差1；然后随机改变OOB样本的第j列：保持其他列不变，对第j列进行随机的上下置换，得到误差2。至此，我们可以用误差1-误差2来刻画变量j的重要性。**基本思想就是，如果一个变量j足够重要，那么改变它会极大的增加测试误差；反之，如果改变它测试误差没有增大，则说明该变量不是那么的重要**。

3. RF与GBDT之间的区别与联系？

1）**相同点**：都是由多棵树组成，最终的结果都是由多棵树一起决定。

2）**不同点**：

- 组成随机森林的树可以分类树也可以是回归树，而**GBDT只由回归树组成**。
- 组成随机森林的树可以**并行生成**，而GBDT是**串行生成**。
- 随机森林的结果是**多棵树表决**的，而GBDT则是**多棵树累加之和**。
- 随机森林对**异常值不敏感**，而GBDT对**异常值比较敏感**。
- 随机森林是**减少模型的方差**，而GBDT是**减少模型的偏差**。
- 随机森林**不需要进行特征归一化**，而GBDT则**需要进行特征归一化**。