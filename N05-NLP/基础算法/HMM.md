# 简述

使用HMM模型时我们的问题一般有这**两个特征**：

1. 问题是**基于序列**的，比如时间序列，或者状态序列。
2. 问题中有两类数据，一类序列数据是可以观测到的，即**观测序列**；而另一类数据是不能观察到的，即隐藏状态序列，简称**状态序列**。

HMM模型做了两个很重要的**假设**如下： 

1. `齐次马尔科夫链假设`。**即任意时刻的隐藏状态只依赖于它前一个隐藏状态**。
2. `观测独立性假设`。**即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设**。

一个HMM模型，可以由**隐藏状态初始概率分布$\Pi$**, **状态转移概率矩阵$A$**和**观测状态概率矩阵$B$**决定。$\Pi$，$A$决定状态序列，$B$决定观测序列。因此，HMM模型可以由一个三元组$\lambda$表示如下：
$$
\lambda = (A, B, \Pi)
$$

HMM模型一共有**三个经典的问题**需要解决：

1. **评估观察序列概率**。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。这个问题的求解需要用到**前向后向算法**，我们在这个系列的第二篇会详细讲解。这个问题是HMM模型三个问题中最简单的。
2. **模型参数估计问题**。即给定观测序列$O =\{o_1,o_2,...o_T\}$，估计模型$\lambda = (A, B, \Pi)$的参数，使该模型下观测序列的条件概率$P(O|\lambda)$最大。这个问题的求解需要用到基于EM算法的**鲍姆-韦尔奇算法**， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。
3. **预测问题，也称为解码问题**。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的**维特比算法**，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。



# HMM模型基础

隐马尔科夫模型（Hidden Markov Model，以下简称HMM）是比较经典的机器学习模型了，它在语言识别，自然语言处理，模式识别等领域得到广泛的应用。当然，随着目前深度学习的崛起，尤其是RNN，LSTM等神经网络序列模型的火热，HMM的地位有所下降。但是作为一个经典的模型，学习HMM的模型和对应算法，对我们解决问题建模的能力提高以及算法思路的拓展还是很好的。

## 什么样的问题需要HMM模型

首先我们来看看什么样的问题解决可以用HMM模型。使用HMM模型时我们的问题一般有这两个特征：

1. 我们的问题是**基于序列**的，比如时间序列，或者状态序列。
2. 我们的问题中有两类数据，一类序列数据是可以观测到的，即**观测序列**；而另一类数据是不能观察到的，即隐藏状态序列，简称**状态序列**。

有了这两个特征，那么这个问题一般可以用HMM模型来尝试解决。这样的问题在实际生活中是很多的。

**比如**：我现在在打字写博客，我在键盘上敲出来的一系列字符就是观测序列，而我实际想写的一段话就是隐藏序列，输入法的任务就是从敲入的一系列字符尽可能的猜测我要写的一段话，并把最可能的词语放在最前面让我选择，这就可以看做一个HMM模型了。

再举一个，我在和你说话，我发出的一串连续的声音就是观测序列，而我实际要表达的一段话就是状态序列，你大脑的任务，就是从这一串连续的声音中判断出我最可能要表达的话的内容。

从这些例子中，我们可以发现，HMM模型可以无处不在。但是上面的描述还不精确，下面我们用精确的数学符号来表述我们的HMM模型。

## HMM模型的定义

对于HMM模型，首先我们假设$Q$是所有可能的**隐藏状态**的集合，$V$是所有可能的**观测状态**的集合，即：
$$
Q = \{q_1,q_2,...,q_N\}, \; V =\{v_1,v_2,...v_M\}
$$

其中，$N$是可能的**隐藏状态数**，$M$是所有的可能的**观察状态数**。

对于一个长度为$T$的序列，$I$对应的状态序列, $O$是对应的观察序列，即：
$$
I = \{i_1,i_2,...,i_T\}, \; O =\{o_1,o_2,...o_T\}
$$

其中，任意一个**隐藏状态$i_t \in Q$**，任意一个**观察状态$o_t \in V$**。



HMM模型做了两个很重要的假设如下： 

`1） 齐次马尔科夫链假设`。**即任意时刻的隐藏状态只依赖于它前一个隐藏状态**。当然这样假设有点极端，因为很多时候我们的某一个隐藏状态不仅仅只依赖于前一个隐藏状态，可能是前两个或者是前三个。但是这样假设的好处就是模型简单，便于求解。如果在时刻$t$的隐藏状态是$i_t= q_i$，在时刻$t+1$的隐藏状态是$i_{t+1} = q_j$，则从时刻$t$到时刻$t+1$的HMM**状态转移概率$a_{ij}$**可以表示为：
$$
a_{ij} = P(i_{t+1} = q_j | i_t= q_i)
$$

这样$a_{ij}$可以组成马尔科夫链的**状态转移矩阵$A$**：
$$
A=\Big [a_{ij}\Big ]_{N \times N}
$$
`2） 观测独立性假设`。**即任意时刻的观察状态只仅仅依赖于当前时刻的隐藏状态，这也是一个为了简化模型的假设**。如果在时刻$t$的隐藏状态是$i_t= q_j$, 而对应的观察状态为$o_t = v_k$, 则该时刻观察状态$v_k$在隐藏状态$q_j$下生成的概率为$b_j(k)$，满足：
$$
b_j(k) = P(o_t = v_k | i_t= q_j)
$$

这样$b_j(k)$可以组成观测状态生成的**概率矩阵$B$**：
$$
B = \Big [b_j(k) \Big ]_{N \times M}
$$

除此之外，我们需要一组在时刻$t=1$的隐藏状态**概率分布$\Pi$**:
$$
\Pi = \Big [ \pi(i)\Big ]_N \; 其中 \;\pi(i) = P(i_1 = q_i)
$$

一个HMM模型，可以由**隐藏状态初始概率分布$\Pi$**, **状态转移概率矩阵$A$**和**观测状态概率矩阵$B$**决定。$\Pi$,$A$决定状态序列，$B$决定观测序列。因此，HMM模型可以由一个三元组$\lambda$表示如下：
$$
\lambda = (A, B, \Pi)
$$

## HMM模型实例

下面我们用一个简单的实例来描述上面抽象出的HMM模型。这是一个盒子与球的模型，例子来源于李航的《统计学习方法》。

假设我们有3个盒子，每个盒子里都有红色和白色两种球，这三个盒子里球的数量分别是：

盒子|1|2|3
:-:|:-:|:-:|:-:
红球数|5|4|7
白球数|5|6|3

按照下面的方法从盒子里抽球，开始的时候，从第一个盒子抽球的概率是0.2，从第二个盒子抽球的概率是0.4，从第三个盒子抽球的概率是0.4。以这个概率抽一次球后，将球放回。然后从当前盒子转移到下一个盒子进行抽球。规则是：

如果当前抽球的盒子是第一个盒子，则以0.5的概率仍然留在第一个盒子继续抽球，以0.2的概率去第二个盒子抽球，以0.3的概率去第三个盒子抽球。

如果当前抽球的盒子是第二个盒子，则以0.5的概率仍然留在第二个盒子继续抽球，以0.3的概率去第一个盒子抽球，以0.2的概率去第三个盒子抽球。

如果当前抽球的盒子是第三个盒子，则以0.5的概率仍然留在第三个盒子继续抽球，以0.2的概率去第一个盒子抽球，以0.3的概率去第二个盒子抽球。

如此下去，直到重复三次，得到一个球的颜色的观测序列:
$$
O=\{红，白，红\}
$$
注意在这个过程中，观察者只能看到球的颜色序列，却不能看到球是从哪个盒子里取出的。

那么按照我们上一节HMM模型的定义，我们的观察集合是:
$$
V=\{红，白\}，M=2
$$


我们的状态集合是：
$$
Q =\{盒子1，盒子2，盒子3\}， N=3
$$


而观察序列和状态序列的长度为3。

初始状态分布为：
$$
\Pi = (0.2,0.4,0.4)^T
$$

状态转移概率分布矩阵为：
$$
A = \left( \begin{array} {ccc} 0.5 & 0.2 & 0.3 \\ 0.3 & 0.5 & 0.2 \\ 0.2 & 0.3 &0.5 \end{array} \right)
$$


 观测状态概率矩阵为：
$$
B = \left( \begin{array} {ccc} 0.5 & 0.5 \\ 0.4 & 0.6 \\ 0.7 & 0.3 \end{array} \right)
$$

## HMM观测序列的生成

从上一节的例子，我们也可以抽象出HMM观测序列生成的过程。

- 输入的是HMM的模型$\lambda = (A, B, \Pi)$，观测序列的长度$T$ 
- 输出是观测序列$O =\{o_1,o_2,...o_T\}$ 

生成的过程如下： 

1）根据初始状态概率分布$\Pi$生成隐藏状态$i_1$ 

2）$for$  t :   $from$  1  $to$  $T$   

​		a. 按照隐藏状态$i_t$的观测状态分布$b_{i_t}(k)$生成观察状态$o_t$ 

​		b. 按照隐藏状态$i_t$的状态转移概率分布$a_{{i_t}{i_{t+1}}}$产生隐藏状态$i_{t+1}$ 

所有的$o_t$一起形成观测序列$O =\{o_1,o_2,...o_T\}$

## HMM模型的三个基本问题

HMM模型一共有三个经典的问题需要解决：

1. **评估观察序列概率**。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，计算在模型$\lambda$下观测序列$O$出现的概率$P(O|\lambda)$。这个问题的求解需要用到**前向后向算法**，我们在这个系列的第二篇会详细讲解。这个问题是HMM模型三个问题中最简单的。
2. **模型参数估计问题**。即给定观测序列$O =\{o_1,o_2,...o_T\}$，估计模型$\lambda = (A, B, \Pi)$的参数，使该模型下观测序列的条件概率$P(O|\lambda)$最大。这个问题的求解需要用到基于EM算法的**鲍姆-韦尔奇算法**， 我们在这个系列的第三篇会详细讲解。这个问题是HMM模型三个问题中最复杂的。
3. **预测问题，也称为解码问题**。即给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，求给定观测序列条件下，最可能出现的对应的状态序列，这个问题的求解需要用到基于动态规划的**维特比算法**，我们在这个系列的第四篇会详细讲解。这个问题是HMM模型三个问题中复杂度居中的算法。

# 前向后向算法评估观察序列概率

我们讲到了HMM模型的基础知识和HMM的三个基本问题，本篇我们就关注于HMM第一个基本问题的解决方法，即已知模型和观测序列，求观测序列出现的概率。

## 回顾HMM问题一：求观测序列的概率

首先我们回顾下HMM模型的问题一。这个问题是这样的。我们已知HMM模型的参数$\lambda = (A, B, \Pi)$。其中$A$是隐藏状态转移概率的矩阵，$B$是观测状态生成概率的矩阵， $\Pi$是隐藏状态的初始概率分布。同时我们也已经得到了观测序列$O =\{o_1,o_2,...o_T\}$,现在我们要求观测序列$O$在模型$\lambda$下出现的条件概率$P(O|\lambda)$。

乍一看，这个问题很简单。因为我们知道所有的隐藏状态之间的转移概率和所有从隐藏状态到观测状态生成概率，那么我们是可以暴力求解的。

我们可以列举出所有可能出现的长度为$T$的隐藏序列$I = \{i_1,i_2,...,i_T\}$,分布求出这些隐藏序列与观测序列$O =\{o_1,o_2,...o_T\}$的联合概率分布$P(O,I|\lambda)$，这样我们就可以很容易的求出边缘分布$P(O|\lambda)$了。

具体暴力求解的方法是这样的：首先，任意一个隐藏序列$I = \{i_1,i_2,...,i_T\}$出现的概率是：
$$
P(I|\lambda) = \pi_{i_1} a_{i_1i_2} a_{i_2i_3}... a_{i_{T-1}{i_T}}
$$

对于固定的状态序列$I = \{i_1,i_2,...,i_T\}$，我们要求的观察序列$O =\{o_1,o_2,...o_T\}$出现的概率是：
$$
P(O|I, \lambda) = b_{i_1}(o_1)b_{i_2}(o_2)...b_{i_T}(o_T)
$$

则$O$和$I$联合出现的概率是：
$$
P(O,I|\lambda) = P(I|\lambda)P(O|I, \lambda) = \pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}{i_T}}b_{i_T}(o_T)
$$

然后求边缘概率分布，即可得到观测序列$O$在模型$λ$下出现的条件概率$P(O|\lambda)$：
$$
P(O|\lambda) = \sum\limits_{I}P(O,I|\lambda)  = \sum\limits_{i_1,i_2,...i_T}\pi_{i_1}b_{i_1}(o_1)a_{i_1i_2}b_{i_2}(o_2)...a_{i_{T-1}{i_T}}b_{i_T}(o_T)
$$

虽然上述方法有效，但是如果我们的隐藏状态数$N$非常多的那就麻烦了，此时我们预测状态有$N^T$种组合，算法的时间复杂度是$O(TN^T)$阶的。因此对于一些隐藏状态数极少的模型，我们可以用暴力求解法来得到观测序列出现的概率，但是如果隐藏状态多，则上述算法太耗时，我们需要寻找其他简洁的算法。

前向后向算法就是来帮助我们在较低的时间复杂度情况下求解这个问题的。



## 用前向算法求HMM观测序列的概率

前向后向算法是前向算法和后向算法的统称，这两个算法都可以用来求HMM观测序列的概率。我们先来看看前向算法是如何求解这个问题的。

前向算法本质上属于动态规划的算法，也就是我们要通过找到局部状态递推的公式，这样一步步的从子问题的最优解拓展到整个问题的最优解。

在前向算法中，通过定义**前向概率**来定义动态规划的这个局部状态。什么是前向概率呢, 其实定义很简单：**定义时刻$t$时隐藏状态为$q_i$, 观测状态的序列为$o_1,o_2,...o_t$的概率为前向概率**。记为：
$$
\alpha_t(i) = P(o_1,o_2,...o_t, i_t =q_i | \lambda)
$$

既然是动态规划，我们就要递推了，现在我们假设我们已经找到了在时刻$t$时各个隐藏状态的前向概率，现在我们需要递推出时刻$t+1$时各个隐藏状态的前向概率。

从下图可以看出，我们可以基于时刻$t$时各个隐藏状态的前向概率，再乘以对应的状态转移概率，即$\alpha_t(j)a_{ji}$就是在时刻$t$观测到$o_1,o_2,...o_t$，并且时刻$t$隐藏状态$q_j$, 时刻$t+1$隐藏状态$q_i$的概率。

如果将想下面所有的线对应的概率求和，即$\sum\limits_{j=1}^N\alpha_t(j)a_{ji}$就是在时刻$t$观测到$o_1,o_2,...o_t$，并且时刻$t+1$隐藏状态$q_i$的概率。继续一步，由于观测状态$o_{t+1}$只依赖于$t+1$时刻隐藏状态$q_i$, 这样$[\sum\limits_{j=1}^N\alpha_t(j)a_{ji}]b_i(o_{t+1})$就是在在时刻$t+1$观测到$o_1,o_2,...o_t，o_{t+1}$，并且时刻$t+1$隐藏状态$q_i$的概率。而这个概率，恰恰就是时刻$t+1$对应的隐藏状态$i$的前向概率，这样我们得到了前向概率的递推关系式如下：
$$
\alpha_{t+1}(i) = \Big[\sum\limits_{j=1}^N\alpha_t(j)a_{ji}\Big]b_i(o_{t+1})
$$

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/HMM-20201214-201033-287202.png)

我们的动态规划从时刻1开始，到时刻$T$结束，由于$\alpha_T(i)$表示在时刻$T$观测序列为$o_1,o_2,...o_T$，并且时刻$T$隐藏状态$q_i$的概率，我们只要将所有隐藏状态对应的概率相加，即$\sum\limits_{i=1}^N\alpha_T(i)$就得到了在时刻$T$观测序列为$o_1,o_2,...o_T$的概率。



下面总结下前向算法。

- 输入：HMM模型$\lambda = (A, B, \Pi)$，观测序列$O=(o_1,o_2,...o_T)$ 
- 输出：观测序列概率$P(O|\lambda)$
	　

1) 计算时刻1的各个隐藏状态前向概率：
$$
\alpha_1(i) = \pi_ib_i(o_1),\; i=1,2,...N
$$

2) 递推时刻$2,3,...T$时刻的前向概率：
$$
\alpha_{t+1}(i) = \Big[\sum\limits_{j=1}^N\alpha_t(j)a_{ji}\Big]b_i(o_{t+1}),\; i=1,2,...N
$$

3) 计算最终结果：
$$
P(O|\lambda) = \sum\limits_{i=1}^N\alpha_T(i)
$$

从递推公式可以看出，我们的算法时间复杂度是$O(TN^2)$，比暴力解法的时间复杂度$O(TN^T)$少了几个数量级。



## HMM前向算法求解实例

这里我们用隐马尔科夫模型HMM（一）HMM模型中盒子与球的例子来显示前向概率的计算。

我们的观察集合是:
$$
V=\{红，白\}，M=2
$$

我们的状态集合是：
$$
Q =\{盒子1，盒子2，盒子3\}， N=3
$$

而观察序列和状态序列的长度为3.

初始状态分布为：
$$
\Pi = (0.2,0.4,0.4)^T
$$

状态转移概率分布矩阵为：
$$
A = \left( \begin{array} {ccc} 0.5 & 0.2 & 0.3 \\ 0.3 & 0.5 & 0.2 \\ 0.2 & 0.3 &0.5 \end{array} \right)
$$

 观测状态概率矩阵为：
$$
B = \left( \begin{array} {ccc} 0.5 & 0.5 \\ 0.4 & 0.6 \\ 0.7 & 0.3 \end{array} \right)
$$

球的颜色的观测序列:
$$
O=\{红，白，红\}
$$

按照我们上一节的前向算法。首先计算时刻1三个状态的前向概率：

时刻1是红色球，隐藏状态是盒子1的概率为：
$$
\alpha_1(1) = \pi_1b_1(o_1) = 0.2 \times 0.5 = 0.1
$$

隐藏状态是盒子2的概率为：
$$
\alpha_1(2) = \pi_2b_2(o_1) = 0.4 \times 0.4 = 0.16
$$

隐藏状态是盒子3的概率为：
$$
\alpha_1(3) = \pi_3b_3(o_1) = 0.4 \times 0.7 = 0.28
$$

现在我们可以开始递推了，首先递推时刻2三个状态的前向概率：

时刻2是白色球，隐藏状态是盒子1的概率为：
$$
\alpha_2(1) =  \Big[\sum\limits_{i=1}^3\alpha_1(i)a_{i1}\Big]b_1(o_2) = [0.1*0.5+0.16*0.3+0.28*0.2 ] \times 0.5 = 0.077
$$

隐藏状态是盒子2的概率为：
$$
\alpha_2(2) =  \Big[\sum\limits_{i=1}^3\alpha_1(i)a_{i2}\Big]b_2(o_2) = [0.1*0.2+0.16*0.5+0.28*0.3 ] \times 0.6 = 0.1104
$$

隐藏状态是盒子3的概率为：
$$
\alpha_2(3) =  \Big[\sum\limits_{i=1}^3\alpha_1(i)a_{i3}\Big]b_3(o_2) = [0.1*0.3+0.16*0.2+0.28*0.5 ] \times 0.3 = 0.0606
$$

继续递推，现在我们递推时刻3三个状态的前向概率：

时刻3是红色球，隐藏状态是盒子1的概率为：
$$
\alpha_3(1) =  \Big[\sum\limits_{i=1}^3\alpha_2(i)a_{i1}\Big]b_1(o_3) = [0.077*0.5+0.1104*0.3+0.0606*0.2 ] \times 0.5 = 0.04187
$$

隐藏状态是盒子2的概率为：
$$
\alpha_3(2) =  \Big[\sum\limits_{i=1}^3\alpha_2(i)a_{i2}\Big]b_2(o_3) = [0.077*0.2+0.1104*0.5+0.0606*0.3 ] \times 0.4 = 0.03551
$$

隐藏状态是盒子3的概率为：
$$
\alpha_3(3) =  \Big[\sum\limits_{i=1}^3\alpha_3(i)a_{i3}\Big]b_3(o_3) = [0.077*0.3+0.1104*0.2+0.0606*0.5 ] \times 0.7 = 0.05284
$$

最终我们求出观测序列：$O=\{红，白，红\}$的概率为：
$$
P(O|\lambda) = \sum\limits_{i=1}^3\alpha_3(i) = 0.13022
$$



## 用后向算法求HMM观测序列的概率

熟悉了用前向算法求HMM观测序列的概率，现在我们再来看看怎么用后向算法求HMM观测序列的概率。

后向算法和前向算法非常类似，都是用的动态规划，唯一的区别是选择的局部状态不同，后向算法用的是**后向概率**，那么后向概率是如何定义的呢？

**定义时刻t时隐藏状态为$q_i$, 从时刻$t+1$到最后时刻$T$的观测状态的序列为$o_{t+1},o_{t+2},...o_T$的概率为后向概率**。记为：
$$
\beta_t(i) = P(o_{t+1},o_{t+2},...o_T| i_t =q_i , \lambda)
$$

后向概率的动态规划递推公式和前向概率是相反的。现在我们假设我们已经找到了在时刻$t+1$时各个隐藏状态的后向概率$\beta_{t+1}(j)$，现在我们需要递推出时刻$t$时各个隐藏状态的后向概率。如下图，我们可以计算出观测状态的序列为$o_{t+2},o_{t+3},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}\beta_{t+1}(j)$, 接着可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$, 时刻$t+1$隐藏状态为$q_j$的概率为$a_{ij}b_j(o_{t+1})\beta_{t+1}(j)$, 则把下面所有线对应的概率加起来，我们可以得到观测状态的序列为$o_{t+1},o_{t+2},...o_T$， $t$时隐藏状态为$q_i$的概率为$\sum\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j)$，这个概率即为时刻$t$的后向概率。
![](https://gitee.com/liuhuihe/Ehe/raw/master/images/HMM-20201214-201033-280037.png)

这样我们得到了后向概率的递推关系式如下：
$$
\beta_{t}(i) = \sum\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$

现在我们总结下后向算法的流程,注意下和前向算法的相同点和不同点：



**输入：**HMM模型$\lambda = (A, B, \Pi)$，观测序列$O=(o_1,o_2,...o_T)$ 

**输出：**观测序列概率$P(O|\lambda)$ 

1) 初始化时刻$T$的各个隐藏状态后向概率：
$$
\beta_T(i) = 1,\; i=1,2,...N
$$

2) 递推时刻$T-1,T-2,...1$时刻的后向概率：
$$
\beta_{t}(i) = \sum\limits_{j=1}^{N}a_{ij}b_j(o_{t+1})\beta_{t+1}(j),\; i=1,2,...N
$$

3) 计算最终结果：
$$
P(O|\lambda) = \sum\limits_{i=1}^N\pi_ib_i(o_1)\beta_1(i)
$$

此时我们的算法时间复杂度仍然是$O(TN^2)$。



## HMM常用概率的计算

利用前向概率和后向概率，我们可以计算出HMM中单个状态和两个状态的概率公式。

1）给定模型$\lambda$和观测序列$O$,在时刻t处于状态$q_i$的概率记为:
$$
\gamma_t(i) = P(i_t = q_i | O,\lambda) = \frac{P(i_t = q_i ,O|\lambda)}{P(O|\lambda)}
$$
利用前向概率和后向概率的定义可知：

$$
P(i_t = q_i ,O|\lambda) = \alpha_t(i)\beta_t(i)
$$
于是我们得到：
​
$$
\gamma_t(i) = \frac{ \alpha_t(i)\beta_t(i)}{\sum\limits_{j=1}^N \alpha_t(j)\beta_t(j)}
$$

2）给定模型$\lambda$和观测序列$O$,在时刻$t$处于状态$q_i$，且时刻$t+1$处于状态$q_j$的概率记为:
$$
\xi_t(i,j) = P(i_t = q_i, i_{t+1}=q_j | O,\lambda) = \frac{ P(i_t = q_i, i_{t+1}=q_j , O|\lambda)}{P(O|\lambda)}
$$

而$P(i_t = q_i, i_{t+1}=q_j , O|\lambda)$可以由前向后向概率来表示为:
$$
P(i_t = q_i, i_{t+1}=q_j , O|\lambda) = \alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)
$$

从而最终我们得到$\xi_t(i,j)$的表达式如下：
$$
\xi_t(i,j) = \frac{\alpha_t(i)a_{ij}b_j(o_{t+1})\beta_{t+1}(j)}{\sum\limits_{r=1}^N\sum\limits_{s=1}^N\alpha_t(r)a_{rs}b_s(o_{t+1})\beta_{t+1}(s)}
$$

3）将$\gamma_t(i)$和$\xi_t(i,j)$在各个时刻$t$求和，可以得到：

- 在观测序列$O$下状态$i$出现的期望值$\sum\limits_{t=1}^T\gamma_t(i)$ 
- 在观测序列$O$下由状态$i$转移的期望值$\sum\limits_{t=1}^{T-1}\gamma_t(i)$ 
- 在观测序列$O$下由状态$i$转移到状态$j$的期望值$\sum\limits_{t=1}^{T-1}\xi_t(i,j)$ 
	

上面这些常用的概率值在求解HMM问题二，即求解HMM模型参数的时候需要用到。



# 鲍姆-韦尔奇算法求解HMM参数

在本篇我们会讨论HMM模型参数求解的问题，这个问题在HMM三个问题里算是最复杂的。在研究这个问题之前，建议先阅读这个系列的前两篇以熟悉HMM模型和HMM的前向后向算法，以及EM算法原理总结，这些在本篇里会用到。在李航的《统计学习方法》中，这个算法的讲解只考虑了单个观测序列的求解，因此无法用于实际多样本观测序列的模型求解，本文关注于如何使用多个观测序列来求解HMM模型参数。

## HMM模型参数求解概述

HMM模型参数求解根据已知的条件可以分为两种情况。

第一种情况较为简单，就是我们已知$D$个长度为$T$的观测序列和对应的隐藏状态序列，即$\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\}$是已知的，此时我们可以很容易的用最大似然来求解模型参数。

假设样本从隐藏状态$q_i$转移到$q_j$的频率计数是$A_{ij}$,那么状态转移矩阵求得为：
$$
A = \Big[a_{ij}\Big], \;其中a_{ij} = \frac{A_{ij}}{\sum\limits_{s=1}^{N}A_{is}}
$$

假设样本隐藏状态为$q_j$且观测状态为$v_k$的频率计数是$B_{jk}$,那么观测状态概率矩阵为：
$$
B= \Big[b_{j}(k)\Big], \;其中b_{j}(k) = \frac{B_{jk}}{\sum\limits_{s=1}^{M}B_{js}}
$$

假设所有样本中初始隐藏状态为$q_i$的频率计数为$C(i)$,那么初始概率分布为：
$$
\Pi = \pi(i) = \frac{C(i)}{\sum\limits_{s=1}^{N}C(s)}
$$

可见第一种情况下求解模型还是很简单的。但是在很多时候，我们无法得到HMM样本观察序列对应的隐藏序列，只有$D$个长度为$T$的观测序列，即$\{(O_1), (O_2), ...(O_D)\}$是已知的，此时我们能不能求出合适的HMM模型参数呢？这就是我们的第二种情况，也是我们本文要讨论的重点。它的解法最常用的是鲍姆-韦尔奇算法，其实就是基于EM算法的求解，只不过鲍姆-韦尔奇算法出现的时代，EM算法还没有被抽象出来，所以我们本文还是说鲍姆-韦尔奇算法。

## 鲍姆-韦尔奇算法原理

鲍姆-韦尔奇算法原理既然使用的就是EM算法的原理，那么我们需要在E步求出联合分布$P(O,I|\lambda)$基于条件概率$P(I|O,\overline{\lambda})$的期望，其中$\overline{\lambda}$为当前的模型参数，然后再M步最大化这个期望，得到更新的模型参数$\lambda$。接着不停的进行EM迭代，直到模型参数的值收敛为止。

首先来看看E步，当前模型参数为$\overline{\lambda}$, 联合分布$P(O,I|\lambda)$基于条件概率$P(I|O,\overline{\lambda})$的期望表达式为：
$$
L(\lambda, \overline{\lambda}) = \sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)
$$

在M步，我们极大化上式，然后得到更新后的模型参数如下：　
$$
\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)
$$

通过不断的E步和M步的迭代，直到$\overline{\lambda}$收敛。下面我们来看看鲍姆-韦尔奇算法的推导过程。

## 鲍姆-韦尔奇算法的推导

我们的训练数据为$\{(O_1, I_1), (O_2, I_2), ...(O_D, I_D)\}$，其中任意一个观测序列$O_d = \{o_1^{(d)}, o_2^{(d)}, ... o_T^{(d)}\}$,其对应的未知的隐藏状态序列表示为：$I_d = \{i_1^{(d)}, i_2^{(d)}, ... i_T^{(d)}\}$ 

首先看鲍姆-韦尔奇算法的E步，我们需要先计算联合分布$P(O,I|\lambda)$的表达式如下：
$$
P(O,I|\lambda) = \prod_{d=1}^D\pi_{i_1^{(d)}}b_{i_1^{(d)}}(o_1^{(d)})a_{i_1^{(d)}i_2^{(d)}}b_{i_2^{(d)}}(o_2^{(d)})...a_{i_{T-1}^{(d)}i_T^{(d)}}b_{i_T^{(d)}}(o_T^{(d)})
$$
我们的E步得到的期望表达式为：
$$
L(\lambda, \overline{\lambda}) = \sum\limits_{I}P(I|O,\overline{\lambda})logP(O,I|\lambda)
$$

在M步我们要极大化上式。由于$P(I|O,\overline{\lambda}) = P(I,O|\overline{\lambda})/P(O|\overline{\lambda})$,而$P(O|\overline{\lambda})$是常数，因此我们要极大化的式子等价于：
$$
\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{I}P(O,I|\overline{\lambda})logP(O,I|\lambda)
$$

我们将上面$P(O,I|\lambda)$的表达式带入我们的极大化式子，得到的表达式如下：
$$
\overline{\lambda} = arg\;\max_{\lambda}\sum\limits_{d=1}^D\sum\limits_{I}P(O,I|\overline{\lambda})(log\pi_{i_1} + \sum\limits_{t=1}^{T-1}log\;a_{i_t,i_{t+1}} +  \sum\limits_{t=1}^Tb_{i_t}(o_t))
$$

我们的隐藏模型参数$\lambda =(A,B,\Pi)$,因此下面我们只需要对上式分别对$A,B,\Pi$求导即可得到我们更新的模型参数$\overline{\lambda}$  

 

首先我们看看对模型参数$\Pi$的求导。由于$\Pi$只在上式中括号里的第一部分出现，因此我们对于$\Pi$的极大化式子为：
$$
\overline{\pi_i} = arg\;\max_{\pi_{i_1}} \sum\limits_{d=1}^D\sum\limits_{I}P(O,I|\overline{\lambda})log\pi_{i_1} = arg\;\max_{\pi_{i}} \sum\limits_{d=1}^D\sum\limits_{i=1}^NP(O,i_1^{(d)} =i|\overline{\lambda})log\pi_{i}
$$

由于$\pi_i$还满足$\sum\limits_{i=1}^N\pi_i =1$，因此根据拉格朗日子乘法，我们得到$\pi_i$要极大化的拉格朗日函数为：
$$
arg\;\max_{\pi_{i}}\sum\limits_{d=1}^D\sum\limits_{i=1}^NP(O,i_1^{(d)} =i|\overline{\lambda})log\pi_{i} + \gamma(\sum\limits_{i=1}^N\pi_i -1)
$$

其中，$\gamma$为拉格朗日系数。上式对$\pi_i$求偏导数并令结果为0， 我们得到：
$$
\sum\limits_{d=1}^DP(O,i_1^{(d)} =i|\overline{\lambda}) + \gamma\pi_i = 0
$$

令$i$分别等于从1到$N$，从上式可以得到$N$个式子，对这$N$个式子求和可得：
$$
\sum\limits_{d=1}^DP(O|\overline{\lambda}) + \gamma = 0
$$

从上两式消去$\gamma$,得到$\pi_i$的表达式为：
$$
\pi_i =\frac{\sum\limits_{d=1}^DP(O,i_1^{(d)} =i|\overline{\lambda})}{\sum\limits_{d=1}^DP(O|\overline{\lambda})} = \frac{\sum\limits_{d=1}^DP(O,i_1^{(d)} =i|\overline{\lambda})}{DP(O|\overline{\lambda})} = \frac{\sum\limits_{d=1}^DP(i_1^{(d)} =i|O, \overline{\lambda})}{D} =  \frac{\sum\limits_{d=1}^DP(i_1^{(d)} =i|O^{(d)}, \overline{\lambda})}{D}
$$

利用我们在隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率里第二节中前向概率的定义可得：
$$
P(i_1^{(d)} =i|O^{(d)}, \overline{\lambda}) = \gamma_1^{(d)}(i)
$$

因此最终我们在M步$\pi_i$的迭代公式为：
$$
\pi_i =  \frac{\sum\limits_{d=1}^D\gamma_1^{(d)}(i)}{D}
$$



现在我们来看看$A$的迭代公式求法。方法和$\Pi$的类似。由于$A$只在最大化函数式中括号里的第二部分出现，而这部分式子可以整理为：
$$
\sum\limits_{d=1}^D\sum\limits_{I}\sum\limits_{t=1}^{T-1}P(O,I|\overline{\lambda})log\;a_{i_t,i_{t+1}} = \sum\limits_{d=1}^D\sum\limits_{i=1}^N\sum\limits_{j=1}^N\sum\limits_{t=1}^{T-1}P(O,i_t^{(d)} = i, i_{t+1}^{(d)} = j|\overline{\lambda})log\;a_{ij}
$$

由于$a_{ij}$还满足$\sum\limits_{j=1}^Na_{ij} =1$。和求解$\pi_i$类似，我们可以用拉格朗日子乘法并对$a_{ij}$求导，并令结果为0，可以得到$a_{ij}$的迭代表达式为：
$$
a_{ij} = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i, i_{t+1}^{(d)} = j|\overline{\lambda})}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}P(O^{(d)}, i_t^{(d)} = i|\overline{\lambda})}
$$

利用隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率里第二节中前向概率的定义和第五节$\xi_t(i,j)$的定义可得们在M步$a_{ij}$的迭代公式为：
$$
a_{ij} = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\xi_t^{(d)}(i,j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\gamma_t^{(d)}(i)}
$$



现在我们来看看$B$的迭代公式求法。方法和$Pi$的类似。由于$B$只在最大化函数式中括号里的第三部分出现，而这部分式子可以整理为：
$$
\sum\limits_{d=1}^D\sum\limits_{I}\sum\limits_{t=1}^{T}P(O,I|\overline{\lambda})log\;b_{i_t}(o_t) = \sum\limits_{d=1}^D\sum\limits_{j=1}^N\sum\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\overline{\lambda})log\;b_{j}(o_t)
$$

由于$b_{j}(o_t)$还满足$\sum\limits_{k=1}^Mb_{j}(o_t =v_k) =1$。和求解$\pi_i$类似，我们可以用拉格朗日子乘法并对$b_{j}(k)$求导，并令结果为0，得到$b_{j}(k)$的迭代表达式为：
$$
b_{j}(k) = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\overline{\lambda})I(o_t^{(d)}=v_k)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}P(O,i_t^{(d)} = j|\overline{\lambda})}
$$

其中$I(o_t^{(d)}=v_k)$当且仅当$o_t^{(d)}=v_k$时为1，否则为0. 利用隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率里第二节中前向概率的定义可得$b_{j}(o_t)$的最终表达式为：
$$
b_{j}(k) = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1, o_t^{(d)}=v_k}^{T}\gamma_t^{(d)}(j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}\gamma_t^{(d)}(j)}
$$

有了$\pi_i, a_{ij},b_{j}(k)$的迭代公式，我们就可以迭代求解HMM模型参数了。



## 鲍姆-韦尔奇算法流程总结

这里我们概括总结下鲍姆-韦尔奇算法的流程。

**输入**： $D$个观测序列样本$\{(O_1), (O_2), ...(O_D)\}$ 

**输出**：HMM模型参数

1)  随机初始化所有的$\pi_i, a_{ij},b_{j}(k)$ 
2)  对于每个样本$d = 1,2,...D$，用前向后向算法计算$\gamma_t^{(d)}(i)，\xi_t^{(d)}(i,j), t =1,2...T$ 
3)  更新模型参数：
$$
\pi_i =  \frac{\sum\limits_{d=1}^D\gamma_1^{(d)}(i)}{D}
$$


$$
a_{ij} = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\xi_t^{(d)}(i,j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T-1}\gamma_t^{(d)}(i)}
$$


$$
b_{j}(k) = \frac{\sum\limits_{d=1}^D\sum\limits_{t=1, o_t^{(d)}=v_k}^{T}\gamma_t^{(d)}(j)}{\sum\limits_{d=1}^D\sum\limits_{t=1}^{T}\gamma_t^{(d)}(j)}
$$
4) 如果$\pi_i, a_{ij},b_{j}(k)$的值已经收敛，则算法结束，否则回到第2）步继续迭代。

以上就是鲍姆-韦尔奇算法的整个过程。



# 维特比算法解码隐藏状态序列

在本篇我们会讨论HMM模型最后一个问题的求解，即给定模型和观测序列，求给定观测序列条件下，最可能出现的对应的隐藏状态序列。

HMM模型的解码问题最常用的算法是维特比算法，当然也有其他的算法可以求解这个问题。同时维特比算法是一个通用的求序列最短路径的动态规划算法，也可以用于很多其他问题，比如之前讲到的文本挖掘的分词原理中我们讲到了单独用维特比算法来做分词。

本文关注于用维特比算法来解码HMM的的最可能隐藏状态序列。

## HMM最可能隐藏状态序列求解概述

在HMM模型的解码问题中，给定模型$\lambda = (A, B, \Pi)$和观测序列$O =\{o_1,o_2,...o_T\}$，求给定观测序列$O$的条件下，最可能出现的对应的状态序列$I^*= \{i_1^*,i_2^*,...i_T^*\}$，即$P(I^*|O)$要最大化。

一个可能的近似解法是求出观测序列$O$在每个时刻$t$最可能的隐藏状态$i_t^*$然后得到一个近似的隐藏状态序列$I^*= \{i_1^*,i_2^*,...i_T^*\}$。要这样近似求解不难，利用隐马尔科夫模型HMM（二）前向后向算法评估观察序列概率中第五节的定义：在给定模型$\lambda$和观测序列$O$时，在时刻$t$处于状态$i$的概率是$\gamma_t(i)$，这个概率可以通过HMM的前向算法与后向算法计算。这样我们有：
$$
i_t^* = arg \max_{1 \leq i \leq N}[\gamma_t(i)], \; t =1,2,...T
$$

近似算法很简单，但是却不能保证预测的状态序列是整体是最可能的状态序列，因为预测的状态序列中某些相邻的隐藏状态可能存在转移概率为0的情况。

而维特比算法可以将HMM的状态序列作为一个整体来考虑，避免近似算法的问题，下面我们来看看维特比算法进行HMM解码的方法。

## 维特比算法概述

维特比算法是一个通用的解码算法，是基于动态规划的求序列最短路径的方法。

既然是动态规划算法，那么就需要找到合适的局部状态，以及局部状态的递推公式。在HMM中，维特比算法定义了两个局部状态用于递推。

**第一个局部状态**是在时刻$t$隐藏状态为$i$所有可能的状态转移路径$i_1,i_2,...i_t$中的概率最大值。记为$\delta_t(i)$:
$$
\delta_t(i) = \max_{i_1,i_2,...i_{t-1}}\;P(i_t=i, i_1,i_2,...i_{t-1},o_t,o_{t-1},...o_1|\lambda),\; i =1,2,...N
$$

由$\delta_t(i)$的定义可以得到$\delta$的递推表达式：
$$
\begin{align} \delta_{t+1}(i) & =  \max_{i_1,i_2,...i_{t}}\;P(i_{t+1}=i, i_1,i_2,...i_{t},o_{t+1},o_{t},...o_1|\lambda) \\ & = \max_{1 \leq j \leq N}\;[\delta_t(j)a_{ji}]b_i(o_{t+1})\end{align}
$$

**第二个局部状态**由第一个局部状态递推得到。我们定义在时刻$t$隐藏状态为$i$的所有单个状态转移路径$(i_1,i_2,...,i_{t-1},i)$中概率最大的转移路径中第$t−1$个节点的隐藏状态为$\Psi_t(i)$,其递推表达式可以表示为：
$$
\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]
$$

有了这两个局部状态，我们就可以从时刻0一直递推到时刻$T$，然后利用$\Psi_t(i)$记录的前一个最可能的状态节点回溯，直到找到最优的隐藏状态序列。



## 维特比算法流程总结

现在我们来总结下维特比算法的流程：

**输入**：HMM模型$\lambda = (A, B, \Pi)$，观测序列$O=(o_1,o_2,...o_T)$ 

**输出**：最有可能的隐藏状态序列$I^*= \{i_1^*,i_2^*,...i_T^*\}$ 

1. 初始化局部状态：

$$
\delta_1(i) = \pi_ib_i(o_1),\;i=1,2...N
$$

$$
\Psi_1(i)=0,\;i=1,2...N
$$


2) 进行动态规划递推时刻$t=2,3,...T$时刻的局部状态：
$$
\delta_{t}(i) = \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}]b_i(0_{t}),\;i=1,2...N
$$

$$
\Psi_t(i) = arg \; \max_{1 \leq j \leq N}\;[\delta_{t-1}(j)a_{ji}],\;i=1,2...N
$$


3) 计算时刻$T$最大的$\delta_{T}(i)$,即为最可能隐藏状态序列出现的概率。计算时刻$T$最大的$\Psi_t(i)$,即为时刻$T$最可能的隐藏状态。
$$
P* = \max_{1 \leq j \leq N}\delta_{T}(i)
$$

$$
i_T^* = arg \; \max_{1 \leq j \leq N}\;[\delta_{T}(i)]
$$


4) 利用局部状态$\Psi(i)$开始回溯。对于$t=T-1,T-2,...,1$：
$$
i_t^* = \Psi_{t+1}(i_{t+1}^*)
$$

最终得到最有可能的隐藏状态序列$I^*= \{i_1^*,i_2^*,...i_T^*\}$  



## HMM维特比算法求解实例

下面我们仍然用隐马尔科夫模型HMM（一）HMM模型中盒子与球的例子来看看HMM维特比算法求解。

我们的观察集合是：
$$
V=\{红，白\}，M=2
$$

我们的状态集合是：
$$
Q =\{盒子1，盒子2，盒子3\}， N=3
$$

而观察序列和状态序列的长度为3。

初始状态分布为：
$$
\Pi = (0.2,0.4,0.4)^T
$$

状态转移概率分布矩阵为：
$$
A = \left( \begin{array} {ccc} 0.5 & 0.2 & 0.3 \\ 0.3 & 0.5 & 0.2 \\ 0.2 & 0.3 &0.5 \end{array} \right)
$$

 观测状态概率矩阵为：
$$
B = \left( \begin{array} {ccc} 0.5 & 0.5 \\ 0.4 & 0.6 \\ 0.7 & 0.3 \end{array} \right)
$$

球的颜色的观测序列:
$$
O=\{红，白，红\}
$$

按照我们上一节的维特比算法，首先需要得到三个隐藏状态在时刻1时对应的各自两个局部状态，此时观测状态为1：
$$
\delta_1(1) = \pi_1b_1(o_1) = 0.2 \times 0.5 = 0.1
$$

$$
\delta_1(2) = \pi_2b_2(o_1) = 0.4 \times 0.4 = 0.16
$$

$$
\delta_1(3) = \pi_3b_3(o_1) = 0.4 \times 0.7 = 0.28
$$

$$
\Psi_1(1)=\Psi_1(2) =\Psi_1(3) =0
$$


现在开始递推三个隐藏状态在时刻2时对应的各自两个局部状态，此时观测状态为2：
$$
\delta_2(1) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j1}]b_1(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.5, 0.16 \times 0.3, 0.28\times 0.2] \times 0.5 = 0.028
$$

$$
\Psi_2(1)=3
$$

$$
\delta_2(2) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j2}]b_2(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.2, 0.16 \times 0.5, 0.28\times 0.3] \times 0.6 = 0.0504
$$

$$
\Psi_2(2)=3
$$

$$
\delta_2(3) = \max_{1\leq j \leq 3}[\delta_1(j)a_{j3}]b_3(o_2) = \max_{1\leq j \leq 3}[0.1 \times 0.3, 0.16 \times 0.2, 0.28\times 0.5] \times 0.3 = 0.042
$$

$$
\Psi_2(3)=3
$$


继续递推三个隐藏状态在时刻3时对应的各自两个局部状态，此时观测状态为1：
$$
\delta_3(1) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j1}]b_1(o_3) = \max_{1\leq j \leq 3}[0.028 \times 0.5, 0.0504 \times 0.3, 0.042\times 0.2] \times 0.5 = 0.00756
$$

$$
\Psi_3(1)=2
$$



$$
\delta_3(2) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j2}]b_2(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.2, 0.0504\times 0.5, 0.042\times 0.3] \times 0.4 = 0.01008
$$

$$
\Psi_3(2)=2
$$



$$
\delta_3(3) = \max_{1\leq j \leq 3}[\delta_2(j)a_{j3}]b_3(o_3) = \max_{1\leq j \leq 3}[0.028  \times 0.3, 0.0504 \times 0.2, 0.042\times 0.5] \times 0.7 = 0.0147
$$

$$
\Psi_3(3)=3
$$

此时已经到最后的时刻，我们开始准备回溯。此时最大概率为$\delta_3(3)$,从而得到$i_3^* =3$ 
由于$\Psi_3(3)=3$,所以$i_2^* =3$, 而又由于$\Psi_2(3)=3$,所以$i_1^* =3$。从而得到最终的最可能的隐藏状态序列为：$(3,3,3)$   



## HMM模型维特比算法总结

如果大家看过之前写的文本挖掘的分词原理中的维特比算法，就会发现这两篇之中的维特比算法稍有不同。主要原因是在中文分词时，我们没有观察状态和隐藏状态的区别，只有一种状态。但是维特比算法的核心是定义动态规划的局部状态与局部递推公式，这一点在中文分词维特比算法和HMM的维特比算法是相同的，也是维特比算法的精华所在。

维特比算法也是寻找序列最短路径的一个通用方法，和dijkstra算法有些类似，但是dijkstra算法并没有使用动态规划，而是贪心算法。同时维特比算法仅仅局限于求序列最短路径，而dijkstra算法是通用的求最短路径的方法。



# 用hmmlearn学习隐马尔科夫模型HMM

在之前的HMM系列中，我们对隐马尔科夫模型HMM的原理以及三个问题的求解方法做了总结。本文我们就从实践的角度用Python的hmmlearn库来学习HMM的使用。关于hmmlearn的更多资料在官方文档有介绍。

## hmmlearn概述

hmmlearn安装很简单，"pip install hmmlearn"即可完成。

hmmlearn实现了三种HMM模型类，按照观测状态是连续状态还是离散状态，可以分为两类。GaussianHMM和GMMHMM是连续观测状态的HMM模型，而MultinomialHMM是离散观测状态的模型，也是我们在HMM原理系列篇里面使用的模型。

对于MultinomialHMM的模型，使用比较简单，"startprob\_"参数对应我们的隐藏状态初始分布$\Pi$, "transmat\_"对应我们的状态转移矩阵A, "emissionprob_"对应我们的观测状态概率矩阵B。

对于连续观测状态的HMM模型，GaussianHMM类假设观测状态符合高斯分布，而GMMHMM类则假设观测状态符合混合高斯分布。一般情况下我们使用GaussianHMM即高斯分布的观测状态即可。以下对于连续观测状态的HMM模型，我们只讨论GaussianHMM类。

在GaussianHMM类中，"startprob_"参数对应我们的隐藏状态初始分布$\Pi$, "transmat_"对应我们的状态转移矩阵A, 比较特殊的是观测状态概率的表示方法，此时由于观测状态是连续值，我们无法像MultinomialHMM一样直接给出矩阵B。而是采用给出各个隐藏状态对应的观测状态高斯分布的概率密度函数的参数。

如果观测序列是一维的，则观测状态的概率密度函数是一维的普通高斯分布。如果观测序列是N维的，则隐藏状态对应的观测状态的概率密度函数是N维高斯分布。高斯分布的概率密度函数参数可以用$\mu$表示高斯分布的期望向量，$\Sigma$表示高斯分布的协方差矩阵。在GaussianHMM类中，“means”用来表示各个隐藏状态对应的高斯分布期望向量$\mu$形成的矩阵，而“covars”用来表示各个隐藏状态对应的高斯分布协方差矩阵$\Sigma$形成的三维张量。

## MultinomialHMM实例

下面我们用我们在HMM系列原理篇中的例子来使用MultinomialHMM跑一遍。

完整代码参见我的github:https://github.com/ljpzzz/machinelearning/blob/master/natural-language-processing/hmm.ipynb

首先建立HMM的模型：


```python
import numpy as np
from hmmlearn import hmm

states = ["box 1", "box 2", "box3"]
n_states = len(states)

observations = ["red", "white"]
n_observations = len(observations)

start_probability = np.array([0.2, 0.4, 0.4])

transition_probability = np.array([
  [0.5, 0.2, 0.3],
  [0.3, 0.5, 0.2],
  [0.2, 0.3, 0.5]
])

emission_probability = np.array([
  [0.5, 0.5],
  [0.4, 0.6],
  [0.7, 0.3]
])

model = hmm.MultinomialHMM(n_components=n_states)
model.startprob_=start_probability
model.transmat_=transition_probability
model.emissionprob_=emission_probability
```



现在我们来跑一跑HMM问题三维特比算法的解码过程，使用和原理篇一样的观测序列来解码，代码如下：


```python
seen = np.array([[0,1,0]]).T
logprob, box = model.decode(seen, algorithm="viterbi") 
print("The ball picked:", 
      ", ".join(list(map(lambda x: observations[int(x)], seen))))
print("The hidden box",         
      ", ".join(list(map(lambda x: states[int(x)], box))))
```

    The ball picked: red, white, red
    The hidden box box3, box3, box3


可以看出，结果和我们原理篇中的手动计算的结果是一样的。

也可以使用predict函数，结果也是一样的，代码如下：


```python
box2 = model.predict(seen)
print("The ball picked:", 
      ", ".join(list(map(lambda x: observations[int(x)], seen))))
print("The hidden box", 
      ", ".join(list(map(lambda x: states[int(x)], box2))))
```

    The ball picked: red, white, red
    The hidden box box3, box3, box3


大家可以跑一下，看看结果是否和decode函数相同。

现在我们再来看看求HMM问题一的观测序列的概率的问题，代码如下：


```python
print(model.score(seen)) 
```

    -2.038545309915233


要注意的是score函数返回的是以自然对数为底的对数概率值，我们在HMM问题一中手动计算的结果是未取对数的原始概率是0.13022。对比一下：
$$ln0.13022 \approx -2.0385$$

现在我们再看看HMM问题二，求解模型参数的问题。由于鲍姆-韦尔奇算法是基于EM算法的近似算法，所以我们需要多跑几次，比如下面我们跑三次，选择一个比较优的模型参数，代码如下：


```python
import numpy as np
from hmmlearn import hmm

states = ["box 1", "box 2", "box3"]
n_states = len(states)

observations = ["red", "white"]
n_observations = len(observations)
model2 = hmm.MultinomialHMM(n_components=n_states, n_iter=20, tol=0.01)
X2 = np.array([[0, 1, 0, 1], 
               [0, 0, 0, 1], 
               [1, 0, 1, 1]])
model2.fit(X2)
print(model2.startprob_)
print(model2.transmat_)
print(model2.emissionprob_)
print(model2.score(X2))

model2.fit(X2)
print(model2.startprob_)
print(model2.transmat_)
print(model2.emissionprob_)
print(model2.score(X2))

model2.fit(X2)
print(model2.startprob_)
print(model2.transmat_)
print(model2.emissionprob_)
print(model2.score(X2))
```

    [9.99999258e-01 7.41528442e-07 2.20682824e-28]
    [[6.07461697e-05 1.38731365e-01 8.61207889e-01]
     [3.51791469e-01 4.49841063e-01 1.98367468e-01]
     [4.69040621e-01 1.90760277e-01 3.40199102e-01]]
    [[9.99204051e-01 7.95948728e-04]
     [6.96877320e-01 3.03122680e-01]
     [4.74499100e-02 9.52550090e-01]]
    -6.706232891929279
    [2.89674215e-09 9.99999997e-01 9.79663711e-20]
    [[1.85532332e-01 6.76812756e-01 1.37654912e-01]
     [2.27391613e-01 7.43973682e-06 7.72600947e-01]
     [1.33844665e-01 5.61442834e-01 3.04712501e-01]]
    [[3.52638443e-01 6.47361557e-01]
     [9.99575418e-01 4.24581826e-04]
     [1.08716362e-01 8.91283638e-01]]
    -6.542774782915058
    [1.00000000e+00 2.86748513e-12 5.23293464e-11]
    [[2.08271737e-05 5.36517959e-01 4.63461214e-01]
     [5.91617412e-01 2.10780545e-01 1.97602044e-01]
     [6.45630315e-01 1.81394168e-01 1.72975517e-01]]
    [[9.99264479e-01 7.35521403e-04]
     [1.45758031e-01 8.54241969e-01]
     [1.87083070e-01 8.12916930e-01]]
    -6.542851326383321



## GaussianHMM实例

下面我们再给一个GaussianHMM的实例，这个实例中，我们的观测状态是二维的，而隐藏状态有4个。因此我们的“means”参数是4×2的矩阵，而“covars”参数是4×2×2的张量。

建立模型如下：


```python
startprob = np.array([0.6, 0.3, 0.1, 0.0])
# The transition matrix, note that there are no transitions possible
# between component 1 and 3
transmat = np.array([[0.7, 0.2, 0.0, 0.1],
                     [0.3, 0.5, 0.2, 0.0],
                     [0.0, 0.3, 0.5, 0.2],
                     [0.2, 0.0, 0.2, 0.6]])
# The means of each component
means = np.array([[0.0,  0.0],
                  [0.0, 11.0],
                  [9.0, 10.0],
                  [11.0, -1.0]])
# The covariance of each component
covars = .5 * np.tile(np.identity(2), (4, 1, 1))

# Build an HMM instance and set parameters
model3 = hmm.GaussianHMM(n_components=4, covariance_type="full")

# Instead of fitting it from the data, we directly set the estimated
# parameters, the means and covariance of the components
model3.startprob_ = startprob
model3.transmat_ = transmat
model3.means_ = means
model3.covars_ = covars
```

 注意上面有个参数covariance_type，取值为"full"意味所有的$\mu$, $\Sigma$都需要指定。取值为“spherical”则$\Sigma$的非对角线元素为0，对角线元素相同。取值为“diag”则$\Sigma$的非对角线元素为0，对角线元素可以不同，"tied"指所有的隐藏状态对应的观测状态分布使用相同的协方差矩阵$\Sigma$

我们现在跑一跑HMM问题一解码的过程，由于观测状态是二维的，我们用的三维观测序列， 所以这里的 输入是一个3×2的矩阵，代码如下：


```python
seen = np.array([[1.1, 2.0], 
                 [-1, 2.0], 
                 [3, 7]], dtype=np.int32)
logprob, state = model3.decode(seen, algorithm="viterbi")
print(state)
```

    [0 0 1]


再看看HMM问题一对数概率的计算：


```python
print(model3.score(seen))
```

    -40.911128137687
