```
# æ·±åº¦å­¦ä¹ ç®€ä»‹
# é¢„å¤‡çŸ¥è¯†
# æ·±åº¦å­¦ä¹ åŸºç¡€
```

# æ·±åº¦å­¦ä¹ è®¡ç®—

## æ¨¡å‹æ„é€ 

è®©æˆ‘ä»¬å›é¡¾ä¸€ä¸‹åœ¨3.10èŠ‚ï¼ˆå¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°ï¼‰ä¸­å«å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºçš„å®ç°æ–¹æ³•ã€‚æˆ‘ä»¬é¦–å…ˆæ„é€ `Sequential`å®ä¾‹ï¼Œç„¶åä¾æ¬¡æ·»åŠ ä¸¤ä¸ªå…¨è¿æ¥å±‚ã€‚å…¶ä¸­ç¬¬ä¸€å±‚çš„è¾“å‡ºå¤§å°ä¸º256ï¼Œå³éšè—å±‚å•å…ƒä¸ªæ•°æ˜¯256ï¼›ç¬¬äºŒå±‚çš„è¾“å‡ºå¤§å°ä¸º10ï¼Œå³è¾“å‡ºå±‚å•å…ƒä¸ªæ•°æ˜¯10ã€‚æˆ‘ä»¬åœ¨ä¸Šä¸€ç« çš„å…¶ä»–èŠ‚ä¸­ä¹Ÿä½¿ç”¨äº†`Sequential`ç±»æ„é€ æ¨¡å‹ã€‚è¿™é‡Œæˆ‘ä»¬ä»‹ç»å¦å¤–ä¸€ç§åŸºäº`Module`ç±»çš„æ¨¡å‹æ„é€ æ–¹æ³•ï¼šå®ƒè®©æ¨¡å‹æ„é€ æ›´åŠ çµæ´»ã€‚

> æ³¨ï¼šå…¶å®å‰é¢æˆ‘ä»¬é™†é™†ç»­ç»­å·²ç»ä½¿ç”¨äº†è¿™äº›æ–¹æ³•äº†ï¼Œæœ¬èŠ‚ç³»ç»Ÿä»‹ç»ä¸€ä¸‹ã€‚

###  ç»§æ‰¿`Module`ç±»æ¥æ„é€ æ¨¡å‹

`Module`ç±»æ˜¯`nn`æ¨¡å—é‡Œæä¾›çš„ä¸€ä¸ªæ¨¡å‹æ„é€ ç±»ï¼Œæ˜¯æ‰€æœ‰ç¥ç»ç½‘ç»œæ¨¡å—çš„åŸºç±»ï¼Œæˆ‘ä»¬å¯ä»¥ç»§æ‰¿å®ƒæ¥å®šä¹‰æˆ‘ä»¬æƒ³è¦çš„æ¨¡å‹ã€‚ä¸‹é¢ç»§æ‰¿`Module`ç±»æ„é€ æœ¬èŠ‚å¼€å¤´æåˆ°çš„å¤šå±‚æ„ŸçŸ¥æœºã€‚è¿™é‡Œå®šä¹‰çš„`MLP`ç±»é‡è½½äº†`Module`ç±»çš„`__init__`å‡½æ•°å’Œ`forward`å‡½æ•°ã€‚å®ƒä»¬åˆ†åˆ«ç”¨äºåˆ›å»ºæ¨¡å‹å‚æ•°å’Œå®šä¹‰å‰å‘è®¡ç®—ã€‚å‰å‘è®¡ç®—ä¹Ÿå³æ­£å‘ä¼ æ’­ã€‚

``` python
import torch
from torch import nn

class MLP(nn.Module):
    # å£°æ˜å¸¦æœ‰æ¨¡å‹å‚æ•°çš„å±‚ï¼Œè¿™é‡Œå£°æ˜äº†ä¸¤ä¸ªå…¨è¿æ¥å±‚
    def __init__(self, **kwargs):
        # è°ƒç”¨MLPçˆ¶ç±»Moduleçš„æ„é€ å‡½æ•°æ¥è¿›è¡Œå¿…è¦çš„åˆå§‹åŒ–ã€‚è¿™æ ·åœ¨æ„é€ å®ä¾‹æ—¶è¿˜å¯ä»¥æŒ‡å®šå…¶ä»–å‡½æ•°
        # å‚æ•°ï¼Œå¦‚â€œæ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«â€ä¸€èŠ‚å°†ä»‹ç»çš„æ¨¡å‹å‚æ•°params
        super(MLP, self).__init__(**kwargs)
        self.hidden = nn.Linear(784, 256) # éšè—å±‚
        self.act = nn.ReLU()
        self.output = nn.Linear(256, 10)  # è¾“å‡ºå±‚
         

    # å®šä¹‰æ¨¡å‹çš„å‰å‘è®¡ç®—ï¼Œå³å¦‚ä½•æ ¹æ®è¾“å…¥xè®¡ç®—è¿”å›æ‰€éœ€è¦çš„æ¨¡å‹è¾“å‡º
    def forward(self, x):
        a = self.act(self.hidden(x))
        return self.output(a)
```

ä»¥ä¸Šçš„`MLP`ç±»ä¸­æ— é¡»å®šä¹‰åå‘ä¼ æ’­å‡½æ•°ã€‚ç³»ç»Ÿå°†é€šè¿‡è‡ªåŠ¨æ±‚æ¢¯åº¦è€Œè‡ªåŠ¨ç”Ÿæˆåå‘ä¼ æ’­æ‰€éœ€çš„`backward`å‡½æ•°ã€‚

æˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–`MLP`ç±»å¾—åˆ°æ¨¡å‹å˜é‡`net`ã€‚ä¸‹é¢çš„ä»£ç åˆå§‹åŒ–`net`å¹¶ä¼ å…¥è¾“å…¥æ•°æ®`X`åšä¸€æ¬¡å‰å‘è®¡ç®—ã€‚å…¶ä¸­ï¼Œ`net(X)`ä¼šè°ƒç”¨`MLP`ç»§æ‰¿è‡ª`Module`ç±»çš„`__call__`å‡½æ•°ï¼Œè¿™ä¸ªå‡½æ•°å°†è°ƒç”¨`MLP`ç±»å®šä¹‰çš„`forward`å‡½æ•°æ¥å®Œæˆå‰å‘è®¡ç®—ã€‚

``` python
X = torch.rand(2, 784)
net = MLP()
print(net)
net(X)
```
è¾“å‡ºï¼š
```
MLP(
  (hidden): Linear(in_features=784, out_features=256, bias=True)
  (act): ReLU()
  (output): Linear(in_features=256, out_features=10, bias=True)
)
tensor([[-0.1798, -0.2253,  0.0206, -0.1067, -0.0889,  0.1818, -0.1474,  0.1845,
         -0.1870,  0.1970],
        [-0.1843, -0.1562, -0.0090,  0.0351, -0.1538,  0.0992, -0.0883,  0.0911,
         -0.2293,  0.2360]], grad_fn=<ThAddmmBackward>)
```

æ³¨æ„ï¼Œè¿™é‡Œå¹¶æ²¡æœ‰å°†`Module`ç±»å‘½åä¸º`Layer`ï¼ˆå±‚ï¼‰æˆ–è€…`Model`ï¼ˆæ¨¡å‹ï¼‰ä¹‹ç±»çš„åå­—ï¼Œè¿™æ˜¯å› ä¸ºè¯¥ç±»æ˜¯ä¸€ä¸ªå¯ä¾›è‡ªç”±ç»„å»ºçš„éƒ¨ä»¶ã€‚å®ƒçš„å­ç±»æ—¢å¯ä»¥æ˜¯ä¸€ä¸ªå±‚ï¼ˆå¦‚PyTorchæä¾›çš„`Linear`ç±»ï¼‰ï¼Œåˆå¯ä»¥æ˜¯ä¸€ä¸ªæ¨¡å‹ï¼ˆå¦‚è¿™é‡Œå®šä¹‰çš„`MLP`ç±»ï¼‰ï¼Œæˆ–è€…æ˜¯æ¨¡å‹çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚æˆ‘ä»¬ä¸‹é¢é€šè¿‡ä¸¤ä¸ªä¾‹å­æ¥å±•ç¤ºå®ƒçš„çµæ´»æ€§ã€‚

###  `Module`çš„å­ç±»

æˆ‘ä»¬åˆšåˆšæåˆ°ï¼Œ`Module`ç±»æ˜¯ä¸€ä¸ªé€šç”¨çš„éƒ¨ä»¶ã€‚äº‹å®ä¸Šï¼ŒPyTorchè¿˜å®ç°äº†ç»§æ‰¿è‡ª`Module`çš„å¯ä»¥æ–¹ä¾¿æ„å»ºæ¨¡å‹çš„ç±»: å¦‚`Sequential`ã€`ModuleList`å’Œ`ModuleDict`ç­‰ç­‰ã€‚

####  `Sequential`ç±»

å½“æ¨¡å‹çš„å‰å‘è®¡ç®—ä¸ºç®€å•ä¸²è”å„ä¸ªå±‚çš„è®¡ç®—æ—¶ï¼Œ`Sequential`ç±»å¯ä»¥é€šè¿‡æ›´åŠ ç®€å•çš„æ–¹å¼å®šä¹‰æ¨¡å‹ã€‚è¿™æ­£æ˜¯`Sequential`ç±»çš„ç›®çš„ï¼šå®ƒå¯ä»¥æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„æœ‰åºå­—å…¸ï¼ˆOrderedDictï¼‰æˆ–è€…ä¸€ç³»åˆ—å­æ¨¡å—ä½œä¸ºå‚æ•°æ¥é€ä¸€æ·»åŠ `Module`çš„å®ä¾‹ï¼Œè€Œæ¨¡å‹çš„å‰å‘è®¡ç®—å°±æ˜¯å°†è¿™äº›å®ä¾‹æŒ‰æ·»åŠ çš„é¡ºåºé€ä¸€è®¡ç®—ã€‚

ä¸‹é¢æˆ‘ä»¬å®ç°ä¸€ä¸ªä¸`Sequential`ç±»æœ‰ç›¸åŒåŠŸèƒ½çš„`MySequential`ç±»ã€‚è¿™æˆ–è®¸å¯ä»¥å¸®åŠ©è¯»è€…æ›´åŠ æ¸…æ™°åœ°ç†è§£`Sequential`ç±»çš„å·¥ä½œæœºåˆ¶ã€‚

``` python
class MySequential(nn.Module):
    from collections import OrderedDict
    def __init__(self, *args):
        super(MySequential, self).__init__()
        if len(args) == 1 and isinstance(args[0], OrderedDict): # å¦‚æœä¼ å…¥çš„æ˜¯ä¸€ä¸ªOrderedDict
            for key, module in args[0].items():
                self.add_module(key, module)  # add_moduleæ–¹æ³•ä¼šå°†moduleæ·»åŠ è¿›self._modules(ä¸€ä¸ªOrderedDict)
        else:  # ä¼ å…¥çš„æ˜¯ä¸€äº›Module
            for idx, module in enumerate(args):
                self.add_module(str(idx), module)
    def forward(self, input):
        # self._modulesè¿”å›ä¸€ä¸ª OrderedDictï¼Œä¿è¯ä¼šæŒ‰ç…§æˆå‘˜æ·»åŠ æ—¶çš„é¡ºåºéå†æˆå‘˜
        for module in self._modules.values():
            input = module(input)
        return input
```

æˆ‘ä»¬ç”¨`MySequential`ç±»æ¥å®ç°å‰é¢æè¿°çš„`MLP`ç±»ï¼Œå¹¶ä½¿ç”¨éšæœºåˆå§‹åŒ–çš„æ¨¡å‹åšä¸€æ¬¡å‰å‘è®¡ç®—ã€‚

``` python
net = MySequential(
        nn.Linear(784, 256),
        nn.ReLU(),
        nn.Linear(256, 10), 
        )
print(net)
net(X)
```
è¾“å‡ºï¼š
```
MySequential(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
tensor([[-0.0100, -0.2516,  0.0392, -0.1684, -0.0937,  0.2191, -0.1448,  0.0930,
          0.1228, -0.2540],
        [-0.1086, -0.1858,  0.0203, -0.2051, -0.1404,  0.2738, -0.0607,  0.0622,
          0.0817, -0.2574]], grad_fn=<ThAddmmBackward>)
```

å¯ä»¥è§‚å¯Ÿåˆ°è¿™é‡Œ`MySequential`ç±»çš„ä½¿ç”¨è·Ÿ3.10èŠ‚ï¼ˆå¤šå±‚æ„ŸçŸ¥æœºçš„ç®€æ´å®ç°ï¼‰ä¸­`Sequential`ç±»çš„ä½¿ç”¨æ²¡ä»€ä¹ˆåŒºåˆ«ã€‚

####  `ModuleList`ç±»

`ModuleList`æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„åˆ—è¡¨ä½œä¸ºè¾“å…¥ï¼Œç„¶åä¹Ÿå¯ä»¥ç±»ä¼¼Listé‚£æ ·è¿›è¡Œappendå’Œextendæ“ä½œ:

``` python
net = nn.ModuleList([nn.Linear(784, 256), nn.ReLU()])
net.append(nn.Linear(256, 10)) # # ç±»ä¼¼Listçš„appendæ“ä½œ
print(net[-1])  # ç±»ä¼¼Listçš„ç´¢å¼•è®¿é—®
print(net)
# net(torch.zeros(1, 784)) # ä¼šæŠ¥NotImplementedError
```
è¾“å‡ºï¼š
```
Linear(in_features=256, out_features=10, bias=True)
ModuleList(
  (0): Linear(in_features=784, out_features=256, bias=True)
  (1): ReLU()
  (2): Linear(in_features=256, out_features=10, bias=True)
)
```

æ—¢ç„¶`Sequential`å’Œ`ModuleList`éƒ½å¯ä»¥è¿›è¡Œåˆ—è¡¨åŒ–æ„é€ ç½‘ç»œï¼Œé‚£äºŒè€…åŒºåˆ«æ˜¯ä»€ä¹ˆå‘¢ã€‚`ModuleList`ä»…ä»…æ˜¯ä¸€ä¸ªå‚¨å­˜å„ç§æ¨¡å—çš„åˆ—è¡¨ï¼Œè¿™äº›æ¨¡å—ä¹‹é—´æ²¡æœ‰è”ç³»ä¹Ÿæ²¡æœ‰é¡ºåºï¼ˆæ‰€ä»¥ä¸ç”¨ä¿è¯ç›¸é‚»å±‚çš„è¾“å…¥è¾“å‡ºç»´åº¦åŒ¹é…ï¼‰ï¼Œè€Œä¸”æ²¡æœ‰å®ç°`forward`åŠŸèƒ½éœ€è¦è‡ªå·±å®ç°ï¼Œæ‰€ä»¥ä¸Šé¢æ‰§è¡Œ`net(torch.zeros(1, 784))`ä¼šæŠ¥`NotImplementedError`ï¼›è€Œ`Sequential`å†…çš„æ¨¡å—éœ€è¦æŒ‰ç…§é¡ºåºæ’åˆ—ï¼Œè¦ä¿è¯ç›¸é‚»å±‚çš„è¾“å…¥è¾“å‡ºå¤§å°ç›¸åŒ¹é…ï¼Œå†…éƒ¨`forward`åŠŸèƒ½å·²ç»å®ç°ã€‚

`ModuleList`çš„å‡ºç°åªæ˜¯è®©ç½‘ç»œå®šä¹‰å‰å‘ä¼ æ’­æ—¶æ›´åŠ çµæ´»ï¼Œè§ä¸‹é¢å®˜ç½‘çš„ä¾‹å­ã€‚
``` python
class MyModule(nn.Module):
    def __init__(self):
        super(MyModule, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

    def forward(self, x):
        # ModuleList can act as an iterable, or be indexed using ints
        for i, l in enumerate(self.linears):
            x = self.linears[i // 2](x) + l(x)
        return x
```

å¦å¤–ï¼Œ`ModuleList`ä¸åŒäºä¸€èˆ¬çš„Pythonçš„`list`ï¼ŒåŠ å…¥åˆ°`ModuleList`é‡Œé¢çš„æ‰€æœ‰æ¨¡å—çš„å‚æ•°ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ•´ä¸ªç½‘ç»œä¸­ï¼Œä¸‹é¢çœ‹ä¸€ä¸ªä¾‹å­å¯¹æ¯”ä¸€ä¸‹ã€‚

``` python
class Module_ModuleList(nn.Module):
    def __init__(self):
        super(Module_ModuleList, self).__init__()
        self.linears = nn.ModuleList([nn.Linear(10, 10)])
    
class Module_List(nn.Module):
    def __init__(self):
        super(Module_List, self).__init__()
        self.linears = [nn.Linear(10, 10)]

net1 = Module_ModuleList()
net2 = Module_List()

print("net1:")
for p in net1.parameters():
    print(p.size())

print("net2:")
for p in net2.parameters():
    print(p)
```
è¾“å‡ºï¼š
```
net1:
torch.Size([10, 10])
torch.Size([10])
net2:
```

####  `ModuleDict`ç±»

`ModuleDict`æ¥æ”¶ä¸€ä¸ªå­æ¨¡å—çš„å­—å…¸ä½œä¸ºè¾“å…¥, ç„¶åä¹Ÿå¯ä»¥ç±»ä¼¼å­—å…¸é‚£æ ·è¿›è¡Œæ·»åŠ è®¿é—®æ“ä½œ:

``` python
net = nn.ModuleDict({
    'linear': nn.Linear(784, 256),
    'act': nn.ReLU(),
})
net['output'] = nn.Linear(256, 10) # æ·»åŠ 
print(net['linear']) # è®¿é—®
print(net.output)
print(net)
# net(torch.zeros(1, 784)) # ä¼šæŠ¥NotImplementedError
```
è¾“å‡ºï¼š
```
Linear(in_features=784, out_features=256, bias=True)
Linear(in_features=256, out_features=10, bias=True)
ModuleDict(
  (act): ReLU()
  (linear): Linear(in_features=784, out_features=256, bias=True)
  (output): Linear(in_features=256, out_features=10, bias=True)
)
```

å’Œ`ModuleList`ä¸€æ ·ï¼Œ`ModuleDict`å®ä¾‹ä»…ä»…æ˜¯å­˜æ”¾äº†ä¸€äº›æ¨¡å—çš„å­—å…¸ï¼Œå¹¶æ²¡æœ‰å®šä¹‰`forward`å‡½æ•°éœ€è¦è‡ªå·±å®šä¹‰ã€‚åŒæ ·ï¼Œ`ModuleDict`ä¹Ÿä¸Pythonçš„`Dict`æœ‰æ‰€ä¸åŒï¼Œ`ModuleDict`é‡Œçš„æ‰€æœ‰æ¨¡å—çš„å‚æ•°ä¼šè¢«è‡ªåŠ¨æ·»åŠ åˆ°æ•´ä¸ªç½‘ç»œä¸­ã€‚

###  æ„é€ å¤æ‚çš„æ¨¡å‹

è™½ç„¶ä¸Šé¢ä»‹ç»çš„è¿™äº›ç±»å¯ä»¥ä½¿æ¨¡å‹æ„é€ æ›´åŠ ç®€å•ï¼Œä¸”ä¸éœ€è¦å®šä¹‰`forward`å‡½æ•°ï¼Œä½†ç›´æ¥ç»§æ‰¿`Module`ç±»å¯ä»¥æå¤§åœ°æ‹“å±•æ¨¡å‹æ„é€ çš„çµæ´»æ€§ã€‚ä¸‹é¢æˆ‘ä»¬æ„é€ ä¸€ä¸ªç¨å¾®å¤æ‚ç‚¹çš„ç½‘ç»œ`FancyMLP`ã€‚åœ¨è¿™ä¸ªç½‘ç»œä¸­ï¼Œæˆ‘ä»¬é€šè¿‡`get_constant`å‡½æ•°åˆ›å»ºè®­ç»ƒä¸­ä¸è¢«è¿­ä»£çš„å‚æ•°ï¼Œå³å¸¸æ•°å‚æ•°ã€‚åœ¨å‰å‘è®¡ç®—ä¸­ï¼Œé™¤äº†ä½¿ç”¨åˆ›å»ºçš„å¸¸æ•°å‚æ•°å¤–ï¼Œæˆ‘ä»¬è¿˜ä½¿ç”¨`Tensor`çš„å‡½æ•°å’ŒPythonçš„æ§åˆ¶æµï¼Œå¹¶å¤šæ¬¡è°ƒç”¨ç›¸åŒçš„å±‚ã€‚

``` python
class FancyMLP(nn.Module):
    def __init__(self, **kwargs):
        super(FancyMLP, self).__init__(**kwargs)
        
        self.rand_weight = torch.rand((20, 20), requires_grad=False) # ä¸å¯è®­ç»ƒå‚æ•°ï¼ˆå¸¸æ•°å‚æ•°ï¼‰
        self.linear = nn.Linear(20, 20)

    def forward(self, x):
        x = self.linear(x)
        # ä½¿ç”¨åˆ›å»ºçš„å¸¸æ•°å‚æ•°ï¼Œä»¥åŠnn.functionalä¸­çš„reluå‡½æ•°å’Œmmå‡½æ•°
        x = nn.functional.relu(torch.mm(x, self.rand_weight.data) + 1)
        
        # å¤ç”¨å…¨è¿æ¥å±‚ã€‚ç­‰ä»·äºä¸¤ä¸ªå…¨è¿æ¥å±‚å…±äº«å‚æ•°
        x = self.linear(x)
        # æ§åˆ¶æµï¼Œè¿™é‡Œæˆ‘ä»¬éœ€è¦è°ƒç”¨itemå‡½æ•°æ¥è¿”å›æ ‡é‡è¿›è¡Œæ¯”è¾ƒ
        while x.norm().item() > 1:
            x /= 2
        if x.norm().item() < 0.8:
            x *= 10
        return x.sum()
```

åœ¨è¿™ä¸ª`FancyMLP`æ¨¡å‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†å¸¸æ•°æƒé‡`rand_weight`ï¼ˆæ³¨æ„å®ƒä¸æ˜¯å¯è®­ç»ƒæ¨¡å‹å‚æ•°ï¼‰ã€åšäº†çŸ©é˜µä¹˜æ³•æ“ä½œï¼ˆ`torch.mm`ï¼‰å¹¶é‡å¤ä½¿ç”¨äº†ç›¸åŒçš„`Linear`å±‚ã€‚ä¸‹é¢æˆ‘ä»¬æ¥æµ‹è¯•è¯¥æ¨¡å‹çš„å‰å‘è®¡ç®—ã€‚

``` python
X = torch.rand(2, 20)
net = FancyMLP()
print(net)
net(X)
```
è¾“å‡ºï¼š
```
FancyMLP(
  (linear): Linear(in_features=20, out_features=20, bias=True)
)
tensor(0.8432, grad_fn=<SumBackward0>)
```

å› ä¸º`FancyMLP`å’Œ`Sequential`ç±»éƒ½æ˜¯`Module`ç±»çš„å­ç±»ï¼Œæ‰€ä»¥æˆ‘ä»¬å¯ä»¥åµŒå¥—è°ƒç”¨å®ƒä»¬ã€‚

``` python
class NestMLP(nn.Module):
    def __init__(self, **kwargs):
        super(NestMLP, self).__init__(**kwargs)
        self.net = nn.Sequential(nn.Linear(40, 30), nn.ReLU()) 

    def forward(self, x):
        return self.net(x)

net = nn.Sequential(NestMLP(), nn.Linear(30, 20), FancyMLP())

X = torch.rand(2, 40)
print(net)
net(X)
```
è¾“å‡ºï¼š
```
Sequential(
  (0): NestMLP(
    (net): Sequential(
      (0): Linear(in_features=40, out_features=30, bias=True)
      (1): ReLU()
    )
  )
  (1): Linear(in_features=30, out_features=20, bias=True)
  (2): FancyMLP(
    (linear): Linear(in_features=20, out_features=20, bias=True)
  )
)
tensor(14.4908, grad_fn=<SumBackward0>)
```

`å°ç»“`

* å¯ä»¥é€šè¿‡ç»§æ‰¿`Module`ç±»æ¥æ„é€ æ¨¡å‹ã€‚
* `Sequential`ã€`ModuleList`ã€`ModuleDict`ç±»éƒ½ç»§æ‰¿è‡ª`Module`ç±»ã€‚
* ä¸`Sequential`ä¸åŒï¼Œ`ModuleList`å’Œ`ModuleDict`å¹¶æ²¡æœ‰å®šä¹‰ä¸€ä¸ªå®Œæ•´çš„ç½‘ç»œï¼Œå®ƒä»¬åªæ˜¯å°†ä¸åŒçš„æ¨¡å—å­˜æ”¾åœ¨ä¸€èµ·ï¼Œéœ€è¦è‡ªå·±å®šä¹‰`forward`å‡½æ•°ã€‚
* è™½ç„¶`Sequential`ç­‰ç±»å¯ä»¥ä½¿æ¨¡å‹æ„é€ æ›´åŠ ç®€å•ï¼Œä½†ç›´æ¥ç»§æ‰¿`Module`ç±»å¯ä»¥æå¤§åœ°æ‹“å±•æ¨¡å‹æ„é€ çš„çµæ´»æ€§ã€‚



-----------
> æ³¨ï¼šæœ¬èŠ‚ä¸åŸä¹¦æ­¤èŠ‚æœ‰ä¸€äº›ä¸åŒï¼Œ[åŸä¹¦ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/model-construction.html)

##  æ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«

åœ¨3.3èŠ‚ï¼ˆçº¿æ€§å›å½’çš„ç®€æ´å®ç°ï¼‰ä¸­ï¼Œæˆ‘ä»¬é€šè¿‡`init`æ¨¡å—æ¥åˆå§‹åŒ–æ¨¡å‹çš„å‚æ•°ã€‚æˆ‘ä»¬ä¹Ÿä»‹ç»äº†è®¿é—®æ¨¡å‹å‚æ•°çš„ç®€å•æ–¹æ³•ã€‚æœ¬èŠ‚å°†æ·±å…¥è®²è§£å¦‚ä½•è®¿é—®å’Œåˆå§‹åŒ–æ¨¡å‹å‚æ•°ï¼Œä»¥åŠå¦‚ä½•åœ¨å¤šä¸ªå±‚ä¹‹é—´å…±äº«åŒä¸€ä»½æ¨¡å‹å‚æ•°ã€‚

æˆ‘ä»¬å…ˆå®šä¹‰ä¸€ä¸ªä¸ä¸Šä¸€èŠ‚ä¸­ç›¸åŒçš„å«å•éšè—å±‚çš„å¤šå±‚æ„ŸçŸ¥æœºã€‚æˆ‘ä»¬ä¾ç„¶ä½¿ç”¨é»˜è®¤æ–¹å¼åˆå§‹åŒ–å®ƒçš„å‚æ•°ï¼Œå¹¶åšä¸€æ¬¡å‰å‘è®¡ç®—ã€‚ä¸ä¹‹å‰ä¸åŒçš„æ˜¯ï¼Œåœ¨è¿™é‡Œæˆ‘ä»¬ä»`nn`ä¸­å¯¼å…¥äº†`init`æ¨¡å—ï¼Œå®ƒåŒ…å«äº†å¤šç§æ¨¡å‹åˆå§‹åŒ–æ–¹æ³•ã€‚

``` python
import torch
from torch import nn
from torch.nn import init

net = nn.Sequential(nn.Linear(4, 3), nn.ReLU(), nn.Linear(3, 1))  # pytorchå·²è¿›è¡Œé»˜è®¤åˆå§‹åŒ–

print(net)
X = torch.rand(2, 4)
Y = net(X).sum()
```
è¾“å‡ºï¼š
```
Sequential(
  (0): Linear(in_features=4, out_features=3, bias=True)
  (1): ReLU()
  (2): Linear(in_features=3, out_features=1, bias=True)
)
```

###  è®¿é—®æ¨¡å‹å‚æ•°

å›å¿†ä¸€ä¸‹ä¸Šä¸€èŠ‚ä¸­æåˆ°çš„`Sequential`ç±»ä¸`Module`ç±»çš„ç»§æ‰¿å…³ç³»ã€‚å¯¹äº`Sequential`å®ä¾‹ä¸­å«æ¨¡å‹å‚æ•°çš„å±‚ï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡`Module`ç±»çš„`parameters()`æˆ–è€…`named_parameters`æ–¹æ³•æ¥è®¿é—®æ‰€æœ‰å‚æ•°ï¼ˆä»¥è¿­ä»£å™¨çš„å½¢å¼è¿”å›ï¼‰ï¼Œåè€…é™¤äº†è¿”å›å‚æ•°`Tensor`å¤–è¿˜ä¼šè¿”å›å…¶åå­—ã€‚ä¸‹é¢ï¼Œè®¿é—®å¤šå±‚æ„ŸçŸ¥æœº`net`çš„æ‰€æœ‰å‚æ•°ï¼š
``` python
print(type(net.named_parameters()))
for name, param in net.named_parameters():
    print(name, param.size())
```
è¾“å‡ºï¼š
```
<class 'generator'>
0.weight torch.Size([3, 4])
0.bias torch.Size([3])
2.weight torch.Size([1, 3])
2.bias torch.Size([1])
```
å¯è§è¿”å›çš„åå­—è‡ªåŠ¨åŠ ä¸Šäº†å±‚æ•°çš„ç´¢å¼•ä½œä¸ºå‰ç¼€ã€‚
æˆ‘ä»¬å†æ¥è®¿é—®`net`ä¸­å•å±‚çš„å‚æ•°ã€‚å¯¹äºä½¿ç”¨`Sequential`ç±»æ„é€ çš„ç¥ç»ç½‘ç»œï¼Œæˆ‘ä»¬å¯ä»¥é€šè¿‡æ–¹æ‹¬å·`[]`æ¥è®¿é—®ç½‘ç»œçš„ä»»ä¸€å±‚ã€‚ç´¢å¼•0è¡¨ç¤ºéšè—å±‚ä¸º`Sequential`å®ä¾‹æœ€å…ˆæ·»åŠ çš„å±‚ã€‚

``` python
for name, param in net[0].named_parameters():
    print(name, param.size(), type(param))
```
è¾“å‡ºï¼š
```
weight torch.Size([3, 4]) <class 'torch.nn.parameter.Parameter'>
bias torch.Size([3]) <class 'torch.nn.parameter.Parameter'>
```
å› ä¸ºè¿™é‡Œæ˜¯å•å±‚çš„æ‰€ä»¥æ²¡æœ‰äº†å±‚æ•°ç´¢å¼•çš„å‰ç¼€ã€‚å¦å¤–è¿”å›çš„`param`çš„ç±»å‹ä¸º`torch.nn.parameter.Parameter`ï¼Œå…¶å®è¿™æ˜¯`Tensor`çš„å­ç±»ï¼Œå’Œ`Tensor`ä¸åŒçš„æ˜¯å¦‚æœä¸€ä¸ª`Tensor`æ˜¯`Parameter`ï¼Œé‚£ä¹ˆå®ƒä¼šè‡ªåŠ¨è¢«æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨é‡Œï¼Œæ¥çœ‹ä¸‹é¢è¿™ä¸ªä¾‹å­ã€‚
``` python
class MyModel(nn.Module):
    def __init__(self, **kwargs):
        super(MyModel, self).__init__(**kwargs)
        self.weight1 = nn.Parameter(torch.rand(20, 20))
        self.weight2 = torch.rand(20, 20)
    def forward(self, x):
        pass
    
n = MyModel()
for name, param in n.named_parameters():
    print(name)
```
è¾“å‡º:
```
weight1
```
ä¸Šé¢çš„ä»£ç ä¸­`weight1`åœ¨å‚æ•°åˆ—è¡¨ä¸­ä½†æ˜¯`weight2`å´æ²¡åœ¨å‚æ•°åˆ—è¡¨ä¸­ã€‚

å› ä¸º`Parameter`æ˜¯`Tensor`ï¼Œå³`Tensor`æ‹¥æœ‰çš„å±æ€§å®ƒéƒ½æœ‰ï¼Œæ¯”å¦‚å¯ä»¥æ ¹æ®`data`æ¥è®¿é—®å‚æ•°æ•°å€¼ï¼Œç”¨`grad`æ¥è®¿é—®å‚æ•°æ¢¯åº¦ã€‚
``` python
weight_0 = list(net[0].parameters())[0]
print(weight_0.data)
print(weight_0.grad) # åå‘ä¼ æ’­å‰æ¢¯åº¦ä¸ºNone
Y.backward()
print(weight_0.grad)
```
è¾“å‡ºï¼š
```
tensor([[ 0.2719, -0.0898, -0.2462,  0.0655],
        [-0.4669, -0.2703,  0.3230,  0.2067],
        [-0.2708,  0.1171, -0.0995,  0.3913]])
None
tensor([[-0.2281, -0.0653, -0.1646, -0.2569],
        [-0.1916, -0.0549, -0.1382, -0.2158],
        [ 0.0000,  0.0000,  0.0000,  0.0000]])
```

###  åˆå§‹åŒ–æ¨¡å‹å‚æ•°

æˆ‘ä»¬åœ¨3.15èŠ‚ï¼ˆæ•°å€¼ç¨³å®šæ€§å’Œæ¨¡å‹åˆå§‹åŒ–ï¼‰ä¸­æåˆ°äº†PyTorchä¸­`nn.Module`çš„æ¨¡å—å‚æ•°éƒ½é‡‡å–äº†è¾ƒä¸ºåˆç†çš„åˆå§‹åŒ–ç­–ç•¥ï¼ˆä¸åŒç±»å‹çš„layerå…·ä½“é‡‡æ ·çš„å“ªä¸€ç§åˆå§‹åŒ–æ–¹æ³•çš„å¯å‚è€ƒ[æºä»£ç ](https://github.com/pytorch/pytorch/tree/master/torch/nn/modules)ï¼‰ã€‚ä½†æˆ‘ä»¬ç»å¸¸éœ€è¦ä½¿ç”¨å…¶ä»–æ–¹æ³•æ¥åˆå§‹åŒ–æƒé‡ã€‚PyTorchçš„`init`æ¨¡å—é‡Œæä¾›äº†å¤šç§é¢„è®¾çš„åˆå§‹åŒ–æ–¹æ³•ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­ä¸­ï¼Œæˆ‘ä»¬å°†æƒé‡å‚æ•°åˆå§‹åŒ–æˆå‡å€¼ä¸º0ã€æ ‡å‡†å·®ä¸º0.01çš„æ­£æ€åˆ†å¸ƒéšæœºæ•°ï¼Œå¹¶ä¾ç„¶å°†åå·®å‚æ•°æ¸…é›¶ã€‚

``` python
for name, param in net.named_parameters():
    if 'weight' in name:
        init.normal_(param, mean=0, std=0.01)
        print(name, param.data)
```
è¾“å‡ºï¼š
```
0.weight tensor([[ 0.0030,  0.0094,  0.0070, -0.0010],
        [ 0.0001,  0.0039,  0.0105, -0.0126],
        [ 0.0105, -0.0135, -0.0047, -0.0006]])
2.weight tensor([[-0.0074,  0.0051,  0.0066]])
```

ä¸‹é¢ä½¿ç”¨å¸¸æ•°æ¥åˆå§‹åŒ–æƒé‡å‚æ•°ã€‚

``` python
for name, param in net.named_parameters():
    if 'bias' in name:
        init.constant_(param, val=0)
        print(name, param.data)
```
è¾“å‡ºï¼š
```
0.bias tensor([0., 0., 0.])
2.bias tensor([0.])
```

###  è‡ªå®šä¹‰åˆå§‹åŒ–æ–¹æ³•

æœ‰æ—¶å€™æˆ‘ä»¬éœ€è¦çš„åˆå§‹åŒ–æ–¹æ³•å¹¶æ²¡æœ‰åœ¨`init`æ¨¡å—ä¸­æä¾›ã€‚è¿™æ—¶ï¼Œå¯ä»¥å®ç°ä¸€ä¸ªåˆå§‹åŒ–æ–¹æ³•ï¼Œä»è€Œèƒ½å¤Ÿåƒä½¿ç”¨å…¶ä»–åˆå§‹åŒ–æ–¹æ³•é‚£æ ·ä½¿ç”¨å®ƒã€‚åœ¨è¿™ä¹‹å‰æˆ‘ä»¬å…ˆæ¥çœ‹çœ‹PyTorchæ˜¯æ€ä¹ˆå®ç°è¿™äº›åˆå§‹åŒ–æ–¹æ³•çš„ï¼Œä¾‹å¦‚`torch.nn.init.normal_`ï¼š
``` python
def normal_(tensor, mean=0, std=1):
    with torch.no_grad():
        return tensor.normal_(mean, std)
```
å¯ä»¥çœ‹åˆ°è¿™å°±æ˜¯ä¸€ä¸ªinplaceæ”¹å˜`Tensor`å€¼çš„å‡½æ•°ï¼Œè€Œä¸”è¿™ä¸ªè¿‡ç¨‹æ˜¯ä¸è®°å½•æ¢¯åº¦çš„ã€‚
ç±»ä¼¼çš„æˆ‘ä»¬æ¥å®ç°ä¸€ä¸ªè‡ªå®šä¹‰çš„åˆå§‹åŒ–æ–¹æ³•ã€‚åœ¨ä¸‹é¢çš„ä¾‹å­é‡Œï¼Œæˆ‘ä»¬ä»¤æƒé‡æœ‰ä¸€åŠæ¦‚ç‡åˆå§‹åŒ–ä¸º0ï¼Œæœ‰å¦ä¸€åŠæ¦‚ç‡åˆå§‹åŒ–ä¸º$[-10,-5]$å’Œ$[5,10]$ä¸¤ä¸ªåŒºé—´é‡Œå‡åŒ€åˆ†å¸ƒçš„éšæœºæ•°ã€‚

``` python
def init_weight_(tensor):
    with torch.no_grad():
        tensor.uniform_(-10, 10)
        tensor *= (tensor.abs() >= 5).float()

for name, param in net.named_parameters():
    if 'weight' in name:
        init_weight_(param)
        print(name, param.data)
```
è¾“å‡ºï¼š
```
0.weight tensor([[ 7.0403,  0.0000, -9.4569,  7.0111],
        [-0.0000, -0.0000,  0.0000,  0.0000],
        [ 9.8063, -0.0000,  0.0000, -9.7993]])
2.weight tensor([[-5.8198,  7.7558, -5.0293]])
```

æ­¤å¤–ï¼Œå‚è€ƒ2.3.2èŠ‚ï¼Œæˆ‘ä»¬è¿˜å¯ä»¥é€šè¿‡æ”¹å˜è¿™äº›å‚æ•°çš„`data`æ¥æ”¹å†™æ¨¡å‹å‚æ•°å€¼åŒæ—¶ä¸ä¼šå½±å“æ¢¯åº¦:
``` python
for name, param in net.named_parameters():
    if 'bias' in name:
        param.data += 1
        print(name, param.data)
```
è¾“å‡ºï¼š
```
0.bias tensor([1., 1., 1.])
2.bias tensor([1.])
```

###  å…±äº«æ¨¡å‹å‚æ•°

åœ¨æœ‰äº›æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¸Œæœ›åœ¨å¤šä¸ªå±‚ä¹‹é—´å…±äº«æ¨¡å‹å‚æ•°ã€‚4.1.3èŠ‚æåˆ°äº†å¦‚ä½•å…±äº«æ¨¡å‹å‚æ•°: `Module`ç±»çš„`forward`å‡½æ•°é‡Œå¤šæ¬¡è°ƒç”¨åŒä¸€ä¸ªå±‚ã€‚æ­¤å¤–ï¼Œå¦‚æœæˆ‘ä»¬ä¼ å…¥`Sequential`çš„æ¨¡å—æ˜¯åŒä¸€ä¸ª`Module`å®ä¾‹çš„è¯å‚æ•°ä¹Ÿæ˜¯å…±äº«çš„ï¼Œä¸‹é¢æ¥çœ‹ä¸€ä¸ªä¾‹å­: 

``` python
linear = nn.Linear(1, 1, bias=False)
net = nn.Sequential(linear, linear) 
print(net)
for name, param in net.named_parameters():
    init.constant_(param, val=3)
    print(name, param.data)
```
è¾“å‡ºï¼š
```
Sequential(
  (0): Linear(in_features=1, out_features=1, bias=False)
  (1): Linear(in_features=1, out_features=1, bias=False)
)
0.weight tensor([[3.]])
```

åœ¨å†…å­˜ä¸­ï¼Œè¿™ä¸¤ä¸ªçº¿æ€§å±‚å…¶å®ä¸€ä¸ªå¯¹è±¡:
``` python
print(id(net[0]) == id(net[1]))
print(id(net[0].weight) == id(net[1].weight))
```
è¾“å‡º:
```
True
True
```

å› ä¸ºæ¨¡å‹å‚æ•°é‡ŒåŒ…å«äº†æ¢¯åº¦ï¼Œæ‰€ä»¥åœ¨åå‘ä¼ æ’­è®¡ç®—æ—¶ï¼Œè¿™äº›å…±äº«çš„å‚æ•°çš„æ¢¯åº¦æ˜¯ç´¯åŠ çš„:
``` python
x = torch.ones(1, 1)
y = net(x).sum()
print(y)
y.backward()
print(net[0].weight.grad) # å•æ¬¡æ¢¯åº¦æ˜¯3ï¼Œä¸¤æ¬¡æ‰€ä»¥å°±æ˜¯6
```
è¾“å‡º:
```
tensor(9., grad_fn=<SumBackward0>)
tensor([[6.]])
```

`å°ç»“`

* æœ‰å¤šç§æ–¹æ³•æ¥è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«æ¨¡å‹å‚æ•°ã€‚
* å¯ä»¥è‡ªå®šä¹‰åˆå§‹åŒ–æ–¹æ³•ã€‚

-----------
> æ³¨ï¼šæœ¬èŠ‚ä¸åŸä¹¦æ­¤èŠ‚æœ‰ä¸€äº›ä¸åŒï¼Œ[åŸä¹¦ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/parameters.html)

##  æ¨¡å‹å‚æ•°çš„å»¶ååˆå§‹åŒ–

ç”±äºä½¿ç”¨Gluonåˆ›å»ºçš„å…¨è¿æ¥å±‚çš„æ—¶å€™ä¸éœ€è¦æŒ‡å®šè¾“å…¥ä¸ªæ•°ã€‚æ‰€ä»¥å½“è°ƒç”¨`initialize`å‡½æ•°æ—¶ï¼Œç”±äºéšè—å±‚è¾“å…¥ä¸ªæ•°ä¾ç„¶æœªçŸ¥ï¼Œç³»ç»Ÿä¹Ÿæ— æ³•å¾—çŸ¥è¯¥å±‚æƒé‡å‚æ•°çš„å½¢çŠ¶ã€‚åªæœ‰åœ¨å½“å½¢çŠ¶å·²çŸ¥çš„è¾“å…¥`X`ä¼ è¿›ç½‘ç»œåšå‰å‘è®¡ç®—`net(X)`æ—¶ï¼Œç³»ç»Ÿæ‰æ¨æ–­å‡ºè¯¥å±‚çš„æƒé‡å‚æ•°å½¢çŠ¶ä¸ºå¤šå°‘ï¼Œæ­¤æ—¶æ‰è¿›è¡ŒçœŸæ­£çš„åˆå§‹åŒ–æ“ä½œã€‚ä½†æ˜¯ä½¿ç”¨PyTorchåœ¨å®šä¹‰æ¨¡å‹çš„æ—¶å€™å°±è¦æŒ‡å®šè¾“å…¥çš„å½¢çŠ¶ï¼Œæ‰€ä»¥ä¹Ÿå°±ä¸å­˜åœ¨è¿™ä¸ªé—®é¢˜äº†ï¼Œæ‰€ä»¥æœ¬èŠ‚ç•¥ã€‚æœ‰å…´è¶£çš„å¯ä»¥å»çœ‹çœ‹åŸæ–‡ï¼Œ[ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/deferred-init.html)ã€‚

##  è‡ªå®šä¹‰å±‚

æ·±åº¦å­¦ä¹ çš„ä¸€ä¸ªé­…åŠ›åœ¨äºç¥ç»ç½‘ç»œä¸­å„å¼å„æ ·çš„å±‚ï¼Œä¾‹å¦‚å…¨è¿æ¥å±‚å’Œåé¢ç« èŠ‚ä¸­å°†è¦ä»‹ç»çš„å·ç§¯å±‚ã€æ± åŒ–å±‚ä¸å¾ªç¯å±‚ã€‚è™½ç„¶PyTorchæä¾›äº†å¤§é‡å¸¸ç”¨çš„å±‚ï¼Œä½†æœ‰æ—¶å€™æˆ‘ä»¬ä¾ç„¶å¸Œæœ›è‡ªå®šä¹‰å±‚ã€‚æœ¬èŠ‚å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨`Module`æ¥è‡ªå®šä¹‰å±‚ï¼Œä»è€Œå¯ä»¥è¢«é‡å¤è°ƒç”¨ã€‚

###  ä¸å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚

æˆ‘ä»¬å…ˆä»‹ç»å¦‚ä½•å®šä¹‰ä¸€ä¸ªä¸å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ã€‚äº‹å®ä¸Šï¼Œè¿™å’Œ4.1èŠ‚ï¼ˆæ¨¡å‹æ„é€ ï¼‰ä¸­ä»‹ç»çš„ä½¿ç”¨`Module`ç±»æ„é€ æ¨¡å‹ç±»ä¼¼ã€‚ä¸‹é¢çš„`CenteredLayer`ç±»é€šè¿‡ç»§æ‰¿`Module`ç±»è‡ªå®šä¹‰äº†ä¸€ä¸ªå°†è¾“å…¥å‡æ‰å‡å€¼åè¾“å‡ºçš„å±‚ï¼Œå¹¶å°†å±‚çš„è®¡ç®—å®šä¹‰åœ¨äº†`forward`å‡½æ•°é‡Œã€‚è¿™ä¸ªå±‚é‡Œä¸å«æ¨¡å‹å‚æ•°ã€‚

``` python
import torch
from torch import nn

class CenteredLayer(nn.Module):
    def __init__(self, **kwargs):
        super(CenteredLayer, self).__init__(**kwargs)
    def forward(self, x):
        return x - x.mean()
```

æˆ‘ä»¬å¯ä»¥å®ä¾‹åŒ–è¿™ä¸ªå±‚ï¼Œç„¶ååšå‰å‘è®¡ç®—ã€‚

``` python
layer = CenteredLayer()
layer(torch.tensor([1, 2, 3, 4, 5], dtype=torch.float))
```
è¾“å‡ºï¼š
```
tensor([-2., -1.,  0.,  1.,  2.])
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ç”¨å®ƒæ¥æ„é€ æ›´å¤æ‚çš„æ¨¡å‹ã€‚

``` python
net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())
```

ä¸‹é¢æ‰“å°è‡ªå®šä¹‰å±‚å„ä¸ªè¾“å‡ºçš„å‡å€¼ã€‚å› ä¸ºå‡å€¼æ˜¯æµ®ç‚¹æ•°ï¼Œæ‰€ä»¥å®ƒçš„å€¼æ˜¯ä¸€ä¸ªå¾ˆæ¥è¿‘0çš„æ•°ã€‚

``` python
y = net(torch.rand(4, 8))
y.mean().item()
```
è¾“å‡ºï¼š
```
0.0
```

###  å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚

æˆ‘ä»¬è¿˜å¯ä»¥è‡ªå®šä¹‰å«æ¨¡å‹å‚æ•°çš„è‡ªå®šä¹‰å±‚ã€‚å…¶ä¸­çš„æ¨¡å‹å‚æ•°å¯ä»¥é€šè¿‡è®­ç»ƒå­¦å‡ºã€‚

åœ¨4.2èŠ‚ï¼ˆæ¨¡å‹å‚æ•°çš„è®¿é—®ã€åˆå§‹åŒ–å’Œå…±äº«ï¼‰ä¸­ä»‹ç»äº†`Parameter`ç±»å…¶å®æ˜¯`Tensor`çš„å­ç±»ï¼Œå¦‚æœä¸€ä¸ª`Tensor`æ˜¯`Parameter`ï¼Œé‚£ä¹ˆå®ƒä¼šè‡ªåŠ¨è¢«æ·»åŠ åˆ°æ¨¡å‹çš„å‚æ•°åˆ—è¡¨é‡Œã€‚æ‰€ä»¥åœ¨è‡ªå®šä¹‰å«æ¨¡å‹å‚æ•°çš„å±‚æ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å°†å‚æ•°å®šä¹‰æˆ`Parameter`ï¼Œé™¤äº†åƒ4.2.1èŠ‚é‚£æ ·ç›´æ¥å®šä¹‰æˆ`Parameter`ç±»å¤–ï¼Œè¿˜å¯ä»¥ä½¿ç”¨`ParameterList`å’Œ`ParameterDict`åˆ†åˆ«å®šä¹‰å‚æ•°çš„åˆ—è¡¨å’Œå­—å…¸ã€‚

`ParameterList`æ¥æ”¶ä¸€ä¸ª`Parameter`å®ä¾‹çš„åˆ—è¡¨ä½œä¸ºè¾“å…¥ç„¶åå¾—åˆ°ä¸€ä¸ªå‚æ•°åˆ—è¡¨ï¼Œä½¿ç”¨çš„æ—¶å€™å¯ä»¥ç”¨ç´¢å¼•æ¥è®¿é—®æŸä¸ªå‚æ•°ï¼Œå¦å¤–ä¹Ÿå¯ä»¥ä½¿ç”¨`append`å’Œ`extend`åœ¨åˆ—è¡¨åé¢æ–°å¢å‚æ•°ã€‚
``` python
class MyDense(nn.Module):
    def __init__(self):
        super(MyDense, self).__init__()
        self.params = nn.ParameterList([nn.Parameter(torch.randn(4, 4)) for i in range(3)])
        self.params.append(nn.Parameter(torch.randn(4, 1)))

    def forward(self, x):
        for i in range(len(self.params)):
            x = torch.mm(x, self.params[i])
        return x
net = MyDense()
print(net)
```
è¾“å‡ºï¼š
```
MyDense(
  (params): ParameterList(
      (0): Parameter containing: [torch.FloatTensor of size 4x4]
      (1): Parameter containing: [torch.FloatTensor of size 4x4]
      (2): Parameter containing: [torch.FloatTensor of size 4x4]
      (3): Parameter containing: [torch.FloatTensor of size 4x1]
  )
)
```
è€Œ`ParameterDict`æ¥æ”¶ä¸€ä¸ª`Parameter`å®ä¾‹çš„å­—å…¸ä½œä¸ºè¾“å…¥ç„¶åå¾—åˆ°ä¸€ä¸ªå‚æ•°å­—å…¸ï¼Œç„¶åå¯ä»¥æŒ‰ç…§å­—å…¸çš„è§„åˆ™ä½¿ç”¨äº†ã€‚ä¾‹å¦‚ä½¿ç”¨`update()`æ–°å¢å‚æ•°ï¼Œä½¿ç”¨`keys()`è¿”å›æ‰€æœ‰é”®å€¼ï¼Œä½¿ç”¨`items()`è¿”å›æ‰€æœ‰é”®å€¼å¯¹ç­‰ç­‰ï¼Œå¯å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://pytorch.org/docs/stable/nn.html#parameterdict)ã€‚

``` python
class MyDictDense(nn.Module):
    def __init__(self):
        super(MyDictDense, self).__init__()
        self.params = nn.ParameterDict({
                'linear1': nn.Parameter(torch.randn(4, 4)),
                'linear2': nn.Parameter(torch.randn(4, 1))
        })
        self.params.update({'linear3': nn.Parameter(torch.randn(4, 2))}) # æ–°å¢

    def forward(self, x, choice='linear1'):
        return torch.mm(x, self.params[choice])

net = MyDictDense()
print(net)
```
è¾“å‡ºï¼š
```
MyDictDense(
  (params): ParameterDict(
      (linear1): Parameter containing: [torch.FloatTensor of size 4x4]
      (linear2): Parameter containing: [torch.FloatTensor of size 4x1]
      (linear3): Parameter containing: [torch.FloatTensor of size 4x2]
  )
)
```
è¿™æ ·å°±å¯ä»¥æ ¹æ®ä¼ å…¥çš„é”®å€¼æ¥è¿›è¡Œä¸åŒçš„å‰å‘ä¼ æ’­ï¼š
``` python
x = torch.ones(1, 4)
print(net(x, 'linear1'))
print(net(x, 'linear2'))
print(net(x, 'linear3'))
```
è¾“å‡ºï¼š
```
tensor([[1.5082, 1.5574, 2.1651, 1.2409]], grad_fn=<MmBackward>)
tensor([[-0.8783]], grad_fn=<MmBackward>)
tensor([[ 2.2193, -1.6539]], grad_fn=<MmBackward>)
```

æˆ‘ä»¬ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰å±‚æ„é€ æ¨¡å‹ã€‚å®ƒå’ŒPyTorchçš„å…¶ä»–å±‚åœ¨ä½¿ç”¨ä¸Šå¾ˆç±»ä¼¼ã€‚

``` python
net = nn.Sequential(
    MyDictDense(),
    MyListDense(),
)
print(net)
print(net(x))
```
è¾“å‡ºï¼š
```
Sequential(
  (0): MyDictDense(
    (params): ParameterDict(
        (linear1): Parameter containing: [torch.FloatTensor of size 4x4]
        (linear2): Parameter containing: [torch.FloatTensor of size 4x1]
        (linear3): Parameter containing: [torch.FloatTensor of size 4x2]
    )
  )
  (1): MyListDense(
    (params): ParameterList(
        (0): Parameter containing: [torch.FloatTensor of size 4x4]
        (1): Parameter containing: [torch.FloatTensor of size 4x4]
        (2): Parameter containing: [torch.FloatTensor of size 4x4]
        (3): Parameter containing: [torch.FloatTensor of size 4x1]
    )
  )
)
tensor([[-101.2394]], grad_fn=<MmBackward>)
```

`å°ç»“`

* å¯ä»¥é€šè¿‡`Module`ç±»è‡ªå®šä¹‰ç¥ç»ç½‘ç»œä¸­çš„å±‚ï¼Œä»è€Œå¯ä»¥è¢«é‡å¤è°ƒç”¨ã€‚


-----------
> æ³¨ï¼šæœ¬èŠ‚ä¸åŸä¹¦æ­¤èŠ‚æœ‰ä¸€äº›ä¸åŒï¼Œ[åŸä¹¦ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/custom-layer.html)

##  è¯»å–å’Œå­˜å‚¨

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä»‹ç»äº†å¦‚ä½•å¤„ç†æ•°æ®ä»¥åŠå¦‚ä½•æ„å»ºã€è®­ç»ƒå’Œæµ‹è¯•æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚ç„¶è€Œåœ¨å®é™…ä¸­ï¼Œæˆ‘ä»¬æœ‰æ—¶éœ€è¦æŠŠè®­ç»ƒå¥½çš„æ¨¡å‹éƒ¨ç½²åˆ°å¾ˆå¤šä¸åŒçš„è®¾å¤‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬å¯ä»¥æŠŠå†…å­˜ä¸­è®­ç»ƒå¥½çš„æ¨¡å‹å‚æ•°å­˜å‚¨åœ¨ç¡¬ç›˜ä¸Šä¾›åç»­è¯»å–ä½¿ç”¨ã€‚

###  è¯»å†™`Tensor`

æˆ‘ä»¬å¯ä»¥ç›´æ¥ä½¿ç”¨`save`å‡½æ•°å’Œ`load`å‡½æ•°åˆ†åˆ«å­˜å‚¨å’Œè¯»å–`Tensor`ã€‚`save`ä½¿ç”¨Pythonçš„pickleå®ç”¨ç¨‹åºå°†å¯¹è±¡è¿›è¡Œåºåˆ—åŒ–ï¼Œç„¶åå°†åºåˆ—åŒ–çš„å¯¹è±¡ä¿å­˜åˆ°diskï¼Œä½¿ç”¨`save`å¯ä»¥ä¿å­˜å„ç§å¯¹è±¡,åŒ…æ‹¬æ¨¡å‹ã€å¼ é‡å’Œå­—å…¸ç­‰ã€‚è€Œ`load`ä½¿ç”¨pickle unpickleå·¥å…·å°†pickleçš„å¯¹è±¡æ–‡ä»¶ååºåˆ—åŒ–ä¸ºå†…å­˜ã€‚

ä¸‹é¢çš„ä¾‹å­åˆ›å»ºäº†`Tensor`å˜é‡`x`ï¼Œå¹¶å°†å…¶å­˜åœ¨æ–‡ä»¶ååŒä¸º`x.pt`çš„æ–‡ä»¶é‡Œã€‚

``` python
import torch
from torch import nn

x = torch.ones(3)
torch.save(x, 'x.pt')
```

ç„¶åæˆ‘ä»¬å°†æ•°æ®ä»å­˜å‚¨çš„æ–‡ä»¶è¯»å›å†…å­˜ã€‚

``` python
x2 = torch.load('x.pt')
x2
```
è¾“å‡ºï¼š
```
tensor([1., 1., 1.])
```

æˆ‘ä»¬è¿˜å¯ä»¥å­˜å‚¨ä¸€ä¸ª`Tensor`åˆ—è¡¨å¹¶è¯»å›å†…å­˜ã€‚

``` python
y = torch.zeros(4)
torch.save([x, y], 'xy.pt')
xy_list = torch.load('xy.pt')
xy_list
```
è¾“å‡ºï¼š
```
[tensor([1., 1., 1.]), tensor([0., 0., 0., 0.])]
```

å­˜å‚¨å¹¶è¯»å–ä¸€ä¸ªä»å­—ç¬¦ä¸²æ˜ å°„åˆ°`Tensor`çš„å­—å…¸ã€‚

``` python
torch.save({'x': x, 'y': y}, 'xy_dict.pt')
xy = torch.load('xy_dict.pt')
xy
```
è¾“å‡ºï¼š
```
{'x': tensor([1., 1., 1.]), 'y': tensor([0., 0., 0., 0.])}
```

###  è¯»å†™æ¨¡å‹

####  `state_dict`

åœ¨PyTorchä¸­ï¼Œ`Module`çš„å¯å­¦ä¹ å‚æ•°(å³æƒé‡å’Œåå·®)ï¼Œæ¨¡å—æ¨¡å‹åŒ…å«åœ¨å‚æ•°ä¸­(é€šè¿‡`model.parameters()`è®¿é—®)ã€‚`state_dict`æ˜¯ä¸€ä¸ªä»å‚æ•°åç§°éšå°„åˆ°å‚æ•°`Tesnor`çš„å­—å…¸å¯¹è±¡ã€‚
``` python
class MLP(nn.Module):
    def __init__(self):
        super(MLP, self).__init__()
        self.hidden = nn.Linear(3, 2)
        self.act = nn.ReLU()
        self.output = nn.Linear(2, 1)

    def forward(self, x):
        a = self.act(self.hidden(x))
        return self.output(a)

net = MLP()
net.state_dict()
```
è¾“å‡ºï¼š
```
OrderedDict([('hidden.weight', tensor([[ 0.2448,  0.1856, -0.5678],
                      [ 0.2030, -0.2073, -0.0104]])),
             ('hidden.bias', tensor([-0.3117, -0.4232])),
             ('output.weight', tensor([[-0.4556,  0.4084]])),
             ('output.bias', tensor([-0.3573]))])
```

æ³¨æ„ï¼Œåªæœ‰å…·æœ‰å¯å­¦ä¹ å‚æ•°çš„å±‚(å·ç§¯å±‚ã€çº¿æ€§å±‚ç­‰)æ‰æœ‰`state_dict`ä¸­çš„æ¡ç›®ã€‚ä¼˜åŒ–å™¨(`optim`)ä¹Ÿæœ‰ä¸€ä¸ª`state_dict`ï¼Œå…¶ä¸­åŒ…å«å…³äºä¼˜åŒ–å™¨çŠ¶æ€ä»¥åŠæ‰€ä½¿ç”¨çš„è¶…å‚æ•°çš„ä¿¡æ¯ã€‚
``` python
optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)
optimizer.state_dict()
```
è¾“å‡ºï¼š
```
{'param_groups': [{'dampening': 0,
   'lr': 0.001,
   'momentum': 0.9,
   'nesterov': False,
   'params': [4736167728, 4736166648, 4736167368, 4736165352],
   'weight_decay': 0}],
 'state': {}}
```

####  ä¿å­˜å’ŒåŠ è½½æ¨¡å‹

PyTorchä¸­ä¿å­˜å’ŒåŠ è½½è®­ç»ƒæ¨¡å‹æœ‰ä¸¤ç§å¸¸è§çš„æ–¹æ³•:
1. ä¿å­˜å’ŒåŠ è½½`state_dict`(æ¨èæ–¹å¼)

ä¿å­˜ï¼š
``` python
torch.save(model.state_dict(), PATH) # æ¨èçš„æ–‡ä»¶åç¼€åæ˜¯ptæˆ–pth
```
åŠ è½½ï¼š
``` python
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
```

2. ä¿å­˜å’ŒåŠ è½½æ•´ä¸ªæ¨¡å‹

ä¿å­˜ï¼š
``` python
torch.save(model, PATH)
```
åŠ è½½ï¼š
``` python
model = torch.load(PATH)
```

æˆ‘ä»¬é‡‡ç”¨æ¨èçš„æ–¹æ³•ä¸€æ¥å®éªŒä¸€ä¸‹:
``` python
X = torch.randn(2, 3)
Y = net(X)

PATH = "./net.pt"
torch.save(net.state_dict(), PATH)

net2 = MLP()
net2.load_state_dict(torch.load(PATH))
Y2 = net2(X)
Y2 == Y
```
è¾“å‡ºï¼š
```
tensor([[1],
        [1]], dtype=torch.uint8)
```

å› ä¸ºè¿™`net`å’Œ`net2`éƒ½æœ‰åŒæ ·çš„æ¨¡å‹å‚æ•°ï¼Œé‚£ä¹ˆå¯¹åŒä¸€ä¸ªè¾“å…¥`X`çš„è®¡ç®—ç»“æœå°†ä¼šæ˜¯ä¸€æ ·çš„ã€‚ä¸Šé¢çš„è¾“å‡ºä¹ŸéªŒè¯äº†è¿™ä¸€ç‚¹ã€‚

æ­¤å¤–ï¼Œè¿˜æœ‰ä¸€äº›å…¶ä»–ä½¿ç”¨åœºæ™¯ï¼Œä¾‹å¦‚GPUä¸CPUä¹‹é—´çš„æ¨¡å‹ä¿å­˜ä¸è¯»å–ã€ä½¿ç”¨å¤šå—GPUçš„æ¨¡å‹çš„å­˜å‚¨ç­‰ç­‰ï¼Œä½¿ç”¨çš„æ—¶å€™å¯ä»¥å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](https://pytorch.org/tutorials/beginner/saving_loading_models.html)ã€‚

`å°ç»“`

* é€šè¿‡`save`å‡½æ•°å’Œ`load`å‡½æ•°å¯ä»¥å¾ˆæ–¹ä¾¿åœ°è¯»å†™`Tensor`ã€‚
* é€šè¿‡`save`å‡½æ•°å’Œ`load_state_dict`å‡½æ•°å¯ä»¥å¾ˆæ–¹ä¾¿åœ°è¯»å†™æ¨¡å‹çš„å‚æ•°ã€‚

-----------
> æ³¨ï¼šæœ¬èŠ‚ä¸åŸä¹¦æ­¤èŠ‚æœ‰ä¸€äº›ä¸åŒï¼Œ[åŸä¹¦ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/read-write.html)
>
> ##  GPUè®¡ç®—

åˆ°ç›®å‰ä¸ºæ­¢ï¼Œæˆ‘ä»¬ä¸€ç›´åœ¨ä½¿ç”¨CPUè®¡ç®—ã€‚å¯¹å¤æ‚çš„ç¥ç»ç½‘ç»œå’Œå¤§è§„æ¨¡çš„æ•°æ®æ¥è¯´ï¼Œä½¿ç”¨CPUæ¥è®¡ç®—å¯èƒ½ä¸å¤Ÿé«˜æ•ˆã€‚åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨å•å—NVIDIA GPUæ¥è®¡ç®—ã€‚æ‰€ä»¥éœ€è¦ç¡®ä¿å·²ç»å®‰è£…å¥½äº†PyTorch GPUç‰ˆæœ¬ã€‚å‡†å¤‡å·¥ä½œéƒ½å®Œæˆåï¼Œä¸‹é¢å°±å¯ä»¥é€šè¿‡`nvidia-smi`å‘½ä»¤æ¥æŸ¥çœ‹æ˜¾å¡ä¿¡æ¯äº†ã€‚

``` python
!nvidia-smi  # å¯¹Linux/macOSç”¨æˆ·æœ‰æ•ˆ
```
è¾“å‡ºï¼š
```
Sun Mar 17 14:59:57 2019       
+-----------------------------------------------------------------------------+
| NVIDIA-SMI 390.48                 Driver Version: 390.48                    |
|-------------------------------+----------------------+----------------------+
| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
|===============================+======================+======================|
|   0  GeForce GTX 1050    Off  | 00000000:01:00.0 Off |                  N/A |
| 20%   36C    P5    N/A /  75W |   1223MiB /  2000MiB |      0%      Default |
+-------------------------------+----------------------+----------------------+
                                                                               
+-----------------------------------------------------------------------------+
| Processes:                                                       GPU Memory |
|  GPU       PID   Type   Process name                             Usage      |
|=============================================================================|
|    0      1235      G   /usr/lib/xorg/Xorg                           434MiB |
|    0      2095      G   compiz                                       163MiB |
|    0      2660      G   /opt/teamviewer/tv_bin/TeamViewer              5MiB |
|    0      4166      G   /proc/self/exe                               416MiB |
|    0     13274      C   /home/tss/anaconda3/bin/python               191MiB |
+-----------------------------------------------------------------------------+
```
å¯ä»¥çœ‹åˆ°æˆ‘è¿™é‡Œåªæœ‰ä¸€å—GTX 1050ï¼Œæ˜¾å­˜ä¸€å…±åªæœ‰2000Mï¼ˆå¤ªæƒ¨äº†ğŸ˜­ï¼‰ã€‚

###  è®¡ç®—è®¾å¤‡

PyTorchå¯ä»¥æŒ‡å®šç”¨æ¥å­˜å‚¨å’Œè®¡ç®—çš„è®¾å¤‡ï¼Œå¦‚ä½¿ç”¨å†…å­˜çš„CPUæˆ–è€…ä½¿ç”¨æ˜¾å­˜çš„GPUã€‚é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šå°†æ•°æ®åˆ›å»ºåœ¨å†…å­˜ï¼Œç„¶ååˆ©ç”¨CPUæ¥è®¡ç®—ã€‚

ç”¨`torch.cuda.is_available()`æŸ¥çœ‹GPUæ˜¯å¦å¯ç”¨:
``` python
import torch
from torch import nn

torch.cuda.is_available() # è¾“å‡º True
```

æŸ¥çœ‹GPUæ•°é‡ï¼š
``` python
torch.cuda.device_count() # è¾“å‡º 1
```
æŸ¥çœ‹å½“å‰GPUç´¢å¼•å·ï¼Œç´¢å¼•å·ä»0å¼€å§‹ï¼š
``` python
torch.cuda.current_device() # è¾“å‡º 0
```
æ ¹æ®ç´¢å¼•å·æŸ¥çœ‹GPUåå­—:
``` python
torch.cuda.get_device_name(0) # è¾“å‡º 'GeForce GTX 1050'
```

###  `Tensor`çš„GPUè®¡ç®—

é»˜è®¤æƒ…å†µä¸‹ï¼Œ`Tensor`ä¼šè¢«å­˜åœ¨å†…å­˜ä¸Šã€‚å› æ­¤ï¼Œä¹‹å‰æˆ‘ä»¬æ¯æ¬¡æ‰“å°`Tensor`çš„æ—¶å€™çœ‹ä¸åˆ°GPUç›¸å…³æ ‡è¯†ã€‚
``` python
x = torch.tensor([1, 2, 3])
x
```
è¾“å‡ºï¼š
```
tensor([1, 2, 3])
```
ä½¿ç”¨`.cuda()`å¯ä»¥å°†CPUä¸Šçš„`Tensor`è½¬æ¢ï¼ˆå¤åˆ¶ï¼‰åˆ°GPUä¸Šã€‚å¦‚æœæœ‰å¤šå—GPUï¼Œæˆ‘ä»¬ç”¨`.cuda(i)`æ¥è¡¨ç¤ºç¬¬ $i$ å—GPUåŠç›¸åº”çš„æ˜¾å­˜ï¼ˆ$i$ä»0å¼€å§‹ï¼‰ä¸”`cuda(0)`å’Œ`cuda()`ç­‰ä»·ã€‚
``` python
x = x.cuda(0)
x
```
è¾“å‡ºï¼š
```
tensor([1, 2, 3], device='cuda:0')
```

æˆ‘ä»¬å¯ä»¥é€šè¿‡`Tensor`çš„`device`å±æ€§æ¥æŸ¥çœ‹è¯¥`Tensor`æ‰€åœ¨çš„è®¾å¤‡ã€‚
```python
x.device
```
è¾“å‡ºï¼š
```
device(type='cuda', index=0)
```
æˆ‘ä»¬å¯ä»¥ç›´æ¥åœ¨åˆ›å»ºçš„æ—¶å€™å°±æŒ‡å®šè®¾å¤‡ã€‚
``` python
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

x = torch.tensor([1, 2, 3], device=device)
# or
x = torch.tensor([1, 2, 3]).to(device)
x
```
è¾“å‡ºï¼š
```
tensor([1, 2, 3], device='cuda:0')
```
å¦‚æœå¯¹åœ¨GPUä¸Šçš„æ•°æ®è¿›è¡Œè¿ç®—ï¼Œé‚£ä¹ˆç»“æœè¿˜æ˜¯å­˜æ”¾åœ¨GPUä¸Šã€‚
``` python
y = x**2
y
```
è¾“å‡ºï¼š
```
tensor([1, 4, 9], device='cuda:0')
```
éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œå­˜å‚¨åœ¨ä¸åŒä½ç½®ä¸­çš„æ•°æ®æ˜¯ä¸å¯ä»¥ç›´æ¥è¿›è¡Œè®¡ç®—çš„ã€‚å³å­˜æ”¾åœ¨CPUä¸Šçš„æ•°æ®ä¸å¯ä»¥ç›´æ¥ä¸å­˜æ”¾åœ¨GPUä¸Šçš„æ•°æ®è¿›è¡Œè¿ç®—ï¼Œä½äºä¸åŒGPUä¸Šçš„æ•°æ®ä¹Ÿæ˜¯ä¸èƒ½ç›´æ¥è¿›è¡Œè®¡ç®—çš„ã€‚
``` python
z = y + x.cpu()
```
ä¼šæŠ¥é”™:
```
RuntimeError: Expected object of type torch.cuda.LongTensor but found type torch.LongTensor for argument #3 'other'
```

###  æ¨¡å‹çš„GPUè®¡ç®—

åŒ`Tensor`ç±»ä¼¼ï¼ŒPyTorchæ¨¡å‹ä¹Ÿå¯ä»¥é€šè¿‡`.cuda`è½¬æ¢åˆ°GPUä¸Šã€‚æˆ‘ä»¬å¯ä»¥é€šè¿‡æ£€æŸ¥æ¨¡å‹çš„å‚æ•°çš„`device`å±æ€§æ¥æŸ¥çœ‹å­˜æ”¾æ¨¡å‹çš„è®¾å¤‡ã€‚

``` python
net = nn.Linear(3, 1)
list(net.parameters())[0].device
```
è¾“å‡ºï¼š
```
device(type='cpu')
```
å¯è§æ¨¡å‹åœ¨CPUä¸Šï¼Œå°†å…¶è½¬æ¢åˆ°GPUä¸Š:
``` python
net.cuda()
list(net.parameters())[0].device
```
è¾“å‡ºï¼š
```
device(type='cuda', index=0)
```

åŒæ ·çš„ï¼Œæˆ‘ä¹ˆéœ€è¦ä¿è¯æ¨¡å‹è¾“å…¥çš„`Tensor`å’Œæ¨¡å‹éƒ½åœ¨åŒä¸€è®¾å¤‡ä¸Šï¼Œå¦åˆ™ä¼šæŠ¥é”™ã€‚

``` python
x = torch.rand(2,3).cuda()
net(x)
```
è¾“å‡ºï¼š
```
tensor([[-0.5800],
        [-0.2995]], device='cuda:0', grad_fn=<ThAddmmBackward>)
```

`å°ç»“`

* PyTorchå¯ä»¥æŒ‡å®šç”¨æ¥å­˜å‚¨å’Œè®¡ç®—çš„è®¾å¤‡ï¼Œå¦‚ä½¿ç”¨å†…å­˜çš„CPUæˆ–è€…ä½¿ç”¨æ˜¾å­˜çš„GPUã€‚åœ¨é»˜è®¤æƒ…å†µä¸‹ï¼ŒPyTorchä¼šå°†æ•°æ®åˆ›å»ºåœ¨å†…å­˜ï¼Œç„¶ååˆ©ç”¨CPUæ¥è®¡ç®—ã€‚
* PyTorchè¦æ±‚è®¡ç®—çš„æ‰€æœ‰è¾“å…¥æ•°æ®éƒ½åœ¨å†…å­˜æˆ–åŒä¸€å—æ˜¾å¡çš„æ˜¾å­˜ä¸Šã€‚


-----------
> æ³¨ï¼šæœ¬èŠ‚ä¸åŸä¹¦æ­¤èŠ‚æœ‰ä¸€äº›ä¸åŒï¼Œ[åŸä¹¦ä¼ é€é—¨](https://zh.d2l.ai/chapter_deep-learning-computation/use-gpu.html)
