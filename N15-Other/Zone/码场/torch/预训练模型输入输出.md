# Bert

```python
from transformers import BertTokenizer, BertConfig, BertModel

from pprint import pprint

model_path = "E:/RC/model_hub/bert-base-chinese"

bertTokenizer = BertTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = bertTokenizer(sen, return_tensors='pt')
pprint(inputs.keys())
pprint(inputs)

print("=" * 100)
tokens = bertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)

print("=" * 100)
bertModel = BertModel.from_pretrained(model_path)
outputs = bertModel(**inputs)
# print(len(outputs))
print(outputs[0].shape, outputs[1].shape)



# ------------------------------------------------------------------------------------------------
print("+" * 100)
tokens_a = '我 爱 北 京 天 安 门'.split(' ')
tokens_b = '我 爱 打 英 雄 联 盟 啊 啊'.split(' ')

encode_dict = bertTokenizer.encode_plus(text=tokens_a,
                                        text_pair=tokens_b,
                                        max_length=20,
                                        padding=True,
                                        truncation=True,
                                        is_pretokenized=True,
                                        return_token_type_ids=True,
                                        return_attention_mask=True)
tokens = " ".join(['[CLS]'] + tokens_a + ['[SEP]'] + tokens_b + ['[SEP]'])
token_ids = encode_dict['input_ids']
attention_masks = encode_dict['attention_mask']
token_type_ids = encode_dict['token_type_ids']

print(tokens)
print("=" * 100)

print(token_ids)
print("=" * 100)

print(attention_masks)
print("=" * 100)

print(token_type_ids)
```



# Albert

```python
from transformers import BertTokenizer, AlbertModel

model_path = "E:/RC/model_hub/albert_chinese_tiny"

albertTokenizer = BertTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = albertTokenizer(sen, return_tensors='pt')
print(inputs)

print("=" * 100)
tokens = albertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)

print("=" * 100)
albertModel = AlbertModel.from_pretrained(model_path)
outputs = albertModel(**inputs)
# print(len(outputs))
print(outputs[0].shape, outputs[1].shape)

```





# Roberta

```python
from transformers import BertTokenizer, BertConfig, BertModel

model_path = "E:/RC/model_hub/chinese-roberta-wwm-ext"

robertTokenizer = BertTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = robertTokenizer(sen, return_tensors='pt')
print(inputs)

print("=" * 100)
tokens = robertTokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)

print("=" * 100)
robertModel = BertModel.from_pretrained(model_path)
outputs = robertModel(**inputs)
print(outputs)

print("=" * 100)
print(outputs[0].shape, outputs[1].shape)

```



# Macbert

```python
from transformers import AutoTokenizer, AutoModel

model_path = "E:/RC/model_hub/chinese-macbert-base"
mactokenizer = AutoTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = mactokenizer(sen, return_tensors='pt')
print(inputs)
print("=" * 100)

tokens = mactokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)
print("=" * 100)

macModel = AutoModel.from_pretrained(model_path)
outputs = macModel(**inputs)
# print(outputs)
print(outputs[0].shape)

```



# xlnet

```python
from transformers import AutoTokenizer, AutoModel

model_path = "E:/RC/model_hub/chinese-xlnet-base"

xlnettokenizer = AutoTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = xlnettokenizer(sen, return_tensors='pt')
print(inputs)
print("=" * 100)

tokens = xlnettokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)
print("=" * 100)

xlnetModel = AutoModel.from_pretrained(model_path)
outputs = xlnetModel(**inputs)
# print(outputs)
print(outputs[0].shape, len(outputs[1]))
```



# electra

```python
from transformers import AutoTokenizer, AutoModel

model_path = "E:/RC/model_hub/chinese-electra-180g-base-discriminator"

electratokenizer = AutoTokenizer.from_pretrained(model_path)

sen = 'Transformers提供了NLP领域大量state-of-art的 预训练语言模型结构的模型和调用框架。'
inputs = electratokenizer(sen, return_tensors='pt')
print(inputs)
print("=" * 100)

tokens = electratokenizer.convert_ids_to_tokens(inputs['input_ids'][0])
print(tokens)
print("=" * 100)

electraModel = AutoModel.from_pretrained(model_path)
outputs = electraModel(**inputs)
# print(outputs)
print(outputs[0].shape)
```

