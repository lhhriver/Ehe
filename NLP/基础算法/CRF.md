# 从随机场到线性链条件随机场

条件随机场(Conditional Random Fields, 以下简称CRF)是**给定一组输入序列条件下另一组输出序列的条件概率分布模型**，在自然语言处理中得到了广泛应用。本系列主要关注于CRF的特殊形式：`线性链`(Linear chain) CRF。本文关注与CRF的模型基础。

## 什么样的问题需要CRF模型

和HMM类似，在讨论CRF之前，我们来看看什么样的问题需要CRF模型。这里举一个简单的例子：

假设我们有Bob一天从早到晚的一系列照片，Bob想考考我们，要我们猜这一系列的每张照片对应的活动，比如: 工作的照片，吃饭的照片，唱歌的照片等等。一个比较直观的办法就是，我们找到Bob之前的日常生活的一系列照片，然后找Bob问清楚这些照片代表的活动标记，这样我们就可以用监督学习的方法来训练一个分类模型，比如逻辑回归，接着用模型去预测这一天的每张照片最可能的活动标记。

这种办法虽然是可行的，但是却忽略了一个重要的问题，就是这些照片之间的顺序其实是**有很大的时间顺序关系**的，而用上面的方法则会忽略这种关系。比如我们现在看到了一张Bob闭着嘴的照片，那么这张照片我们怎么标记Bob的活动呢？比较难去打标记。但是如果我们有Bob在这一张照片前一点点时间的照片的话，那么这张照片就好标记了。如果在时间序列上前一张的照片里Bob在吃饭，那么这张闭嘴的照片很有可能是在吃饭咀嚼。而如果在时间序列上前一张的照片里Bob在唱歌，那么这张闭嘴的照片很有可能是在唱歌。

为了让我们的分类器表现的更好，可以在标记数据的时候，可以**考虑相邻数据的标记信息**。这一点，是普通的分类器难以做到的。而这一块，也是CRF比较擅长的地方。

在实际应用中，自然语言处理中的词性标注(POS Tagging)就是非常适合CRF使用的地方。词性标注的目标是给出一个句子中每个词的词性（名词，动词，形容词等）。而这些词的词性往往和上下文的词的词性有关，因此，使用CRF来处理是很适合的，当然CRF不是唯一的选择，也有很多其他的词性标注方法。

## 从**随机场**到**马尔科夫随机场**

首先，我们来看看什么是随机场。随机场的名字取的很玄乎，其实理解起来不难。**随机场是由若干个位置组成的整体，当给每一个位置中按照某种分布随机赋予一个值之后，其全体就叫做`随机场`**。

还是举词性标注的例子：假如我们有一个十个词形成的句子需要做词性标注。这十个词每个词的词性可以在我们已知的词性集合（名词，动词...）中去选择。当我们为每个词选择完词性后，这就形成了一个随机场。

了解了随机场，我们再来看看**马尔科夫随机场**。马尔科夫随机场是随机场的特例，它假设随机场中某一个位置的赋值仅仅与和它相邻的位置的赋值有关，和与其不相邻的位置的赋值无关。

继续举十个词的句子词性标注的例子：如果我们假设所有词的词性只和它相邻的词的词性有关时，这个随机场就特化成一个马尔科夫随机场。比如第三个词的词性除了与自己本身的位置有关外，只与第二个词和第四个词的词性有关。

## 从马尔科夫随机场到**条件随机场**

理解了马尔科夫随机场，再理解CRF就容易了。**CRF是马尔科夫随机场的特例，它假设马尔科夫随机场中只有$X$和$Y$两种变量，$X$一般是给定的，而$Y$一般是在给定$X$的条件下我们的输出**。这样马尔科夫随机场就特化成了条件随机场。在我们十个词的句子词性标注的例子中，$X$是词，$Y$是词性。因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个CRF。

对于CRF，我们给出准确的数学语言描述：**设$X$与$Y$是随机变量，$P(Y|X)$是给定$X$时$Y$的条件概率分布，若随机变量$Y$构成的是一个马尔科夫随机场，则称条件概率分布$P(Y|X)$是条件随机场**。

## 从条件随机场到线性链条件随机场

注意在CRF的定义中，我们并没有要求$X$和$Y$有相同的结构。而实现中，我们一般都**假设$X$和$Y$有相同的结构**，即：

$$
X =(X_1,X_2,...X_n),\;\;Y=(Y_1,Y_2,...Y_n)
$$
我们一般考虑如下图所示的结构：**$X$和$Y$有相同的结构的CRF就构成了线性链条件随机场**(Linear chain Conditional Random Fields,以下简称 linear-CRF)。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/CRF-20201214-201033-151480.png)

在我们的十个词的句子的词性标记中，词有十个，词性也是十个，因此，如果我们假设它是一个马尔科夫随机场，那么它也就是一个linear-CRF。

我们再来看看 linear-CRF的**数学定义**：

设$X =(X_1,X_2,...X_n),\;\;Y=(Y_1,Y_2,...Y_n)$均为线性链表示的随机变量序列，在给定随机变量序列$X$的情况下，随机变量$Y$的条件概率分布$P(Y|X)$构成条件随机场，即满足马尔科夫性：

$$
P(Y_i|X,Y_1,Y_2,...Y_n) = P(Y_i|X,Y_{i-1},Y_{i+1})
$$
则称$P(Y|X)$为**线性链条件随机场**。

## 线性链条件随机场的参数化形式

对于上一节讲到的linear-CRF，我们如何将其转化为可以学习的机器学习模型呢？这是**通过特征函数和其权重系数来定义的**。什么是特征函数呢？在linear-CRF中，`特征函数`分为两类：

**第一类是定义在$Y$节点上的节点特征函数，这类特征函数只和当前节点有关**，记为：
$$
s_l(y_i, x,i),\;\; l =1,2,...L
$$


其中$L$是定义在该节点的节点特征函数的总个数，$i$是当前节点在序列的位置。 

**第二类是定义在$Y$上下文的局部特征函数，这类特征函数只和当前节点和上一个节点有关**，记为：
$$
t_k(y_{i-1},y_i, x,i),\;\; k =1,2,...K
$$
其中$K$是定义在该节点的局部特征函数的总个数，$i$是当前节点在序列的位置。之所以只有上下文相关的局部特征函数，没有不相邻节点之间的特征函数，是因为我们的linear-CRF满足马尔科夫性。

无论是节点特征函数还是局部特征函数，它们的取值只能是0或者1。即满足特征条件或者不满足特征条件。同时，我们可以为每个特征函数赋予一个权值，用以表达我们对这个特征函数的信任度。假设$t_k$的权重系数是$λ_k$,$s_l$的权重系数是$μ_l$,则linear-CRF由我们所有的$t_k,λ_k,s_l,μ_l$共同决定。

此时我们得到了linear-CRF的参数化形式如下：

$$
P(y|x) = \frac{1}{Z(x)}exp\Big(\sum\limits_{i,k} \lambda_kt_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big)
$$


其中，$Z(x)$为规范化因子：

$$
Z(x) =\sum\limits_{y} exp\Big(\sum\limits_{i,k} \lambda_kt_k(y_{i-1},y_i, x,i) +\sum\limits_{i,l}\mu_ls_l(y_i, x,i)\Big)
$$


回到特征函数本身，每个特征函数定义了一个linear-CRF的规则，则其系数定义了这个规则的可信度。所有的规则和其可信度一起构成了我们的linear-CRF的最终的条件概率分布。

## 线性链条件随机场实例

 　这里我们给出一个linear-CRF用于词性标注的实例，为了方便，我们简化了词性的种类。假设输入的都是三个词的句子，即$X=(X_1,X_2,X_3)$,输出的词性标记为$Y=(Y_1,Y_2,Y_3)$,其中$Y \in \{1(名词)，2(动词)\}$

这里只标记出取值为1的特征函数如下：

$$
t_1 =t_1(y_{i-1} = 1, y_i =2,x,i), i =2,3,\;\;\lambda_1=1
$$

$$
t_2 =t_2(y_1=1,y_2=1,x,2)\;\;\lambda_2=0.5
$$

$$
t_3 =t_3(y_2=2,y_3=1,x,3)\;\;\lambda_3=1
$$

$$
t_4 =t_4(y_1=2,y_2=1,x,2)\;\;\lambda_4=1
$$

$$
t_5 =t_5(y_2=2,y_3=2,x,3)\;\;\lambda_5=0.2
$$

$$
s_1 =s_1(y_1=1,x,1)\;\;\mu_1 =1
$$

$$
s_2 =s_2( y_i =2,x,i), i =1,2,\;\;\mu_2=0.5
$$

$$
s_3 =s_3( y_i =1,x,i), i =2,3,\;\;\mu_3=0.
$$

$$
s_4 =s_4(y_3=2,x,3)\;\;\mu_4 =0.5
$$

求标记(1,2,2)的非规范化概率。

利用linear-CRF的参数化公式，我们有：

$$
P(y|x) \propto exp\Big[\sum\limits_{k=1}^5\lambda_k\sum\limits_{i=2}^3t_k(y_{i-1},y_i, x,i) + \sum\limits_{l=1}^4\mu_l\sum\limits_{i=1}^3s_l(y_i, x,i) \Big]
$$


带入(1,2,2)我们有：

$$
P(y_1=1,y_2=2,y_3=2|x) \propto exp(3.2)
$$

##  线性链条件随机场的简化形式

在上几节里面，我们用$s_l$表示节点特征函数，用$t_k$表示局部特征函数，同时也用了不同的符号表示权重系数，导致表示起来比较麻烦。其实我们可以对特征函数稍加整理，将其统一起来。

假设我们在某一节点我们有$K1$个局部特征函数和$K2$个节点特征函数，总共有$K=K_1+K_2$个特征函数。我们用一个特征函数$f_k(y_{i-1},y_i, x,i)$来统一表示如下:

$$
f_k(y_{i-1},y_i, x,i)= \begin{cases} t_k(y_{i-1},y_i, x,i) & {k=1,2,...K_1}\\ s_l(y_i, x,i)& {k=K_1+l,\; l=1,2...,K_2} \end{cases}
$$


对$f_k(y_{i-1},y_i, x,i)$在各个序列位置求和得到：

$$
f_k(y,x) = \sum\limits_{i=1}^nf_k(y_{i-1},y_i, x,i)
$$


同时我们也统一$f_k(y_{i-1},y_i, x,i)$对应的权重系数$w_k$如下：

$$
w_k= \begin{cases} \lambda_k & {k=1,2,...K_1}\\ \mu_l & {k=K_1+l,\; l=1,2...,K_2} \end{cases}
$$


这样，我们的linear-CRF的参数化形式简化为：

$$
P(y|x) =  \frac{1}{Z(x)}exp\sum\limits_{k=1}^Kw_kf_k(y,x)
$$


其中，$Z(x)$为规范化因子：

$$
Z(x) =\sum\limits_{y}exp\sum\limits_{k=1}^Kw_kf_k(y,x)
$$


如果将上两式中的$wk$与$fk$的用向量表示，即:

$$
w=(w_1,w_2,...w_K)^T\;\;\; F(y,x) =(f_1(y,x),f_2(y,x),...f_K(y,x))^T
$$


则linear-CRF的参数化形式简化为内积形式如下：

$$
P_w(y|x) = \frac{exp(w \bullet F(y,x))}{Z_w(x)} = \frac{exp(w \bullet F(y,x))}{\sum\limits_{y}exp(w \bullet F(y,x))}
$$

## 线性链条件随机场的矩阵形式

将上一节统一后的linear-CRF公式加以整理，我们还可以将linear-CRF的参数化形式写成矩阵形式。为此我们定义一个$m×m$的矩阵$M$，mm为$y$所有可能的状态的取值个数。$M$定义如下：

$$
M_i(x) = \Big[ M_i(y_{i-1},y_i |x)\Big] =  \Big[  exp(W_i(y_{i-1},y_i |x))\Big] = \Big[  exp(\sum\limits_{k=1}^Kw_kf_k(y_{i-1},y_i, x,i))\Big]
$$


我们引入起点和终点标记$y_0 =start, y_{n+1} = stop$, 这样，标记序列$y$的规范化概率可以通过$n+1$个矩阵元素的乘积得到，即：

$$
P_w(y|x) =  \frac{1}{Z_w(x)}\prod_{i=1}^{n+1}M_i(y_{i-1},y_i |x)
$$
其中$Zw(x)$为规范化因子。

以上就是linear-CRF的模型基础，后面我们会讨论linear-CRF和HMM类似的三个问题的求解方法。

---



#  前向后向算法评估标记序列概率



在[条件随机场CRF(一)](http://www.cnblogs.com/pinard/p/7048333.html)中我们总结了CRF的模型，主要是linear-CRF的模型原理。本文就继续讨论linear-CRF需要解决的三个问题：评估，学习和解码。这三个问题和HMM是非常类似的，本文关注于第一个问题：评估。第二个和第三个问题会在下一篇总结。

## linear-CRF的三个基本问题

在**隐马尔科夫模型HMM**中，我们讲到了HMM的三个基本问题，而linear-CRF也有三个类似的的基本问题。不过和HMM不同，在linear-CRF中，我们对于给出的观测序列$x$是一直作为一个整体看待的，也就是不会拆开看$(x_1,x_2,...)$，因此linear-CRF的问题模型要比HMM简单一些，如果你很熟悉HMM，那么CRF的这三个问题的求解就不难了。

1. linear-CRF第一个问题是**评估**，即给定linear-CRF的条件概率分布$P(y|x)$, 在给定输入序列$x$和输出序列$y$时，计算条件概率$P(y_i|x)$和$P(y_{i-1}，y_i|x)$以及对应的期望。
2. linear-CRF第二个问题是**学习**，即给定训练数据集$X$和$Y$，学习linear-CRF的模型参数$w_k$和条件概率$P_w(y|x)$，这个问题的求解比HMM的学习算法简单的多，普通的梯度下降法，拟牛顿法都可以解决。
3. linear-CRF第三个问题是**解码**，即给定linear-CRF的条件概率分布$P(y|x)$和输入序列$x$, 计算使条件概率最大的输出序列$y$。类似于HMM，使用维特比算法可以很方便的解决这个问题。　

## linear-CRF的前向后向概率概述

要计算条件概率$P(y_i|x)$和$P(y_{i-1}，y_i|x)$，我们也可以使用和HMM类似的方法，使用前向后向算法来完成。首先我们来看前向概率的计算。

我们定义$\alpha_i(y_i|x)$表示序列位置$i$的标记是$y_i$时，在位置$i$之前的部分标记序列的非规范化概率。之所以是非规范化概率是因为我们不想加入一个不影响结果计算的规范化因子$Z(x)$在分母里面。

在[条件随机场CRF(一)](http://www.cnblogs.com/pinard/p/7048333.html)第八节中，我们定义了下式：

$$
M_i(y_{i-1},y_i |x) = exp(\sum\limits_{k=1}^Kw_kf_k(y_{i-1},y_i, x,i))
$$


这个式子定义了在给定$y_{i−1}$时，从$y_{i−1}$转移到$y_i$的非规范化概率。

这样，我们很容易得到序列位置$i+1$的标记是$y_{i+1}$时，在位置$i+1$之前的部分标记序列的非规范化概率$\alpha_{i+1}(y_{i+1}|x)$的递推公式：

$$
\alpha_{i+1}(y_{i+1}|x) = \alpha_i(y_i|x)M_{i+1}(y_{i+1},y_i|x) \;\; i=1,2,...,n+1
$$


在起点处，我们定义：

$$
\alpha_0(y_0|x)= \begin{cases} 1 & {y_0 =start}\\ 0 & {else} \end{cases}
$$


假设我们可能的标记总数是$m$, 则$y_i$的取值就有$m$个，我们用$\alpha_i(x)$表示这$m$个值组成的前向向量如下：

$$
\alpha_i(x) = (\alpha_i(y_i=1|x), \alpha_i(y_i=2|x), ... \alpha_i(y_i=m|x))^T
$$


同时用矩阵$M_i(x)$表示由$M_i(y_{i-1},y_i |x)$形成的$m \times m$阶矩阵：

$$
M_i(x) = \Big[ M_i(y_{i-1},y_i |x)\Big]
$$


这样递推公式可以用矩阵乘积表示：

$$
\alpha_{i+1}^T(x) = \alpha_i^T(x)M_{i+1}(x)
$$


同样的。我们定义$\beta_i(y_i|x)$表示序列位置$i$的标记是$y_i$时，在位置$i$之后的从$i+1$到$n$的部分标记序列的非规范化概率。

这样，我们很容易得到序列位置$i+1$的标记是$y_{i+1}$时，在位置$i$之后的部分标记序列的非规范化概率$\beta_{i}(y_{i}|x)$的递推公式：

$$
\beta_{i}(y_{i}|x) = M_{i+1}(y_i,y_{i+1}|x)\beta_{i+1}(y_{i+1}|x)
$$


在终点处，我们定义：

$$
\beta_{n+1}(y_{n+1}|x)= \begin{cases} 1 & {y_{n+1} =stop}\\ 0 & {else} \end{cases}
$$


如果用向量表示，则有：

$$
\beta_i(x) = M_{i+1}(x)\beta_{i+1}(x)
$$


由于规范化因子$Z(x)$的表达式是：

$$
Z(x) = \sum\limits_{c=1}^m\alpha_{n}(y_c|x) = \sum\limits_{c=1}^m\beta_{1}(y_c|x)
$$


也可以用向量来表示$Z(x)$:

$$
Z(x) = \alpha_{n}^T(x) \bullet \mathbf{1} = \mathbf{1}^T \bullet \beta_{1}(x)
$$


其中，$1$是$m$维全1向量。



## linear-CRF的前向后向概率计算

有了前向后向概率的定义和计算方法，我们就很容易计算序列位置$i$的标记是$y_i$时的条件概率$P(y_i|x)$:

$$
P(y_i|x) = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{Z(x)} = \frac{\alpha_i^T(y_i|x)\beta_i(y_i|x)}{ \alpha_{n}^T(x) \bullet \mathbf{1}}
$$


也容易计算序列位置$i$的标记是$y_i$，位置$i−1$的标记是$y_{i−1}$ 时的条件概率$P(y_{i-1},y_i|x)$:

$$
P(y_{i-1},y_i|x) = \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{Z(x)} = \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{ \alpha_{n}^T(x) \bullet \mathbf{1}}
$$

## linear-CRF的期望计算

有了上一节计算的条件概率，我们也可以很方便的计算联合分布$P(x,y)$与条件分布$P(y|x)$的期望。

特征函数$f_k(x,y)$关于条件分布$P(y|x)$的期望表达式是：

$$
\begin{align} E_{P(y|x)}[f_k]  & = E_{P(y|x)}[f_k(y,x)] \\ & = \sum\limits_{i=1}^{n+1} \sum\limits_{y_{i-1}\;\;y_i}P(y_{i-1},y_i|x)f_k(y_{i-1},y_i,x, i) \\ & =  \sum\limits_{i=1}^{n+1} \sum\limits_{y_{i-1}\;\;y_i}f_k(y_{i-1},y_i,x, i)  \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{ \alpha_{n}^T(x) \bullet \mathbf{1}} \end{align}
$$


同样可以计算联合分布$P(x,y)$的期望：

$$
\begin{align} E_{P(x,y)}[f_k]  & = \sum\limits_{x,y}P(x,y) \sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) \\& =  \sum\limits_{x}\overline{P}(x) \sum\limits_{y}P(y|x) \sum\limits_{i=1}^{n+1}f_k(y_{i-1},y_i,x, i) \\& =  \sum\limits_{x}\overline{P}(x)\sum\limits_{i=1}^{n+1} \sum\limits_{y_{i-1}\;\;y_i}f_k(y_{i-1},y_i,x, i)  \frac{\alpha_{i-1}^T(y_{i-1}|x)M_i(y_{i-1},y_i|x)\beta_i(y_i|x)}{ \alpha_{n}^T(x) \bullet \mathbf{1}}    \end{align}
$$


假设一共有$K$个特征函数，则$k=1,2,...K$



## linear-CRF前向后向算法总结

以上就是linear-CRF的前向后向算法，个人觉得比HMM简单的多，因此大家如果理解了HMM的前向后向算法，这一篇是很容易理解的。

注意到我们上面的非规范化概率$M_{i+1}(y_{i+1},y_i|x)$起的作用和HMM中的隐藏状态转移概率很像。但是这儿的概率是非规范化的，也就是不强制要求所有的状态的概率和为1。而HMM中的隐藏状态转移概率也规范化的。从这一点看，linear-CRF对序列状态转移的处理要比HMM灵活。

---



#  模型学习与维特比算法解码

在CRF系列的前两篇，我们总结了CRF的模型基础与第一个问题的求解方法，本文我们关注于linear-CRF的第二个问题与第三个问题的求解。第二个问题是模型参数学习的问题，第三个问题是维特比算法解码的问题。



## linear-CRF模型参数学习思路

在linear-CRF模型参数学习问题中，我们给定训练数据集$X$和对应的标记序列$Y$，$K$个特征函数$f_k(x,y)$，需要学习linear-CRF的模型参数$w_k$和条件概率$P_w(y|x)$，其中条件概率$P_w(y|x)$和模型参数$w_k$满足一下关系：

$$
P_w(y|x) = P(y|x) =  \frac{1}{Z_w(x)}exp\sum\limits_{k=1}^Kw_kf_k(x,y) =  \frac{exp\sum\limits_{k=1}^Kw_kf_k(x,y)}{\sum\limits_{y}exp\sum\limits_{k=1}^Kw_kf_k(x,y)}
$$


所以我们的目标就是求出所有的模型参数$w_k$，这样条件概率$P_w(y|x)$可以从上式计算出来。

求解这个问题有很多思路，比如梯度下降法，牛顿法，拟牛顿法。同时，这个模型中$P_w(y|x)$的表达式和[最大熵模型原理小结](http://www.cnblogs.com/pinard/p/6093948.html)中的模型一样，也可以使用最大熵模型中使用的改进的迭代尺度法(improved iterative scaling, IIS)来求解。

下面我们只简要介绍用梯度下降法的求解思路。

## linear-CRF模型参数学习之梯度下降法求解

在使用梯度下降法求解模型参数之前，我们需要定义我们的优化函数，一般极大化条件分布$P_w(y|x)$的对数似然函数如下：

$$
L(w)=  log\prod_{x,y}P_w(y|x)^{\overline{P}(x,y)} = \sum\limits_{x,y}\overline{P}(x,y)logP_w(y|x)
$$


其中$\overline{P}(x,y)$为经验分布，可以从先验知识和训练集样本中得到,这点和最大熵模型类似。为了使用梯度下降法，我们现在极小化$f(w) = -L(P_w)$如下：

$$
\begin{align}
f(w) & = -\sum\limits_{x,y}\overline{P}(x,y)logP_w(y|x) \\ 
&=  \sum\limits_{x,y}\overline{P}(x,y)logZ_w(x) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y) \\
& =  \sum\limits_{x}\overline{P}(x)logZ_w(x) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y) \\
& =  \sum\limits_{x}\overline{P}(x)log\sum\limits_{y}exp\sum\limits_{k=1}^Kw_kf_k(x,y) - \sum\limits_{x,y}\overline{P}(x,y)\sum\limits_{k=1}^Kw_kf_k(x,y)  
\end{align}
$$


对$w$求导可以得到：

$$
\frac{\partial f(w)}{\partial w} = \sum\limits_{x,y}\overline{P}(x)P_w(y|x)f(x,y) -  \sum\limits_{x,y}\overline{P}(x,y)f(x,y)
$$


有了$w$的导数表达书，就可以用梯度下降法来迭代求解最优的$w$了。注意在迭代过程中，每次更新$w$后，需要同步更新$P_w(x,y)$,以用于下一次迭代的梯度计算。

梯度下降法的过程这里就不累述了，如果不熟悉梯度下降算法过程建议阅读之前写的[梯度下降（Gradient Descent）小结](http://www.cnblogs.com/pinard/p/5970503.html)。以上就是linear-CRF模型参数学习之梯度下降法求解思路总结。

## linear-CRF模型维特比算法解码思路

现在我们来看linear-CRF的第三个问题：解码。在这个问题中，给定条件随机场的条件概率$P(y|x)$和一个观测序列$x$,要求出满足$P(y|x)$最大的序列$y$。

这个解码算法最常用的还是和HMM解码类似的维特比算法。到目前为止，我已经在三个地方讲到了维特比算法，第一个是[文本挖掘的分词原理](http://www.cnblogs.com/pinard/p/6677078.html)中用于中文分词，第二个是[隐马尔科夫模型HMM（四）维特比算法解码隐藏状态序列](http://www.cnblogs.com/pinard/p/6991852.html)中用于HMM解码。第三个就是这一篇了。

维特比算法本身是一个动态规划算法，利用了两个局部状态和对应的递推公式，从局部递推到整体，进而得解。对于具体不同的问题，仅仅是这两个局部状态的定义和对应的递推公式不同而已。由于在之前已详述维特比算法，这里就是做一个简略的流程描述。

对于我们linear-CRF中的维特比算法，我们的第一个局部状态定义为$\delta_i(l)$,表示在位置$i$标记$l$各个可能取值$(1,2...m)$对应的非规范化概率的最大值。之所以用非规范化概率是，规范化因子$Z(x)$不影响最大值的比较。根据$\delta_i(l)$的定义，我们递推在位置$i+1$标记$l$的表达式为：

$$
\delta_{i+1}(l) = \max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\;, l=1,2,...m
$$


和HMM的维特比算法类似，我们需要用另一个局部状态$\Psi_{i+1}(l)$来记录使$\delta_{i+1}(l)$达到最大的位置$i$的标记取值,这个值用来最终回溯最优解，$\Psi_{i+1}(l)$的递推表达式为：

$$
\Psi_{i+1}(l) = arg\;\max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\; ,l=1,2,...m
$$

## linear-CRF模型维特比算法流程

现在我们总结下 linear-CRF模型维特比算法流程：

输入：模型的$K$个特征函数，和对应的$K$个权重。观测序列$x=(x_1,x_2,...x_n)$,可能的标记个数$m$

输出：最优标记序列$y^* =(y_1^*,y_2^*,...y_n^*)$

1) 初始化：

$$
\delta_{1}(l) = \sum\limits_{k=1}^Kw_kf_k(y_{0} =start,y_{1} = l,x,i)\;, l=1,2,...m
$$


$$
\Psi_{1}(l) = start\;, l=1,2,...m
$$


2) 对于$i=1,2...n-1$,进行递推：

$$
\delta_{i+1}(l) = \max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\;, l=1,2,...m
$$


$$
\Psi_{i+1}(l) = arg\;\max_{1 \leq j \leq m}\{\delta_i(j) + \sum\limits_{k=1}^Kw_kf_k(y_{i} =j,y_{i+1} = l,x,i)\}\; ,l=1,2,...m
$$


3) 终止：

$$
y_n^* = arg\;\max_{1 \leq j \leq m}\delta_n(j)
$$


4) 回溯：

$$
y_i^* = \Psi_{i+1}(y_{i+1}^*)\;, i=n-1,n-2,...1
$$


最终得到最优标记序列$y^* =(y_1^*,y_2^*,...y_n^*)$



## linear-CRF模型维特比算法实例

下面用一个具体的例子来描述 linear-CRF模型维特比算法，例子的模型和CRF系列第一篇中一样，都来源于《统计学习方法》。

假设输入的都是三个词的句子，即$X=(X_1,X_2,X_3)$,输出的词性标记为$Y=(Y_1,Y_2,Y_3)$,其中$Y \in \{1(名词)，2(动词)\}$

这里只标记出取值为1的特征函数如下：

$$
t_1 =t_1(y_{i-1} = 1, y_i =2,x,i), i =2,3,\;\;\lambda_1=1
$$

$$
t_2 =t_2(y_1=1,y_2=1,x,2)\;\;\lambda_2=0.6
$$

$$
t_3 =t_3(y_2=2,y_3=1,x,3)\;\;\lambda_3=1
$$

$$
t_4 =t_4(y_1=2,y_2=1,x,2)\;\;\lambda_4=1
$$

$$
t_5 =t_5(y_2=2,y_3=2,x,3)\;\;\lambda_5=0.2
$$

$$
s_1 =s_1(y_1=1,x,1)\;\;\mu_1 =1
$$

$$
s_2 =s_2( y_i =2,x,i), i =1,2,\;\;\mu_2=0.5
$$

$$
s_3 =s_3( y_i =1,x,i), i =2,3,\;\;\mu_3=0.8
$$

$$
s_4 =s_4(y_3=2,x,3)\;\;\mu_4 =0.5
$$



求标记(1,2,2)的最可能的标记序列。

首先初始化：

$$
\delta_1(1) = \mu_1s_1 = 1\;\;\;\delta_1(2) = \mu_2s_2 = 0.5\;\;\;\Psi_{1}(1) =\Psi_{1}(2) = start
$$


接下来开始递推，先看位置2的：

$$
\begin{align}
\delta_2(1) &= max\{\delta_1(1) + t_2\lambda_2+\mu_3s_3, \delta_1(2) + t_4\lambda_4+\mu_3s_3 \} \\
&= max\{1+0.6+0.8,0.5+1+0.8\} =2.4\;\;\;\Psi_{2}(1) =1
\end{align}
$$


$$
\begin{align}
\delta_2(2) &= max\{\delta_1(1) + t_1\lambda_1+\mu_2s_2, \delta_1(2) + \mu_2s_2\} \\
&= max\{1+1+0.5,0.5+0.5\} =2.5\;\;\;\Psi_{2}(2) =1
\end{align}
$$


再看位置3的：



$$
\begin{align}
\delta_3(1) &= max\{\delta_2(1) +\mu_3s_3, \delta_2(2) + t_3\lambda_3+\mu_3s_3\} \\
&= max\{2.4+0.8,2.5+1+0.8\} =4.3
\end{align}
$$


$$
\Psi_{3}(1) =2
$$


$$
\begin{align}
\delta_3(2) &= max\{\delta_2(1) +t_1\lambda_1 + \mu_4s_4, \delta_2(2) + t_5\lambda_5+\mu_4s_4\} \\
&= max\{2.4+1+0.5,2.5+0.2+0.5\} =3.9
\end{align}
$$


$$
\Psi_{3}(2) =1
$$


最终得到$y_3^* =\arg\;max\{\delta_3(1), \delta_3(2)\}$,递推回去，得到：

$$
y_2^* = \Psi_3(1) =2\;\;y_1^* = \Psi_2(2) =1
$$


即最终的结果为$(1,2,1)$,即标记为(名词，动词，名词)。

## linear-CRF vs HMM

linear-CRF模型和HMM模型有很多相似之处，尤其是其三个典型问题非常类似，除了模型参数学习的问题求解方法不同以外，概率估计问题和解码问题使用的算法思想基本也是相同的。同时，两者都可以用于序列模型，因此都广泛用于自然语言处理的各个方面。

现在来看看两者的不同点。最大的不同点是linear-CRF模型是判别模型，而HMM是生成模型，即linear-CRF模型要优化求解的是条件概率$P(y|x)$,则HMM要求解的是联合分布$P(x,y)$。第二，linear-CRF是利用最大熵模型的思路去建立条件概率模型，对于观测序列并没有做马尔科夫假设。而HMM是在对观测序列做了马尔科夫假设的前提下建立联合分布的模型。

最后想说的是，只有linear-CRF模型和HMM模型才是可以比较讨论的。但是linear-CRF是CRF的一个特例，CRF本身是一个可以适用于很复杂条件概率的模型，因此理论上CRF的使用范围要比HMM广泛的多。

以上就是CRF系列的所有内容。

