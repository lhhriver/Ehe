#  从性能角度初探并发编程

##  基本概念

在开始讲解理论知识之前，先过一下几个基本概念。虽然咱是进阶教程，但我也希望写得更小白，更通俗易懂。

`串行`：一个人在同一时间段只能干一件事，譬如吃完饭才能看电视；

`并行`：一个人在同一时间段可以干多件事，譬如可以边吃饭边看电视；

在Python中，`多线程` 和 `协程` 虽然是严格上来说是串行，但却比一般的串行程序执行效率高得很。

一般的串行程序，在程序阻塞的时候，只能干等着，不能去做其他事。就好像，电视上播完正剧，进入广告时间，我们却不能去趁广告时间是吃个饭。对于程序来说，这样做显然是效率极低的，是不合理的。

当然，学完这个课程后，我们就懂得，利用广告时间去做其他事，灵活安排时间。这也是我们`多线程`和`协程` 要帮我们要完成的事情，内部合理调度任务，使得程序效率最大化。

虽然 `多线程` 和 `协程` 已经相当智能了。但还是不够高效，最高效的应该是一心多用，边看电视边吃饭边聊天。这就是我们的 `多进程` 才能做的事了。

为了更帮助大家更加直观的理解，在网上找到两张图，来生动形象的解释了多线程和多进程的区别。

- `多线程`，交替执行，另一种意义上的串行。
- `多进程`，并行执行，真正意义上的并发。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-746252.png)

##  单线程VS多线程VS多进程

文字总是苍白无力的，不如用代码直接来测试一下。

在开始之前呢，我要声明一下，本文作为并发章节的第一篇文章，只为了让你对单线程、多线程、多进程有个直观的了解。因此下面的代码中，会有多线程和多进程的的知识点，这些知识点在后面几节才会讲到，如果你看不明白也没有关系。

我的实验环境配置如下

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171028-366254.png)

开始对比之前，首先定义四种类型的场景

- CPU计算密集型
- 磁盘IO密集型
- 网络IO密集型
- 【模拟】IO密集型

为什么是这几种场景，这和`多线程` `多进程`的适用场景有关。结论里，我再说明。

```python
# CPU计算密集型
def count(x=1, y=1):
    # 使程序完成150万计算
    c = 0
    while c < 500000:
        c += 1
        x += x
        y += y


# 磁盘读写IO密集型
def io_disk():
    with open("file.txt", "w") as f:
        for x in range(5000000):
            f.write("python-learning\n")


# 网络IO密集型
header = {
    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.139 Safari/537.36'}
url = "https://www.tieba.com/"

def io_request():
    try:
        webPage = requests.get(url, headers=header)
        html = webPage.text
        return
    except Exception as e:
        return {"error": e}

        
# 【模拟】IO密集型
def io_simulation():
    time.sleep(2)
```

比拼的指标，我们用时间来考量。时间耗费得越少，说明效率越高。

为了方便，使得代码看起来，更加简洁，我这里先定义是一个简单的 `时间计时器` 的装饰器。

如果你对装饰器还不是很了解，也没关系，你只要知道它是用于 计算函数运行时间的东西就可以了。

```python
def timer(mode):
    def wrapper(func):
        def deco(*args, **kw):
            type = kw.setdefault('type', None)
            t1 = time.time()
            func(*args, **kw)
            t2 = time.time()
            cost_time = t2-t1
            print("{}-{}花费时间：{}秒".format(mode, type, cost_time))
        return deco
    return wrapper
```

### 单线程

```python
@timer("【单线程】")
def single_thread(func, type=""):
    for i in range(10):
        func()

# 单线程
single_thread(count, type="CPU计算密集型")
single_thread(io_disk, type="磁盘IO密集型")
single_thread(io_request,type="网络IO密集型")
single_thread(io_simulation,type="模拟IO密集型")
```

看看结果

```
【单线程】-CPU计算密集型花费时间：83.42633867263794秒
【单线程】-磁盘IO密集型花费时间：15.641993284225464秒
【单线程】-网络IO密集型花费时间：1.1397218704223633秒
【单线程】-模拟IO密集型花费时间：20.020972728729248秒
```

### 多线程

```python
@timer("【多线程】")
def multi_thread(func, type=""):
    thread_list = []
    for i in range(10):
        t=Thread(target=func, args=())
        thread_list.append(t)
        t.start()
    e = len(thread_list)

    while True:
        for th in thread_list:
            if not th.is_alive():
                e -= 1
        if e <= 0:
            break

# 多线程
multi_thread(count, type="CPU计算密集型")
multi_thread(io_disk, type="磁盘IO密集型")
multi_thread(io_request, type="网络IO密集型")
multi_thread(io_simulation, type="模拟IO密集型")
```

看看结果

```
【多线程】-CPU计算密集型花费时间：93.82986998558044秒
【多线程】-磁盘IO密集型花费时间：13.270896911621094秒
【多线程】-网络IO密集型花费时间：0.1828296184539795秒
【多线程】-模拟IO密集型花费时间：2.0288875102996826秒
```

### 多进程

```python
@timer("【多进程】")
def multi_process(func, type=""):
    process_list = []
    for x in range(10):
        p = Process(target=func, args=())
        process_list.append(p)
        p.start()
        
    e = process_list.__len__()

    while True:
        for pr in process_list:
            if not pr.is_alive():
                e -= 1
        if e <= 0:
            break

# 多进程
multi_process(count, type="CPU计算密集型")
multi_process(io_disk, type="磁盘IO密集型")
multi_process(io_request, type="网络IO密集型")
multi_process(io_simulation, type="模拟IO密集型")
```

看看结果

```
【多进程】-CPU计算密集型花费时间：9.082211017608643秒
【多进程】-磁盘IO密集型花费时间：1.287339448928833秒
【多进程】-网络IO密集型花费时间：0.13074755668640137秒
【多进程】-模拟IO密集型花费时间：2.0076842308044434秒
```

##  性能对比成果总结

将结果汇总一下，制成表格。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171028-350250.png)


我们来分析下这个表格。

首先是`CPU密集型`，多线程以对比单线程，不仅没有优势，显然还由于要不断的加锁释放GIL全局锁，切换线程而耗费大量时间，效率低下，而多进程，由于是多个CPU同时进行计算工作，相当于十个人做一个人的作业，显然效率是成倍增长的。

然后是IO密集型，`IO密集型`可以是`磁盘IO`，`网络IO`，`数据库IO`等，都属于同一类，计算量很小，主要是IO等待时间的浪费。通过观察，可以发现，我们磁盘IO，网络IO的数据，多线程对比单线程也没体现出很大的优势来。这是由于我们程序的的IO任务不够繁重，所以优势不够明显。

所以我还加了一个「`模拟IO密集型`」，用`sleep`来模拟IO等待时间，就是为了体现出多线程的优势，也能让大家更加直观的理解多线程的工作过程。单线程需要每个线程都要`sleep(2)`，10个线程就是`20s`，而多线程，在`sleep(2)`的时候，会切换到其他线程，使得10个线程同时`sleep(2)`，最终10个线程也就只有`2s`.

可以得出以下几点结论：

- 单线程总是最慢的，多进程总是最快的。
- 多线程适合在IO密集场景下使用，譬如爬虫，网站开发等
- 多进程适合在对CPU计算运算要求较高的场景下使用，譬如大数据分析，机器学习等
- 多进程虽然总是最快的，但是不一定是最优的选择，因为它需要CPU资源支持下才能体现优势

#  创建多线程的几种方法


今天的内容会比较基础，主要是为了让新手也能无障碍地阅读，所以还是要再巩固下基础。学完了基础，你们也就能很顺畅地跟着我的思路理解以后的文章。


经过总结，Python创建多线程主要有如下两种方法：

- 函数
- 类

接下来，我们就来揭开多线程的神秘面纱。

##  用函数创建多线程

在Python3中，Python提供了一个内置模块 `threading.Thread`，可以很方便地让我们创建多线程。

`threading.Thread()` 一般接收两个参数：

- 线程函数名：要放置线程让其后台执行的函数，由我们自已定义，注意不要加`()`；
- 线程函数的参数：线程函数名所需的参数，以元组的形式传入。若不需要参数，可以不指定。

**举个例子**

```python
import time
from threading import Thread

# 自定义线程函数。
def target(name="Python"):
    for i in range(2):
        print("hello", name)
        time.sleep(1)

# 创建线程01，不指定参数
thread_01 = Thread(target=target)
# 启动线程01
thread_01.start()


# 创建线程02，指定参数，注意逗号
thread_02 = Thread(target=target, args=("MING",))
# 启动线程02
thread_02.start()
```

可以看到输出

```python
hello Python
hello MING
hello Python
hello MING
```

##  用类创建多线程

相比较函数而言，使用类创建线程，会比较麻烦一点。

首先，我们要自定义一个类，对于这个类有两点要求，

- 必须继承 `threading.Thread` 这个父类；
- 必须复写 `run` 方法。

这里的 `run` 方法，和我们上面`线程函数`的性质是一样的，可以写我们的业务逻辑程序。在 `start()` 后将会调用。

来看一下例子，为了方便对比，`run`函数我复用上面的`main`。

```python
import time
from threading import Thread

class MyThread(Thread):
    def __init__(self, type="Python"):
        # 注意：super().__init__() 必须写，且最好写在第一行
        super().__init__()
        self.type = type

    def run(self):
        for i in range(2):
            print("hello", self.type)
            time.sleep(1)


if __name__ == '__main__':
    # 创建线程01，不指定参数
    thread_01 = MyThread()
    # 创建线程02，指定参数
    thread_02 = MyThread("MING")

    thread_01.start()
    thread_02.start()
```

当然结果也是一样的。

```python
hello Python
hello MING
hello Python
hello MING
```

##  线程对象的方法

上面介绍了当前 Python 中创建线程两种主要方法。

创建线程是件很容易的事，但要想用好线程，还需要学习线程对象的几个函数。

经过我的总结，大约常用的方法有如下这些：

```python
# 如上所述，创建一个线程
t = Thread(target=func)

# 启动子线程
t.start()

# 阻塞子线程，待子线程结束后，再往下执行
t.join()

# 判断线程是否在执行状态，在执行返回True，否则返回False
t.is_alive()
t.isAlive()

# 设置线程是否随主线程退出而退出，默认为False
t.daemon = True
t.daemon = False

# 设置线程名
t.name = "My-Thread"
```

#  谈谈线程中的“锁机制”

##  什么是锁？

在开发中，**锁** 可以理解为通行证。当你对一段逻辑代码加锁时，意味着在同一时间有且仅能有一个线程在执行这段代码。

在 Python 中的锁可以分为两种：

1. 互斥锁
2. 可重入锁

##  互斥锁的使用

来简单看下代码，学习如何加锁，获取钥匙，释放锁。

```python
import threading

# 生成锁对象，全局唯一
lock = threading.Lock()

# 获取锁。未获取到会阻塞程序，直到获取到锁才会往下执行
lock.acquire()

# 释放锁，归还锁，其他人可以拿去用了
lock.release()
```

需要注意的是，lock.acquire() 和 lock.release()必须成对出现。否则就有可能造成死锁。

很多时候，我们虽然知道，他们必须成对出现，但是还是难免会有忘记的时候。

为了，规避这个问题。我推荐使用使用上下文管理器来加锁。

```python
import threading

lock = threading.Lock()
with lock:
    # 这里写自己的代码
    pass
```

`with` 语句会在这个代码块执行前自动获取锁，在执行结束后自动释放锁。

##  为何要使用锁？

你现在肯定还是一脸懵逼，这么麻烦，我不用锁不行吗？有的时候还真不行。

那么为了说明锁存在的意义。我们分别来看下，不用锁的情形有怎样的问题。

定义两个函数，分别在两个线程中执行。这两个函数 `共用` 一个变量 `n` 。

```python
def job1():
    global n
    for i in range(10):
        n += 1
        print('job1', n)

def job2():
    global n
    for i in range(10):
        n += 10
        print('job2', n)

n = 0
t1 = threading.Thread(target=job1)
t2 = threading.Thread(target=job2)
t1.start()
t2.start()
```

看代码貌似没什么问题，执行下看看输出

```python
job1 1
job1 2
job1 job2 13
job2 23
job2 333
job1 34
job1 35
job2
job1 45 46
job2 56
job1 57
job2
job1 67
job2 68 78
job1 79
job2
job1 89
job2 90 100
job2 110
```

是不是很乱？完全不是我们预想的那样。

解释下这是为什么？因为两个线程共用一个全局变量，又由于两线程是交替执行的，当`job1` 执行三次 `+1` 操作时，`job2`就不管三七二十一 给n做了`+10`操作。两个线程之间，执行完全没有规矩，没有约束。所以会看到输出当然也很乱。

加了锁后，这个问题也就解决，来看看

```python
def job1():
    global n, lock
    # 获取锁
    lock.acquire()
    for i in range(10):
        n += 1
        print('job1', n)
    lock.release()


def job2():
    global n, lock
    # 获取锁
    lock.acquire()
    for i in range(10):
        n += 10
        print('job2', n)
    lock.release()

n = 0
# 生成锁对象
lock = threading.Lock()

t1 = threading.Thread(target=job1)
t2 = threading.Thread(target=job2)
t1.start()
t2.start()
```

由于`job1`的线程，率先拿到了锁，所以在for循环中，没有人有权限对n进行操作。当`job1`执行完毕释放锁后，`job2`这才拿到了锁，开始自己的for循环。

看看执行结果，真如我们预想的那样。

```python
job1 1
job1 2
job1 3
job1 4
job1 5
job1 6
job1 7
job1 8
job1 9
job1 10
job2 20
job2 30
job2 40
job2 50
job2 60
job2 70
job2 80
job2 90
job2 100
job2 110
```

这里，你应该也知道了，加锁是为了对锁内资源（变量）进行锁定，避免其他线程篡改已被锁定的资源，以达到我们预期的效果。

为了避免大家忘记释放锁，后面的例子，我将都使用with上下文管理器来加锁。大家注意一下。

##  可重入锁（RLock）

有时候在同一个线程中，我们可能会多次请求同一资源，俗称锁嵌套。

如果还是按照常规的做法，会造成死锁的。比如，下面这段代码，你可以试着运行一下。会发现并没有输出结果。

```python
import threading

def main():
    n = 0
    lock = threading.Lock()
    with lock:
        for i in range(10):
            n += 1
            with lock:
                print(n)

t1 = threading.Thread(target=main)
t1.start()
```

是因为第二次获取锁(通行证)时，发现锁(通行证)已经被同一线程的人拿走了，拿东西总有个先来后到，别人拿走了，你要想用，你就得干等着，直到有人归还锁（通行证），假如别人一直不归还，那程序就会在这里一直阻塞。

上面的代码中，使用了嵌套锁，在锁还没有释放的时候，又再一次请求锁，这就当然会造成死锁了。

那么如何解决这个问题呢？

`threading`模块除了提供`Lock`锁之外，还提供了一种可重入锁`RLock`，专门来处理这个问题。

```python
import threading

def main():
    n = 0
    # 生成可重入锁对象
    lock = threading.RLock()
    with lock:
        for i in range(10):
            n += 1
            with lock:
                print(n)

t1 = threading.Thread(target=main)
t1.start()
```

执行一下，发现已经有输出了。

```python
1
2
3
4
5
6
7
8
9
10
```

需要注意的是，可重入锁（RLock），只在同一线程里放松对锁(通行证)的获取，意思是，只要在同一线程里，程序就当你是同一个人，这个锁就可以复用，其他的话与`Lock`并无区别。

##  防止死锁的加锁机制

在编写多线程程序时，可能无意中就会写了一个死锁。可以说，死锁的形式有多种多样，但是本质都是相同的，都是对资源不合理竞争的结果。

以本人的经验总结，死锁通常以下几种

- 同一线程，嵌套获取同把互斥锁，造成死锁。 
- 多个线程，不按顺序同时获取多个锁。造成死锁   

对于第一种，上面已经说过了，使用可重入锁。

主要是第二种。可能你还没明白，是如何死锁的。

**举个例子：**

>线程1，嵌套获取A,B两个锁，线程2，嵌套获取B,A两个锁。
>
>由于两个线程是交替执行的，是有机会遇到线程1获取到锁A，而未获取到锁B，在同一时刻，线程2获取到锁B，而未获取到锁A。
>
>由于锁B已经被线程2获取了，所以线程1就卡在了获取锁B处，由于是嵌套锁，线程1未获取并释放B，是不能释放锁A的，这是导致线程2也获取不到锁A，也卡住了。两个线程，各执一锁，各不让步。造成死锁。

经过数学证明，只要两个（或多个）线程获取嵌套锁时，按照固定顺序就能保证程序不会进入死锁状态。

那么问题就转化成如何保证这些锁是按顺序的？

有两个办法

- 人工自觉，人工识别。
- 写一个辅助函数来对锁进行排序。

第一种，就不说了。

第二种，可以参考如下代码

```python
import threading
from contextlib import contextmanager

# Thread-local state to stored information on locks already acquired
_local = threading.local()

@contextmanager
def acquire(*locks):
    # Sort locks by object identifier
    locks = sorted(locks, key=lambda x: id(x))

    # Make sure lock order of previously acquired locks is not violated
    acquired = getattr(_local,'acquired',[])
    
    if acquired and max(id(lock) for lock in acquired) >= id(locks[0]):
        raise RuntimeError('Lock Order Violation')

    # Acquire all of the locks
    acquired.extend(locks)
    _local.acquired = acquired

    try:
        for lock in locks:
            lock.acquire()
        yield
    finally:
        # Release locks in reverse order of acquisition
        for lock in reversed(locks):
            lock.release()
        del acquired[-len(locks):]
```

如何使用呢？

```python
import threading
x_lock = threading.Lock()
y_lock = threading.Lock()

def thread_1():

    while True:
        with acquire(x_lock):
            with acquire(y_lock):
                print('Thread-1')

def thread_2():
    while True:
        with acquire(y_lock):
            with acquire(x_lock):
                print('Thread-2')

t1 = threading.Thread(target=thread_1)
t1.daemon = True
t1.start()

t2 = threading.Thread(target=thread_2)
t2.daemon = True
t2.start()
```

看到没有，表面上`thread_1`的先获取锁x，再获取锁`y`，而`thread_2`是先获取锁`y`，再获取`x`。
但是实际上，`acquire`函数，已经对`x`，`y`两个锁进行了排序。所以`thread_1`，`hread_2`都是以同一顺序来获取锁的，是不是造成死锁的。

##  饱受争议的GIL（全局锁）

在第一节的时候，我就和大家介绍到，多线程和多进程是不一样的。

多进程是真正的并行，而多线程是伪并行，实际上他只是交替执行。

是什么导致多线程，只能交替执行呢？是一个叫`GIL`（`Global Interpreter Lock`，全局解释器锁）的东西。什么是GIL呢？

>任何Python线程执行前，必须先获得GIL锁，然后，每执行100条字节码，解释器就自动释放GIL锁，让别的线程有机会执行。这个GIL全局锁实际上把所有线程的执行代码都给上了锁，所以，多线程在Python中只能交替执行，即使100个线程跑在100核CPU上，也只能用到1个核。

需要注意的是，GIL并不是Python的特性，它是在实现Python解析器(CPython)时所引入的一个概念。而Python解释器，并不是只有CPython，除它之外，还有`PyPy`，`Psyco`，`JPython`，`IronPython`等。

在绝大多数情况下，我们通常都认为 Python `==` CPython，所以也就默许了Python具有GIL锁这个事。

都知道GIL影响性能，那么如何避免受到GIL的影响？

- 使用多进程代替多线程。
- 更换Python解释器，不使用CPython

#  线程消息通信机制

前面我已经向大家介绍了，如何使用创建线程，启动线程。相信大家都会有这样一个想法，线程无非就是创建一下，然后再`start()`下，实在是太简单了。

可是要知道，在真实的项目中，实际场景可要我们举的例子要复杂的多得多，不同线程的执行可能是有顺序的，或者说他们的执行是有条件的，是要受控制的。如果仅仅依靠前面学的那点浅薄的知识，是远远不够的。

那今天，我们就来探讨一下如何控制线程的触发执行。

要实现对多个线程进行控制，其实本质上就是消息通信机制在起作用，利用这个机制发送指令，告诉线程，什么时候可以执行，什么时候不可以执行，执行什么内容。

经过我的总结，线程中通信方法大致有如下三种：

- threading.Event
- threading.Condition
- queue.Queue

接下来我们来一一探讨下。

---

##  Event事件

Python提供了非常简单的通信机制 `Threading.Event`，通用的条件变量。多个线程可以`等待某个事件的发生`，在事件发生后，`所有的线程`都会被`激活`。

关于Event的使用也超级简单，就三个函数

```python
event = threading.Event()

# 重置event，使得所有该event事件都处于待命状态
event.clear()

# 等待接收event的指令，决定是否阻塞程序执行
event.wait()

# 发送event指令，使所有设置该event事件的线程执行
event.set()
```

举个例子来看下。

```python
import time
import threading


class MyThread(threading.Thread):
    def __init__(self, name, event):
        super().__init__()
        self.name = name
        self.event = event

    def run(self):
        print('Thread: {} start at {}'.format(self.name, time.ctime(time.time())))
        # 等待event.set()后，才能往下执行
        self.event.wait()
        print('Thread: {} finish at {}'.format(self.name, time.ctime(time.time())))


threads = []
event = threading.Event()

# 定义五个线程
[threads.append(MyThread(str(i), event)) for i in range(1,5)]

# 重置event，使得event.wait()起到阻塞作用
event.clear()

# 启动所有线程
[t.start() for t in threads]

print('等待5s...')
time.sleep(5)

print('唤醒所有线程...')
event.set()
```

执行一下，看看结果

```python
Thread: 1 start at Sun May 13 20:38:08 2018
Thread: 2 start at Sun May 13 20:38:08 2018
Thread: 3 start at Sun May 13 20:38:08 2018
Thread: 4 start at Sun May 13 20:38:08 2018

等待5s...

唤醒所有线程...
Thread: 1 finish at Sun May 13 20:38:13 2018
Thread: 4 finish at Sun May 13 20:38:13 2018
Thread: 2 finish at Sun May 13 20:38:13 2018
Thread: 3 finish at Sun May 13 20:38:13 2018
```

可见在所有线程都启动（`start()`）后，并不会执行完，而是都在`self.event.wait()`止住了，需要我们通过`event.set()`来给所有线程发送执行指令才能往下执行。

##  Condition

Condition和Event 是类似的，并没有多大区别。

同样，Condition也只需要掌握几个函数即可。

```python
cond = threading.Condition()

# 类似lock.acquire()
cond.acquire()

# 类似lock.release()
cond.release()

# 等待指定触发，同时会释放对锁的获取,直到被notify才重新占有琐。
cond.wait()

# 发送指定，触发执行
cond.notify()
```

举个网上一个比较趣的捉迷藏的例子来看看

```python
import threading, time

class Hider(threading.Thread):
    def __init__(self, cond, name):
        super(Hider, self).__init__()
        self.cond = cond
        self.name = name

    def run(self):
        time.sleep(1)  #确保先运行Seeker中的方法
        self.cond.acquire()

        print(self.name + ': 我已经把眼睛蒙上了')
        self.cond.notify()
        self.cond.wait()
        print(self.name + ': 我找到你了哦 ~_~')
        self.cond.notify() 

        self.cond.release()
        print(self.name + ': 我赢了')

class Seeker(threading.Thread):
    def __init__(self, cond, name):
        super(Seeker, self).__init__()
        self.cond = cond
        self.name = name
        
    def run(self):
        self.cond.acquire()
        self.cond.wait()
        print(self.name + ': 我已经藏好了，你快来找我吧')
        self.cond.notify()
        self.cond.wait()
        self.cond.release()
        print(self.name + ': 被你找到了，哎~~~')
        
cond = threading.Condition()
seeker = Seeker(cond, 'seeker')
hider = Hider(cond, 'hider')
seeker.start()
hider.start()
```

通过cond来通信，阻塞自己，并使对方执行。从而，达到有顺序的执行。

看下结果

```python
hider:   我已经把眼睛蒙上了
seeker:  我已经藏好了，你快来找我吧
hider:   我找到你了 ~_~
hider:   我赢了
seeker:  被你找到了，哎~~~
```

##  Queue队列

最后一个，队列，它是本节的重点，因为它是我们日常开发中最使用频率最高的。

从一个线程向另一个线程发送数据最安全的方式可能就是使用 queue 库中的队列了。创建一个被多个线程共享的 Queue 对象，这些线程通过使用`put()` 和 `get()` 操作来向队列中发送和获取元素。 

同样，对于Queue，我们也只需要掌握几个函数即可。

```python
from queue import Queue

# maxsize默认为0，不受限
# 一旦>0，而消息数又达到限制，q.put()也将阻塞
q = Queue(maxsize=0)

# 默认阻塞程序，等待队列消息，可设置超时时间
q.get(block=True, timeout=None)

# 发送消息：默认会阻塞程序至队列中有空闲位置放入数据
q.put(item, block=True, timeout=None)

# 等待所有的消息都被消费完
q.join()

# 通知队列任务处理已经完成，当所有任务都处理完成时，join() 阻塞将会解除
q.task_done()
```

以下三个方法，知道就好，一般不需要使用

```python
# 查询当前队列的消息个数
q.qsize()

# 队列消息是否都被消费完，返回 True/False
q.empty()

# 检测队列里消息是否已满
q.full()
```



函数会比之前的多一些，同时也从另一方面说明了其功能更加丰富。

我来举个老师点名的例子。

```python
# coding=utf-8
# /usr/bin/env python

'''
Author: wangbm
Email: wongbingming@163.com
Wechat: mrbensonwon
Blog: python-online.cn
公众号：Python编程时光


date: 2020/9/20 下午7:30
desc: 
'''

__author__ = 'wangbm'


from queue import Queue
from threading import Thread
import time

class Student:
    def __init__(self, name):
        self.name = name

    def speak(self):
        print("{}：到！".format(self.name))


class Teacher:
    def __init__(self, queue):
        super().__init__()
        self.queue=queue

    def call(self, student_name):
        if student_name == "exit":
            print("点名结束，开始上课..")
        else:
            print("老师：{}来了没？".format(student_name))
            # 发送消息，要点谁的名
        self.queue.put(student_name)

class CallManager(Thread):
    def __init__(self, queue):
        super().__init__()
        self.students = {}
        self.queue = queue

    def put(self, student):
        self.students.setdefault(student.name, student)

    def run(self):
        while True:
            # 阻塞程序，时刻监听老师，接收消息
            student_name = queue.get()
            if student_name == "exit":
                break
            elif student_name in self.students:
                self.students[student_name].speak()
            else:
                print("老师，咱班，没有 {} 这个人".format(student_name))

queue = Queue()
teacher = Teacher(queue=queue)

s1 = Student(name="小明")
s2 = Student(name="小亮")

cm = CallManager(queue)
cm.put(s1)
cm.put(s2)
cm.start()

print('开始点名~')
teacher.call('小明')
time.sleep(1)
teacher.call('小亮')
time.sleep(1)
teacher.call("exit")
```

运行结果如下

```python
开始点名~
老师：小明来了没？
小明：到！
老师：小亮来了没？
小亮：到！
点名结束，开始上课..
```

其实 queue 还有一个很重要的方法，Queue.task_done()

如果不明白它的原理，我们在写程序，就很有可能卡死。

当我们使用 Queue.get() 从队列取出数据后，这个数据有没有被正常消费，是很重要的。

如果数据没有被正常消费，那么Queue会认为这个任务还在执行中，此时你使用 Queue.join() 会一直阻塞，即使此时你的队列里已经没有消息了。

那么如何解决这种一直阻塞的问题呢？

就是在我们正常消费完数据后，记得调用一下 Queue.task_done()，说明队列这个任务已经结束了。

当队列内部的任务计数器归于零时，调用 Queue.join() 就不会再阻塞了。

要理解这个过程，请参考 https://python.iswbm.com/c02/c02_06.html 里自定义线程池的的例子。

##  消息队列的先进先出

消息队列可不是只有`queue.Queue`这一个类，除它之外，还有`queue.LifoQueue`和`queue.PriorityQueue`这两个类。

从名字上，对于他们之间的区别，你大概也能猜到一二吧。

> `queue.Queue`：先进先出队列
>
> `queue.LifoQueue`：后进先出队列
>
> `queue.PriorityQueue`：优先级队列

先来看看，我们的老朋友，`queue.Queue`。

所谓的`先进先出`（FIFO，First in First Out），就是先进入队列的消息，将优先被消费。

这和我们日常排队买菜是一样的，先排队的人肯定是先买到菜。

用代码来说明一下

```python
import queue

q = queue.Queue()

for i in range(5):
    q.put(i)

while not q.empty():
    print q.get()
```

看看输出，符合我们先进先出的预期。存入队列的顺序是`01234`，被消费的顺序也是`01234`。

```
0
1
2
3
4
```

再来看看`Queue.LifoQueue`，后进先出，就是后进入消息队列的，将优先被消费。

这和我们羽毛球筒是一样的，最后放进羽毛球筒的球，会被第一个取出使用。

用代码来看下

```python
import queue

q = queue.LifoQueue()

for i in range(5):
    q.put(i)

while not q.empty():
    print q.get()
```

来看看输出，符合我们后进后出的预期。存入队列的顺序是`01234`，被消费的顺序也是`43210`。

```
4
3
2
1
0
```

最后来看看`Queue.PriorityQueue`，优先级队列。
这和我们日常生活中的会员机制有些类似，办了金卡的人比银卡的服务优先，办了银卡的人比不办卡的人服务优先。

来用代码看一下

```python
from queue import PriorityQueue

# 重新定义一个类，继承自PriorityQueue
class MyPriorityQueue(PriorityQueue):
    def __init__(self):
        PriorityQueue.__init__(self)
        self.counter = 0

    def put(self, item, priority):
        PriorityQueue.put(self, (priority, self.counter, item))
        self.counter += 1

    def get(self, *args, **kwargs):
        _, _, item = PriorityQueue.get(self, *args, **kwargs)
        return item


queue = MyPriorityQueue()
queue.put('item2', 2)
queue.put('item5', 5)
queue.put('item3', 3)
queue.put('item4', 4)
queue.put('item1', 1)

while True:
    print(queue.get())
```

来看看输出，符合我们的预期。我们存入入队列的顺序是`25341`，对应的优先级也是`25341`，可是被消费的顺序丝毫不受传入顺序的影响，而是根据指定的优先级来消费。

```python
item1
item2
item3
item4
item5
```

##  总结一下

学习了以上三种通信方法，我们很容易就能发现`Event` 和 `Condition` 是threading模块原生提供的模块，原理简单，功能单一，它能发送 `True` 和 `False` 的指令，所以只能适用于某些简单的场景中。

而`Queue`则是比较高级的模块，它可能发送任何类型的消息，包括字符串、字典等。其内部实现其实也引用了`Condition`模块（譬如`put`和`get`函数的阻塞），正是其对`Condition`进行了功能扩展，所以功能更加丰富，更能满足实际应用。



#  线程中的信息隔离

上一篇我们说，线程与线程之间要通过消息通信来控制程序的执行。

讲完了消息通信，今天就来探讨下线程里的`信息隔离`是如何做到的。

##  初步认识信息隔离

什么是`信息隔离`？

比如说，咱有两个线程，线程A里的变量，和线程B里的变量值不能共享。这就是`信息隔离`。

你可能要说，那变量名取不一样不就好啦？

是的，如果所有的线程都不是由一个class实例化出来的同一个对象，确实是可以。这个问题我们暂且挂着，后面我再说明。

那么，如何实现`信息隔离`呢？

在Python中，其提供了`threading.local`这个类，可以很方便的控制变量的隔离，即使是同一个变量，在不同的线程中，其值也是不能共享的。

用代码来看下

```python
from threading import local, Thread, currentThread

# 定义一个local实例
local_data = local()
# 在主线中，存入name这个变量
local_data.name = 'local_data'


class MyThread(Thread):
    def run(self):
        print("赋值前-子线程：", currentThread(),local_data.__dict__)
        # 在子线程中存入name这个变量
        local_data.name = self.getName()
        print("赋值后-子线程：",currentThread(), local_data.__dict__)


if __name__ == '__main__':
    print("开始前-主线程：",local_data.__dict__)

    t1 = MyThread()
    t1.start()
    t1.join()

    t2 = MyThread()
    t2.start()
    t2.join()

    print("结束后-主线程：",local_data.__dict__)
```

来看看输出结果

```python
开始前-主线程： {'name': 'local_data'}

赋值前-子线程： <MyThread(Thread-1, started 4832)> {}
赋值后-子线程： <MyThread(Thread-1, started 4832)> {'name': 'Thread-1'}

赋值前-子线程： <MyThread(Thread-2, started 5616)> {}
赋值后-子线程： <MyThread(Thread-2, started 5616)> {'name': 'Thread-2'}

结束后-主线程： {'name': 'local_data'}
```

从输出来看，我们可以知道，`local`实际是一个`字典型`的对象，其内部可以以`key-value`的形式存入你要做信息隔离的变量。local实例可以是`全局唯一`的，只有一个。因为你在给local存入或访问变量时，它会根据当前的线程的不同从不同的`存储空间`存入或获取。

基于此，我们可以得出以下三点结论：

1. 主线程中的变量，不会因为其是全局变量，而被子线程获取到；
2. 主线程也不能获取到子线程中的变量；
3. 子线程与子线程之间的变量也不能互相访问。

所以如果想在当前线程保存一个全局值，并且各自线程（包括主线程）互不干扰，使用local类吧。

##  信息隔离的意义何在

细心的你，一定已经发现了，上面那个例子，即使我们不用`threading.local`来做信息隔离，两个线程`self.getName()`本身就是隔离的，没有任何关系的。因为这两个线程是由一个class实例出的两个不同的实例对象。自然是可以不用做隔离，因为其本身就是隔离的。

但是，现实开发中。不可排除有多个线程，是由一个class实例出的同一个实例对象而实现的。

譬如，现在新手特别喜欢的爬虫项目。通常都是先给爬虫一个主页，然后获取主页下的所有链接，对这个链接再进行遍历，一直往下，直到把所有的链接都爬完，获取到我们所需的内容。

由于单线程的爬取效率实在是太低了，我们考虑使用多线程来工作。先使用`socket`和`www.sina.con.cn`建立一个TCP连接。然后在这个连接的基础上，对主页上的每个链接（我们这里只举`news.sina.com.cn`和`blog.sina.com.cn`这两个子链接做例子）创建一个线程，这样效率就高多了。

>**友情提醒**：以下代码，若要理解，可能需要你了解下socket的网络编程相关内容。我在前几天的文章中有发布一篇相关的文章，没有基础的同学可以先去看看那篇文章。

```python
import threading
from functools import partial
from socket import socket, AF_INET, SOCK_STREAM

class LazyConnection:
    def __init__(self, address, family=AF_INET, type=SOCK_STREAM):
        self.address = address
        self.family = AF_INET
        self.type = SOCK_STREAM
        self.local = threading.local()

    def __enter__(self):
        if hasattr(self.local, 'sock'):
            raise RuntimeError('Already connected')
        # 把socket连接存入local中
        self.local.sock = socket(self.family, self.type)
        self.local.sock.connect(self.address)
        return self.local.sock

    def __exit__(self, exc_ty, exc_val, tb):
        self.local.sock.close()
        del self.local.sock

def spider(conn, website):
    with conn as s:
        header = 'GET / HTTP/1.1\r\nHost: {}\r\nConnection: close\r\n\r\n'.format(website)
        s.send(header.encode("utf-8"))
        resp = b''.join(iter(partial(s.recv, 100000), b''))
    print('Got {} bytes'.format(len(resp)))

if __name__ == '__main__':
    # 建立一个TCP连接
    conn = LazyConnection(('www.sina.com.cn', 80))

    # 爬取两个页面
    t1 = threading.Thread(target=spider, args=(conn,"news.sina.com.cn"))
    t2 = threading.Thread(target=spider, args=(conn,"blog.sina.com.cn"))
    t1.start()
    t2.start()
    t1.join()
    t2.join()
```

输出结果

```python
Got 765 bytes
Got 513469 bytes
```

如果是在这种场景下，要做到线程之间的状态信息的隔离，就肯定要借助`threading.local`，所以`threading.local`的存在是有存在的意义的。其他还有很多场景是必须借助`threading.local`才能实现的，而这些就要靠你们在真正的业务开发中去发现咯。



# 线程池创建的几种方法

##  线程池的创建

### 使用内置模块

在使用多线程处理任务时也不是线程越多越好，由于在切换线程的时候，需要切换上下文环境，依然会造成cpu的大量开销。为解决这个问题，线程池的概念被提出来了。预先创建好一个合理数量的线程池，让过来的任务立刻能够使用，就形成了线程池。

在Python3中，创建线程池是通过`concurrent.futures`函数库中的`ThreadPoolExecutor`类来实现的。

```python
import time
import threading
from concurrent.futures import ThreadPoolExecutor

def target():
    for i in range(5):
        print('running thread-{}:{}'.format(threading.get_ident(), i))
        time.sleep(1)
        
# 创建一个最大容纳数量为5的线程池
pool = ThreadPoolExecutor(5) 

for i in range(10):
  	# 往线程池上塞任务
    pool.submit(target)
```

创建线程池还可以使用更优雅的方式，就是使用上下文管理器

```python
with ThreadPoolExecutor(5) as pool:
    for i in range(100):
        pool.submit(target) 
```

直接运行代码，从输出可以看出，前面我们设置线程池最大线程数，会保证“同时”仅有五个线程在工作。

```python
running thread-123145483767808:0
running thread-123145489022976:0
running thread-123145494278144:0
running thread-123145499533312:0
running thread-123145504788480:0
running thread-123145483767808:1
running thread-123145489022976:1
running thread-123145499533312:1
running thread-123145494278144:1
running thread-123145504788480:1
running thread-123145489022976:2
running thread-123145499533312:2
running thread-123145483767808:2
running thread-123145504788480:2
running thread-123145494278144:2
....
```



示例完毕，来说明一下：

1. 使用 with 语句 ，通过 ThreadPoolExecutor 构造实例，同时传入 max_workers 参数来设置线程池中最多能同时运行的线程数目。
2. 使用 submit 函数来提交线程需要执行的任务到线程池中，并返回该任务的句柄（类似于文件、画图），注意 submit() 不是阻塞的，而是立即返回。
3. 通过使用 done() 方法判断该任务是否结束。上面的例子可以看出，提交任务后立即判断任务状态，显示四个任务都未完成。在延时2.5后，task1 和 task2 执行完毕，task3 仍在执行中。
4. 使用 result() 方法可以获取任务的返回值。



### 自定义线程池

除了使用上述第三方模块的方法之外，我们还可以自己结合前面所学的消息队列来自定义线程池。

这里我们就使用queue来实现一个上面同样效果的例子，大家感受一下。

```python
import time
import threading
from queue import Queue

def target(queue):
    while True:
        task = queue.get()
        if task == "stop":
            queue.task_done()
            break

        task()
        queue.task_done()

def do_task():
    for i in range(5):
        print('running thread-{}:{}'.format(threading.get_ident(), i))
        time.sleep(1)


class MyQueue(Queue):
    def close(self):
        for i in range(self.maxsize):
            self.put("stop")

def custome_pool(task_func, max_workers):
    queue = MyQueue(max_workers)
    for n in range(max_workers):
        t = threading.Thread(target=task_func, args=(queue,))
        t.daemon = True
        t.start()

    return queue



pool = custome_pool(task_func=target, max_workers=5)

for i in range(10):
    pool.put(do_task)

pool.close()
pool.join()
```

输出是和上面是完全一样的效果

```python
running thread-123145469886464:0
running thread-123145475141632:0
running thread-123145485651968:0
running thread-123145490907136:0
running thread-123145480396800:0
running thread-123145469886464:1
running thread-123145480396800:1
running thread-123145475141632:1
running thread-123145490907136:1
running thread-123145485651968:1
...
```

构建线程池的方法，是可以很灵活的，大家有空可以自己多研究。但是建议只要掌握一种自己熟悉的，能快速上手的就好了。



#  从 yield 开始入门协程

## 简单介绍 yield

本篇文章会先向你介绍一个陌生的 Python 关键词，他和 `return` 就像一对新兄弟，有相似之处，又各有不同。

-   **相似的是**：yield 和 return 都可以在一个函数里将值返回给调用方；
-   **不同的是**：return 后，函数运行就终止了，而 yield 则只是暂停运行。

关于 yield 的简单使用，请先看如下例子

```python
>>> def demo():
...     yield 1
...     yield 2
...     yield 3
...
>>> gen = demo()
>>> gen
<generator object demo at 0x10a9f5970>
>>>
>>>
>>> for i in gen:
...     print(i)
...
1
2
3
```

重点有如下几个：

1.  含有 yield 的函数，不再是普通的函数，直接调用含有 yield 的函数，返回的是一个生成器对象（generator object）
2.  可以使用 for 循环（实际还可以使用 list 或者 next 函数）来遍历该生成器对象，将 yield 的内容一个一个打印出来

更多关于 yield 和生成器的内容，请前往前面的文章，里面有非常详细的讲解：[3.6 【基础】生成器](http://python.iswbm.com/c03/c03_06.html)



## 向生成器中发送消息

函数暂停之后，如果调用者能在下一次恢复函数运行的时候，向它传递一些信息，那么整个程序的灵活性会大大提升。

下面通过一个简明的演示来看看，如何往生成器中发送消息：

```python
def jumping_range(N):
    index = 0
    while index < N:
        # 通过send()发送的信息将赋值给jump
        jump = yield index
        if jump is None:
            jump = 1
        index += jump

if __name__ == '__main__':
    itr = jumping_range(5)
    print(next(itr))
    print(itr.send(2))
    print(next(itr))
    print(itr.send(-1))
```

输出。

```
0
2
3
2
```

这里解释下为什么这么输出。

重点是`jump = yield index`这个语句。

分成两部分：

- `yield index` 是将index `return`给外部调用程序。
- `jump = yield` 可以接收外部程序通过send()发送的信息，并赋值给`jump`

下一节，我将讲一个Python3.5新引入的语法：`yield from`。篇幅也比较多，所以就单独拿出来讲。



#  深入理解yield from语法

##  为什么要使用协程

在上一篇中，我们从生成器的基本认识与使用，成功过渡到了协程。

但一定有许多人，只知道协程是个什么东西，但并不知道为什么要用协程？换句话来说，并不知道在什么情况下用协程？

它相比多线程来说，有哪些过人之处呢？

在开始讲`yield from` 之前，我想先解决一下这个给很多人带来困惑的问题。

举个例子。

假如我们做一个爬虫。我们要爬取多个网页，这里简单举例两个网页(两个spider函数)，获取HTML（耗IO耗时），然后再对HTML对行解析取得我们感兴趣的数据。

我们的代码结构精简如下：

```python
def spider_01(url):
    html = get_html(url)
    ...
    data = parse_html(html)

def spider_02(url):
    html = get_html(url)
    ...
    data = parse_html(html)
```

我们都知道，`get_html()`等待返回网页是非常耗IO的，一个网页还好，如果我们爬取的网页数据极其庞大，这个等待时间就非常惊人，是极大的浪费。

聪明的程序员，当然会想如果能在`get_html()`这里暂停一下，不用傻乎乎地去等待网页返回，而是去做别的事。等过段时间再回过头来到刚刚暂停的地方，接收返回的html内容，然后还可以接下去解析`parse_html(html)`。

利用常规的方法，几乎是没办法实现如上我们想要的效果的。所以Python想得很周到，从语言本身给我们实现了这样的功能，这就是`yield`语法。可以实现在某一函数中暂停的效果。


试着思考一下，假如没有协程，我们要写一个并发程序。可能有以下问题：

1. 使用最常规的同步编程要实现异步并发效果并不理想，或者难度极高。
2. 由于GIL锁的存在，多线程的运行需要频繁的加锁解锁，切换线程，这极大地降低了并发性能。

而协程的出现，刚好可以解决以上的问题。它的特点有：

1. 协程是在单线程里实现任务的切换的
2. 利用同步的方式去实现异步
3. 不再需要锁，提高了并发性能

##  yield from的用法详解

`yield from` 是在Python3.3才出现的语法。所以这个特性在Python2中是没有的。

`yield from` 后面需要加的是可迭代对象，它可以是普通的可迭代对象，也可以是迭代器，甚至是生成器。

###  简单应用：拼接可迭代对象

我们可以用一个使用`yield`和一个使用`yield from`的例子来对比看下。

使用`yield`

```python
# 字符串
astr='ABC'

# 列表
alist=[1, 2, 3]

# 字典
adict={"name":"wangbm", "age":18}

# 生成器
agen=(i for i in range(4,8))

def gen(*args, **kw):
    for item in args:
        for i in item:
            yield i

new_list = gen(astr, alist, adict, agen)
print(list(new_list))

# ['A', 'B', 'C', 1, 2, 3, 'name', 'age', 4, 5, 6, 7]
```

使用`yield from`

```python
# 字符串
astr='ABC'
# 列表
alist=[1,2,3]
# 字典
adict={"name":"wangbm","age":18}
# 生成器
agen=(i for i in range(4,8))

def gen(*args, **kw):
    for item in args:
        yield from item

new_list=gen(astr, alist, adict, agen)
print(list(new_list))
# ['A', 'B', 'C', 1, 2, 3, 'name', 'age', 4, 5, 6, 7]
```

由上面两种方式对比，可以看出，yield from后面加上可迭代对象，他可以把可迭代对象里的每个元素一个一个的yield出来，对比yield来说代码更加简洁，结构更加清晰。

###  复杂应用：生成器的嵌套

如果你认为只是 `yield from` 仅仅只有上述的功能的话，那你就太小瞧了它，它的更强大的功能还在后面。

当 `yield from` 后面加上一个生成器后，就实现了生成的嵌套。

当然实现生成器的嵌套，并不是一定必须要使用`yield from`，而是使用`yield from`可以让我们避免让我们自己处理各种料想不到的异常，而让我们专注于业务代码的实现。

如果自己用`yield`去实现，那只会加大代码的编写难度，降低开发效率，降低代码的可读性。既然Python已经想得这么周到，我们当然要好好利用起来。

讲解它之前，首先要知道这个几个概念

1. `调用方`：调用委派生成器的客户端（调用方）代码
2. `委托生成器`：包含yield from表达式的生成器函数
3. `子生成器`：yield from后面加的生成器函数

你可能不知道他们都是什么意思，没关系，来看下这个例子。

这个例子，是实现实时计算平均值的。

比如，第一次传入10，那返回平均数自然是10.

第二次传入20，那返回平均数是(10+20)/2=15

第三次传入30，那返回平均数(10+20+30)/3=20

```python
# 子生成器
def average_gen():
    total = 0
    count = 0
    average = 0
    while True:
        new_num = yield average
        count += 1
        total += new_num
        average = total/count

# 委托生成器
def proxy_gen():
    while True:
        yield from average_gen()

# 调用方
def main():
    calc_average = proxy_gen()
    next(calc_average)            # 预激下生成器
    print(calc_average.send(10))  # 打印：10.0
    print(calc_average.send(20))  # 打印：15.0
    print(calc_average.send(30))  # 打印：20.0

if __name__ == '__main__':
    main()
```

认真阅读以上代码，你应该很容易能理解，调用方、委托生成器、子生成器之间的关系。我就不多说了

**委托生成器的作用是**：在调用方与子生成器之间建立一个`双向通道`。

所谓的双向通道是什么意思呢？
调用方可以通过`send()`直接发送消息给子生成器，而子生成器yield的值，也是直接返回给调用方。

你可能会经常看到有些代码，还可以在`yield from`前面看到可以赋值。这是什么用法？

你可能会以为，子生成器yield回来的值，被委托生成器给拦截了。你可以亲自写个demo运行试验一下，并不是你想的那样。
因为我们之前说了，委托生成器，只起一个桥梁作用，它建立的是一个`双向通道`，它并没有权利也没有办法，对子生成器yield回来的内容做拦截。

为了解释这个用法，我还是用上述的例子，并对其进行了一些改造。添加了一些注释，希望你能看得明白。

按照惯例，我们还是举个例子。

```python
# 子生成器
def average_gen():
    total = 0
    count = 0
    average = 0
    while True:
        new_num = yield average
        if new_num is None:
            break
        count += 1
        total += new_num
        average = total/count

    # 每一次return，都意味着当前协程结束。
    return total,count,average

# 委托生成器
def proxy_gen():
    while True:
        # 只有子生成器要结束（return）了，yield from左边的变量才会被赋值，后面的代码才会执行。
        total, count, average = yield from average_gen()
        print("计算完毕！！\n总共传入 {} 个数值， 总和：{}，平均数：{}".format(count, total, average))

# 调用方
def main():
    calc_average = proxy_gen()
    next(calc_average)            # 预激协程
    print(calc_average.send(10))  # 打印：10.0
    print(calc_average.send(20))  # 打印：15.0
    print(calc_average.send(30))  # 打印：20.0
    calc_average.send(None)      # 结束协程
    # 如果此处再调用calc_average.send(10)，由于上一协程已经结束，将重开一协程
    
if __name__ == '__main__':
    main()
```

运行后，输出

```python
10.0
15.0
20.0
计算完毕！！
总共传入 3 个数值， 总和：60，平均数：20.0
```

##  为什么要使用yield from

学到这里，我相信你肯定要问，既然委托生成器，起到的只是一个双向通道的作用，我还需要委托生成器做什么？我调用方直接调用子生成器不就好啦？

高能预警~~~

下面我们来一起探讨一下，到底yield from 有什么过人之处，让我们非要用它不可。

###  因为它可以帮我们处理异常

如果我们去掉委托生成器，而直接调用子生成器。那我们就需要把代码改成像下面这样，我们需要自己捕获异常并处理。而不像使`yield from`那样省心。

```python
# 子生成器
# 子生成器
def average_gen():
    total = 0
    count = 0
    average = 0
    while True:
        new_num = yield average
        if new_num is None:
            break
        count += 1
        total += new_num
        average = total/count
    return total,count,average

# 调用方
def main():
    calc_average = average_gen()
    next(calc_average)            # 预激协程
    print(calc_average.send(10))  # 打印：10.0
    print(calc_average.send(20))  # 打印：15.0
    print(calc_average.send(30))  # 打印：20.0

    # ----------------注意-----------------
    try:
        calc_average.send(None)
    except StopIteration as e:
        total, count, average = e.value
        print("计算完毕！！\n总共传入 {} 个数值， 总和：{}，平均数：{}".format(count, total, average))
    # ----------------注意-----------------

if __name__ == '__main__':
    main()
```

此时的你，可能会说，不就一个`StopIteration`的异常吗？自己捕获也没什么大不了的。

你要是知道`yield from`在背后为我们默默无闻地做了哪些事，你就不会这样说了。

具体`yield from`为我们做了哪些事，可以参考如下这段代码。

```python
#一些说明
"""
_i：子生成器，同时也是一个迭代器
_y：子生成器生产的值
_r：yield from 表达式最终的值
_s：调用方通过send()发送的值
_e：异常对象
"""

_i = iter(EXPR)

try:
    _y = next(_i)
except StopIteration as _e:
    _r = _e.value

else:
    while 1:
        try:
            _s = yield _y
        except GeneratorExit as _e:
            try:
                _m = _i.close
            except AttributeError:
                pass
            else:
                _m()
            raise _e
        except BaseException as _e:
            _x = sys.exc_info()
            try:
                _m = _i.throw
            except AttributeError:
                raise _e
            else:
                try:
                    _y = _m(*_x)
                except StopIteration as _e:
                    _r = _e.value
                    break
        else:
            try:
                if _s is None:
                    _y = next(_i)
                else:
                    _y = _i.send(_s)
            except StopIteration as _e:
                _r = _e.value
                break
RESULT = _r
```

以上的代码，稍微有点复杂，有兴趣的同学可以结合以下说明去研究看看。

1. 迭代器（即可指子生成器）产生的值直接返还给调用者
2. 任何使用send()方法发给委派生产器（即外部生产器）的值被直接传递给迭代器。如果send值是None，则调用迭代器next()方法；如果不为None，则调用迭代器的send()方法。如果对迭代器的调用产生StopIteration异常，委派生产器恢复继续执行yield from后面的语句；若迭代器产生其他任何异常，则都传递给委派生产器。
3. 子生成器可能只是一个迭代器，并不是一个作为协程的生成器，所以它不支持.throw()和.close()方法,即可能会产生AttributeError 异常。
4. 除了GeneratorExit 异常外的其他抛给委派生产器的异常，将会被传递到迭代器的throw()方法。如果迭代器throw()调用产生了StopIteration异常，委派生产器恢复并继续执行，其他异常则传递给委派生产器。
5. 如果GeneratorExit异常被抛给委派生产器，或者委派生产器的close()方法被调用，如果迭代器有close()的话也将被调用。如果close()调用产生异常，异常将传递给委派生产器。否则，委派生产器将抛出GeneratorExit 异常。
6. 当迭代器结束并抛出异常时，yield from表达式的值是其StopIteration 异常中的第一个参数。
7. 一个生成器中的return expr语句将会从生成器退出并抛出 StopIteration(expr)异常。

没兴趣看的同学，只要知道，`yield from`帮我们做了很多的异常处理，而且全面，而这些如果我们要自己去实现的话，一个是编写代码难度增加，写出来的代码可读性极差，这些我们就不说了，最主要的是很可能有遗漏，只要哪个异常没考虑到，都有可能导致程序崩溃什么的



#  初识异步IO框架：asyncio 上篇

通过前两节的铺垫（关于协程的使用），今天我们终于可以来介绍我们整个系列的重点 -- `asyncio`。

`asyncio`是Python 3.4版本引入的标准库，直接内置了对异步IO的支持。

有些同学，可能很疑惑，既然有了以生成器为基础的协程，我们直接使用`yield` 和 `yield from` 不就可以手动实现对IO的调度了吗？ 为何Python吃饱了没事干，老重复造轮子。

这个问题很好回答，就跟为什么会有`Django`，为什么会有`Scrapy`，是一个道理。

他们都是框架，将很多很重复性高，复杂度高的工作，提前给你做好，这样你就可以专注于业务代码的研发。

跟着小明学完了协程的那些个难点，你是不是也发现了，协程的知识点我已经掌握了，但是我还是不知道怎么用，如何使用，都说它可以实现并发，但是我还是不知道如何入手？

那是因为，我们现在还缺少一个成熟的框架，帮助你完成那些复杂的动作。这个时候，`ayncio`就这么应运而生了。

##  如何定义/创建协程

还记得在前两章节的时候，我们创建了生成器，是如何去检验我们创建的是不是生成器对象吗？

我们是借助了`isinstance()`函数，来判断是否是`collections.abc` 里的`Generator`类的子类实现的。

同样的方法，我们也可以用在这里。

只要在一个函数前面加上 `async` 关键字，这个函数对象是一个协程，通过`isinstance`函数，它确实是`Coroutine`类型。

```python
from collections.abc import Coroutine

async def hello(name):
    print('Hello,', name)

if __name__ == '__main__':
    # 生成协程对象，并不会运行函数内的代码
    coroutine = hello("World")

    # 检查是否是协程 Coroutine 类型
    print(isinstance(coroutine, Coroutine))  # True
```

前两节，我们说，生成器是协程的基础，那我们是不是有办法，将一个生成器，直接变成协程使用呢。答案是有的。

```python
import asyncio
from collections.abc import Generator, Coroutine

'''
只要在一个生成器函数头部用上 @asyncio.coroutine 装饰器
就能将这个函数对象，【标记】为协程对象。注意这里是【标记】，划重点。
实际上，它的本质还是一个生成器。
标记后，它实际上已经可以当成协程使用。后面会介绍。
'''


@asyncio.coroutine
def hello():
    # 异步调用asyncio.sleep(1):
    yield from asyncio.sleep(1)


if __name__ == '__main__':
    coroutine = hello()
    print(isinstance(coroutine, Generator))  # True
    print(isinstance(coroutine, Coroutine))  # False
```

##  asyncio的几个概念

在了解`asyncio`的使用方法前，首先有必要先介绍一下，这几个贯穿始终的概念。

- `event_loop 事件循环`：程序开启一个无限的循环，程序员会把一些函数（协程）注册到事件循环上。当满足事件发生的时候，调用相应的协程函数。
- `coroutine 协程`：协程对象，指一个使用async关键字定义的函数，它的调用不会立即执行函数，而是会返回一个协程对象。协程对象需要注册到事件循环，由事件循环调用。
- `future 对象`： 代表将来执行或没有执行的任务的结果。它和task上没有本质的区别
- `task 任务`：一个协程对象就是一个原生可以挂起的函数，任务则是对协程进一步封装，其中包含任务的各种状态。Task 对象是 Future 的子类，它将 coroutine 和 Future 联系在一起，将 coroutine 封装成一个 Future 对象。
- `async/await 关键字`：python3.5 用于定义协程的关键字，async定义一个协程，await用于挂起阻塞的异步调用接口。其作用在一定程度上类似于yield。

这几个概念，干看可能很难以理解，没事，往下看实例，然后再回来，我相信你一定能够理解。

##  学习协程是如何工作的

协程完整的工作流程是这样的

- 定义/创建协程对象
- 将协程转为task任务
- 定义事件循环对象容器
- 将task任务扔进事件循环对象中触发

光说不练假把戏，一起来看下

```python
import asyncio

async def hello(name):
    print('Hello,', name)

# 定义协程对象
coroutine = hello("World")

# 定义事件循环对象容器
loop = asyncio.get_event_loop()
# task = asyncio.ensure_future(coroutine)

# 将协程转为task任务
task = loop.create_task(coroutine)

# 将task任务扔进事件循环对象中并触发
loop.run_until_complete(task)
```

输出结果，当然显而易见

```
Hello, World
```

##  await与yield对比

前面我们说，`await`用于挂起阻塞的异步调用接口。其作用在`一定程度上`类似于yield。

注意这里是，一定程度上，意思是效果上一样（都能实现暂停的效果），但是功能上却不兼容。就是你不能在生成器中使用`await`，也不能在async 定义的协程中使用`yield from`。

小明不是胡说八道的。有实锤。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-837255.png)

再来一锤。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-821252.png)

除此之外呢，还有一点很重要的。

- `yield from` 后面可接 `可迭代对象`，也可接`future对象`/协程对象；
- `await` 后面必须要接 `future对象`/`协程对象`

如何验证呢？

`yield from` 后面可接 `可迭代对象`，这个前两章已经说过了，这里不再赘述。

接下来，就只要验证，`yield from`和`await`都可以接`future对象`/`协程对象`就可以了。

验证之前呢，要先介绍一下这个函数：
`asyncio.sleep(n)`，这货是asyncio自带的工具函数，他可以模拟IO阻塞，他返回的是一个协程对象。

```python
func = asyncio.sleep(2)
print(isinstance(func, Future))      # False
print(isinstance(func, Coroutine))   # True
```

还有，要学习如何创建`Future对象`，不然怎么验证。

前面概念里说过，Task是Future的子类，这么说，我们只要创建一个task对象即可。

```python
import asyncio
from asyncio.futures import Future

async def hello(name):
    await asyncio.sleep(2)
    print('Hello, ', name)

coroutine = hello("World")

# 将协程转为task对象
task = asyncio.ensure_future(coroutine)

print(isinstance(task, Future))   # True
```

好了，接下来，开始验证。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-850252.png)



##  绑定回调函数

异步IO的实现原理，就是在IO高的地方挂起，等IO结束后，再继续执行。在绝大部分时候，我们后续的代码的执行是需要依赖IO的返回值的，这就要用到回调了。

回调的实现，有两种，一种是绝大部分程序员喜欢的，利用的同步编程实现的回调。
这就要求我们要能够有办法取得协程的await的返回值。

```python
import asyncio
import time

async def _sleep(x):
    time.sleep(2)
    return '暂停了{}秒！'.format(x)


coroutine = _sleep(2)
loop = asyncio.get_event_loop()

task = asyncio.ensure_future(coroutine)
loop.run_until_complete(task)

# task.result() 可以取得返回结果
print('返回结果：{}'.format(task.result()))
```

输出

```
返回结果：暂停了2秒！
```

还有一种是通过asyncio自带的添加回调函数功能来实现。

```python
import time
import asyncio


async def _sleep(x):
    time.sleep(2)
    return '暂停了{}秒！'.format(x)

def callback(future):
    print('这里是回调函数，获取返回结果是：', future.result())

coroutine = _sleep(2)
loop = asyncio.get_event_loop()
task = asyncio.ensure_future(coroutine)

# 添加回调函数
task.add_done_callback(callback)

loop.run_until_complete(task)
```

输出

```python
这里是回调函数，获取返回结果是： 暂停了2秒！
```

和上面的结果是一样的。非常好。

---



#  深入异步IO框架：asyncio 中篇

今天的内容其实还挺多的，我准备了三天，到今天才整理完毕。希望大家看完，有所收获的，能给小明一个赞。这就是对小明最大的鼓励了。
为了更好地衔接这一节，我们先来回顾一下上一节的内容。

上一节，我们首先介绍了，如何创建一个协程对象.
主要有两种方法

- 通过`async`关键字，
- 通过`@asyncio.coroutine` 装饰函数。

然后有了协程对象，就需要一个事件循环容器来运行我们的协程。其主要的步骤有如下几点：

- 将协程对象转为task任务对象
- 定义一个事件循环对象容器用来存放task
- 将task任务扔进事件循环对象中并触发

为了让大家，对生成器和协程有一个更加清晰的认识，我还介绍了`yield`和`async/await`的区别。

最后，我们还讲了，如何给一个协程添加回调函数。

好了，用个形象的比喻，上一节，其实就只是讲了协程中的`单任务`。哈哈，是不是还挺难的？希望大家一定要多看几遍，多敲代码，不要光看。

那么这一节，我们就来看下，协程中的`多任务`。

##  协程中的并发

协程的并发，和线程一样。举个例子来说，就好像 一个人同时吃三个馒头，咬了第一个馒头一口，就得等这口咽下去，才能去啃第其他两个馒头。就这样交替换着吃。

`asyncio`实现并发，就需要多个协程来完成任务，每当有任务阻塞的时候就await，然后其他协程继续工作。

第一步，当然是创建多个协程的列表。

```python
# 协程函数
async def do_some_work(x):
    print('Waiting: ', x)
    await asyncio.sleep(x)
    return 'Done after {}s'.format(x)

# 协程对象
coroutine1 = do_some_work(1)
coroutine2 = do_some_work(2)
coroutine3 = do_some_work(4)

# 将协程转成task，并组成list
tasks = [
    asyncio.ensure_future(coroutine1),
    asyncio.ensure_future(coroutine2),
    asyncio.ensure_future(coroutine3)
]
```

第二步，如何将这些协程注册到事件循环中呢。

有两种方法，至于这两种方法什么区别，稍后会介绍。

- 使用`asyncio.wait()`

```python
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

- 使用`asyncio.gather()`

```python
# 千万注意，这里的 「*」 不能省略
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.gather(*tasks))
```

最后，return的结果，可以用`task.result()`查看。

```python
for task in tasks:
    print('Task ret: ', task.result())
```

完整代码如下

```python
import asyncio

# 协程函数
async def do_some_work(x):
    print('Waiting: ', x)
    await asyncio.sleep(x)
    return 'Done after {}s'.format(x)

# 协程对象
coroutine1 = do_some_work(1)
coroutine2 = do_some_work(2)
coroutine3 = do_some_work(4)

# 将协程转成task，并组成list
tasks = [
    asyncio.ensure_future(coroutine1),
    asyncio.ensure_future(coroutine2),
    asyncio.ensure_future(coroutine3)
]

loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))

for task in tasks:
    print('Task ret: ', task.result())
```

输出结果

```python
Waiting:  1
Waiting:  2
Waiting:  4
Task ret:  Done after 1s
Task ret:  Done after 2s
Task ret:  Done after 4s
```

##  协程中的嵌套

使用async可以定义协程，协程用于耗时的io操作，我们也可以封装更多的io操作过程，这样就实现了嵌套的协程，即一个协程中await了另外一个协程，如此连接起来。

来看个例子。

```python
import asyncio

# 用于内部的协程函数
async def do_some_work(x):
    print('Waiting: ', x)
    await asyncio.sleep(x)
    return 'Done after {}s'.format(x)

# 外部的协程函数
async def main():
    # 创建三个协程对象
    coroutine1 = do_some_work(1)
    coroutine2 = do_some_work(2)
    coroutine3 = do_some_work(4)

    # 将协程转为task，并组成list
    tasks = [
        asyncio.ensure_future(coroutine1),
        asyncio.ensure_future(coroutine2),
        asyncio.ensure_future(coroutine3)
    ]

    # 【重点】：await 一个task列表（协程）
    # dones：表示已经完成的任务
    # pendings：表示未完成的任务
    dones, pendings = await asyncio.wait(tasks)

    for task in dones:
        print('Task ret: ', task.result())

loop = asyncio.get_event_loop()
loop.run_until_complete(main())
```

如果这边，使用的是`asyncio.gather()`，是这么用的

```python
# 注意这边返回结果，与await不一样

results = await asyncio.gather(*tasks)
for result in results:
    print('Task ret: ', result)
```

输出还是一样的。

```
Waiting:  1
Waiting:  2
Waiting:  4
Task ret:  Done after 1s
Task ret:  Done after 2s
Task ret:  Done after 4s
```

仔细查看，可以发现这个例子完全是由 上面「`协程中的并发`」例子改编而来。结果完全一样。只是把创建协程对象，转换task任务，封装成在一个协程函数里而已。外部的协程，嵌套了一个内部的协程。

其实你如果去看下`asyncio.await()`的源码的话，你会发现下面这种写法

```python
loop.run_until_complete(asyncio.wait(tasks))
```

看似没有嵌套，实际上内部也是嵌套的。

这里也把源码，贴出来，有兴趣可以看下，没兴趣，可以直接跳过。

```python
# 内部协程函数
async def _wait(fs, timeout, return_when, loop):
    assert fs, 'Set of Futures is empty.'
    waiter = loop.create_future()
    timeout_handle = None
    if timeout is not None:
        timeout_handle = loop.call_later(timeout, _release_waiter, waiter)
    counter = len(fs)

    def _on_completion(f):
        nonlocal counter
        counter -= 1
        if (counter <= 0 or
            return_when == FIRST_COMPLETED or
            return_when == FIRST_EXCEPTION and (not f.cancelled() and
                                                f.exception() is not None)):
            if timeout_handle is not None:
                timeout_handle.cancel()
            if not waiter.done():
                waiter.set_result(None)

    for f in fs:
        f.add_done_callback(_on_completion)

    try:
        await waiter
    finally:
        if timeout_handle is not None:
            timeout_handle.cancel()

    done, pending = set(), set()
    for f in fs:
        f.remove_done_callback(_on_completion)
        if f.done():
            done.add(f)
        else:
            pending.add(f)
    return done, pending

# 外部协程函数
async def wait(fs, *, loop=None, timeout=None, return_when=ALL_COMPLETED):
    if futures.isfuture(fs) or coroutines.iscoroutine(fs):
        raise TypeError(f"expect a list of futures, not {type(fs).__name__}")
    if not fs:
        raise ValueError('Set of coroutines/Futures is empty.')
    if return_when not in (FIRST_COMPLETED, FIRST_EXCEPTION, ALL_COMPLETED):
        raise ValueError(f'Invalid return_when value: {return_when}')

    if loop is None:
        loop = events.get_event_loop()

    fs = {ensure_future(f, loop=loop) for f in set(fs)}
    # 【重点】：await一个内部协程
    return await _wait(fs, timeout, return_when, loop)
```

##  协程中的状态

还记得我们在讲生成器的时候，有提及过生成器的状态。同样，在协程这里，我们也了解一下协程（准确的说，应该是Future对象，或者Task任务）有哪些状态。

1. `Pending`：创建future，还未执行
2. `Running`：事件循环正在调用执行任务
3. `Done`：任务执行完毕
4. `Cancelled`：Task被取消后的状态


可手工 `python3 xx.py` 执行这段代码：

```python
import asyncio
import threading
import time

async def hello():
    print("Running in the loop...")
    flag = 0
    while flag < 1000:
        with open("F:\\test.txt", "a") as f:
            f.write("------")
        flag += 1
    print("Stop the loop")

    
if __name__ == '__main__':
    coroutine = hello()
    loop = asyncio.get_event_loop()
    task = loop.create_task(coroutine)

    # Pending：未执行状态
    print(task)
    try:
        t1 = threading.Thread(target=loop.run_until_complete, args=(task,))
        # t1.daemon = True
        t1.start()

        # Running：运行中状态
        time.sleep(1)
        print(task)
        t1.join()
    except KeyboardInterrupt as e:
        # 取消任务
        task.cancel()
        # Cacelled：取消任务
        print(task)
    finally:
        print(task)
```

顺利执行的话，将会打印 `Pending` -> `Pending：Runing` -> `Finished` 的状态变化

假如，执行后 立马按下 Ctrl+C，则会触发task取消，就会打印 `Pending` -> `Cancelling` -> `Cancelling` 的状态变化。

##  gather与wait

还记得上面我说，把多个协程注册进一个事件循环中有两种方法吗？

- 使用`asyncio.wait()`

```python
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.wait(tasks))
```

- 使用`asyncio.gather()`

```python
# 千万注意，这里的 「*」 不能省略
loop = asyncio.get_event_loop()
loop.run_until_complete(asyncio.gather(*tasks))
```

`asyncio.gather` 和 `asyncio.wait` 在asyncio中用得的比较广泛，这里有必要好好研究下这两货。

还是照例用例子来说明，先定义一个协程函数

```python
import asyncio

async def factorial(name, number):
    f = 1
    for i in range(2, number+1):
        print("Task %s: Compute factorial(%s)..." % (name, i))
        await asyncio.sleep(1)
        f *= i
    print("Task %s: factorial(%s) = %s" % (name, number, f))
```

##  接收参数方式

### asyncio.wait

 接收的tasks，必须是一个list对象，这个list对象里，存放多个的task。

它可以这样，用`asyncio.ensure_future`转为task对象

```python
tasks=[
       asyncio.ensure_future(factorial("A", 2)),
       asyncio.ensure_future(factorial("B", 3)),
       asyncio.ensure_future(factorial("C", 4))
]

loop = asyncio.get_event_loop()

loop.run_until_complete(asyncio.wait(tasks))
```

也可以这样，不转为task对象。

```python
loop = asyncio.get_event_loop()

tasks=[
       factorial("A", 2),
       factorial("B", 3),
       factorial("C", 4)
]

loop.run_until_complete(asyncio.wait(tasks))
```

### asyncio.gather

接收的就比较广泛了，他可以接收list对象，但是 `*` 不能省略

```python
tasks=[
       asyncio.ensure_future(factorial("A", 2)),
       asyncio.ensure_future(factorial("B", 3)),
       asyncio.ensure_future(factorial("C", 4))
]

loop = asyncio.get_event_loop()

loop.run_until_complete(asyncio.gather(*tasks))
```

还可以这样，和上面的 `*` 作用一致，这是因为`asyncio.gather()`的第一个参数是 `*coros_or_futures`，它叫 `非命名键值可变长参数列表`，可以集合所有没有命名的变量。

```python
loop = asyncio.get_event_loop()

loop.run_until_complete(asyncio.gather(
    factorial("A", 2),
    factorial("B", 3),
    factorial("C", 4),
))
```

甚至还可以这样

```python
loop = asyncio.get_event_loop()

group1 = asyncio.gather(*[factorial("A" ,i) for i in range(1, 3)])
group2 = asyncio.gather(*[factorial("B", i) for i in range(1, 5)])
group3 = asyncio.gather(*[factorial("B", i) for i in range(1, 7)])

loop.run_until_complete(asyncio.gather(group1, group2, group3))
```

##  返回结果不同

### asyncio.wait

`asyncio.wait` 返回`dones`和`pendings`

- `dones`：表示已经完成的任务
- `pendings`：表示未完成的任务

如果我们需要获取，运行结果，需要手工去收集获取。

```python
dones, pendings = await asyncio.wait(tasks)

for task in dones:
    print('Task ret: ', task.result())
```

### asyncio.gather

`asyncio.gather` 它会把值直接返回给我们，不需要手工去收集。

```python
results = await asyncio.gather(*tasks)

for result in results:
    print('Task ret: ', result)
```

##  wait有控制功能

```python
import asyncio
import random


async def coro(tag):
    await asyncio.sleep(random.uniform(0.5, 5))

loop = asyncio.get_event_loop()

tasks = [coro(i) for i in range(1, 11)]


# 【控制运行任务数】：运行第一个任务就返回
# FIRST_COMPLETED ：第一个任务完全返回
# FIRST_EXCEPTION：产生第一个异常返回
# ALL_COMPLETED：所有任务完成返回 （默认选项）
dones, pendings = loop.run_until_complete(
    asyncio.wait(tasks, return_when=asyncio.FIRST_COMPLETED))
print("第一次完成的任务数:", len(dones))


# 【控制时间】：运行一秒后，就返回
dones2, pendings2 = loop.run_until_complete(
    asyncio.wait(pendings, timeout=1))
print("第二次完成的任务数:", len(dones2))


# 【默认】：所有任务完成后返回
dones3, pendings3 = loop.run_until_complete(asyncio.wait(pendings2))

print("第三次完成的任务数:", len(dones3))

loop.close()
```

输出结果

```python
第一次完成的任务数: 1
第二次完成的任务数: 4
第三次完成的任务数: 5
```

#  实战异步IO框架：asyncio 下篇

##   动态添加协程

在实战之前，我们要先了解下在asyncio中如何将协程态添加到事件循环中的。这是前提。

如何实现呢，有两种方法：

- 主线程是同步的

```python
import time
import asyncio
from queue import Queue
from threading import Thread

def start_loop(loop):
    # 一个在后台永远运行的事件循环
    asyncio.set_event_loop(loop)
    loop.run_forever()

def do_sleep(x, queue, msg=""):
    time.sleep(x)
    queue.put(msg)

queue = Queue()

new_loop = asyncio.new_event_loop()

# 定义一个线程，并传入一个事件循环对象
t = Thread(target=start_loop, args=(new_loop,))
t.start()

print(time.ctime())

# 动态添加两个协程
# 这种方法，在主线程是同步的
new_loop.call_soon_threadsafe(do_sleep, 6, queue, "第一个")
new_loop.call_soon_threadsafe(do_sleep, 3, queue, "第二个")

while True:
    msg = queue.get()
    print("{} 协程运行完..".format(msg))
    print(time.ctime())
```

由于是同步的，所以总共耗时6+3=9秒.

输出结果

```python
Thu May 31 22:11:16 2018
第一个 协程运行完..
Thu May 31 22:11:22 2018
第二个 协程运行完..
Thu May 31 22:11:25 2018
```

- 主线程是异步的，这是重点，一定要掌握。。


```python
import time
import asyncio
from queue import Queue
from threading import Thread

def start_loop(loop):
    # 一个在后台永远运行的事件循环
    asyncio.set_event_loop(loop)
    loop.run_forever()

async def do_sleep(x, queue, msg=""):
    await asyncio.sleep(x)
    queue.put(msg)

queue = Queue()

new_loop = asyncio.new_event_loop()

# 定义一个线程，并传入一个事件循环对象
t = Thread(target=start_loop, args=(new_loop,))
t.start()

print(time.ctime())

# 动态添加两个协程
# 这种方法，在主线程是异步的
asyncio.run_coroutine_threadsafe(do_sleep(6, queue, "第一个"), new_loop)
asyncio.run_coroutine_threadsafe(do_sleep(3, queue, "第二个"), new_loop)

while True:
    msg = queue.get()
    print("{} 协程运行完..".format(msg))
    print(time.ctime())
```

输出结果

由于是异步的，所以总共耗时max(6, 3)=`6`秒

```python
Thu May 31 22:23:35 2018
第二个 协程运行完..
Thu May 31 22:23:38 2018
第一个 协程运行完..
Thu May 31 22:23:41 2018
```

##  利用redis实现动态添加任务

对于并发任务，通常是用生成消费模型，对队列的处理可以使用类似master-worker的方式，master主要用户获取队列的msg，worker用户处理消息。

为了简单起见，并且协程更适合单线程的方式，我们的主线程用来监听队列，子线程用于处理队列。这里使用redis的队列。主线程中有一个是无限循环，用户消费队列。

先安装Redis，到 https://github.com/MicrosoftArchive/redis/releases 下载

![](https://i.loli.net/2018/06/03/5b13ba8525bcf.png)

解压到你的路径。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-881257.png)

然后，在当前路径运行cmd，运行redis的服务端。

![](https://i.loli.net/2018/06/03/5b13bab682a32.png)

服务开启后，我们就可以运行我们的客户端了。

并依次输入key=queue，value=5,3,1的消息。

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-911251.png)

一切准备就绪之后，我们就可以运行我们的代码了。

```python
import time
import redis
import asyncio
from queue import Queue
from threading import Thread

def start_loop(loop):
    # 一个在后台永远运行的事件循环
    asyncio.set_event_loop(loop)
    loop.run_forever()

async def do_sleep(x, queue):
    await asyncio.sleep(x)
    queue.put("ok")

def get_redis():
    connection_pool = redis.ConnectionPool(host='127.0.0.1', db=0)
    return redis.Redis(connection_pool=connection_pool)

def consumer():
    while True:
        task = rcon.rpop("queue")
        if not task:
            time.sleep(1)
            continue
        asyncio.run_coroutine_threadsafe(do_sleep(int(task), queue), new_loop)


if __name__ == '__main__':
    print(time.ctime())
    new_loop = asyncio.new_event_loop()

    # 定义一个线程，运行一个事件循环对象，用于实时接收新任务
    loop_thread = Thread(target=start_loop, args=(new_loop,))
    loop_thread.setDaemon(True)
    loop_thread.start()
    # 创建redis连接
    rcon = get_redis()

    queue = Queue()

    # 子线程：用于消费队列消息，并实时往事件对象容器中添加新任务
    consumer_thread = Thread(target=consumer)
    consumer_thread.setDaemon(True)
    consumer_thread.start()

    while True:
        msg = queue.get()
        print("协程运行完..")
        print("当前时间：", time.ctime())
```

稍微讲下代码

`loop_thread`：单独的线程，运行着一个事件对象容器，用于实时接收新任务。

`consumer_thread`：单独的线程，实时接收来自Redis的消息队列，并实时往事件对象容器中添加新任务。

输出结果

```python
Thu May 31 23:42:48 2018
协程运行完..
当前时间： Thu May 31 23:42:49 2018

协程运行完..
当前时间： Thu May 31 23:42:51 2018

协程运行完..
当前时间： Thu May 31 23:42:53 2018
```

我们在Redis，分别发起了5s，3s，1s的任务。
从结果来看，这三个任务，确实是并发执行的，1s的任务最先结束，三个任务完成总耗时5s

运行后，程序是一直运行在后台的，我们每一次在

Redis中输入新值，都会触发新任务的执行。。

#  生成器与协程，你分清了吗？

如你所见，下面这代码将定义一个生成器的。

```python
import time

def eat():
    while True:
        if food:
            print("小明 吃完{}了".format(food))
        yield
        print("小明 要开始吃{}...".format(food))
        time.sleep(1)

food = None
MING = eat()     # 产生一个生成器
MING.send(None)  # 预激
food = "面包"
MING.send('面包')
MING.send('苹果')
MING.send('香肠')
```

运行一下，从结果中可以看出，不管我们塞给小明什么东西，小明都将只能将他们当成面包吃。

```
小明 要开始吃面包...
小明 吃完面包了
小明 要开始吃面包...
小明 吃完面包了
小明 要开始吃面包...
小明 吃完面包了
```

那再来看一下协程的。

```python
import time

def eat():
    food = None
    while True:
        if food:
            print("小明 吃完{}了".format(food))
        food = yield
        print("小明 开始吃{}...".format(food))
        time.sleep(1)

MING = eat()      # 产生一个生成器
MING.send(None)   # 预激
MING.send('面包')
MING.send('苹果')
MING.send('香肠')
```

运行一下，从结果中可以看出，小明已经可以感知我们塞给他的是什么食物。

```
小明 开始吃面包...
小明 吃完面包了
小明 开始吃苹果...
小明 吃完苹果了
小明 开始吃香肠...
小明 吃完香肠了
```

仔细观察一下，上面两段代码并没有太大的区别，我们将主要关注点集中在 `yield`  关键词上。

可以发现，生成器里 `yield` 左边并没有变量，而在协程里，`yield` 左边有一个变量。

在函数被调用后，一个生成器就产生了，而一般的生成器不能再往生成器内部传递参数了，而这个当生成器里的 yield 左边有变量时，就不一样了，它仍然可以在外部接收新的参数。这就是生成器与协程的最大区别。

**协程的优点：**

- 线程属于系统级别调度，而协程是程序员级别的调度。使用协程避免了无意义的调度，减少了线程上下文切换的开销，由此可以提高性能。
- 高并发+高扩展性+低成本：一个CPU支持上万的协程都不是问题。所以很适合用于高并发处理。
- 无需原子操作锁定及同步的开销
- 方便切换控制流，简化编程模型

**协程的缺点：**

1. 无法利用多核资源：协程的本质是个单线程,它不能同时将 单个CPU 的多个核用上,协程需要和进程配合才能运行在多CPU上.当然我们日常所编写的绝大部分应用都没有这个必要，除非是cpu密集型应用。
2. 进行阻塞（Blocking）操作（如IO时）会阻塞掉整个程序

协程很类似于Javascript单线程下异步处理的概念，协程同样是单线程的，之所以能够进行并发是因为通过某种方式保存了执行栈的上下文，在一定条件下将执行权交由其他栈，在一定条件下又通过执行栈上下文恢复栈。



#  浅谈线程安全那些事儿

在并发编程时，如果多个线程访问同一资源，我们需要保证访问的时候不会产生冲突，数据修改不会发生错误，这就是我们常说的 **线程安全** 。

那什么情况下，访问数据时是安全的？什么情况下，访问数据是不安全的？如何知道你的代码是否线程安全？要如何访问数据才能保证数据的安全？

本篇文章会一一回答你的问题。

## 线程不安全是怎样的？

要搞清楚什么是线程安全，就要先了解线程不安全是什么样的。

比如下面这段代码，开启两个线程，对全局变量 number 各自增 10万次，每次自增 1。

```python
from threading import Thread, Lock

number = 0

def target():
    global number
    for _ in range(1000000):
        number += 1

thread_01 = Thread(target=target)
thread_02 = Thread(target=target)
thread_01.start()
thread_02.start()

thread_01.join()
thread_02.join()

print(number)
```

正常我们的预期输出结果，一个线程自增100万，两个线程就自增 200 万嘛，输出肯定为 2000000 。

可事实却并不是你想的那样，不管你运行多少次，每次输出的结果都会不一样，而这些输出结果都有一个特点是，都小于 200 万。

以下是执行三次的结果

```python
1459782
1379891
1432921
```

这种现象就是线程不安全，究其根因，其实是我们的操作 `number += 1` ，不是原子操作，才会导致的线程不安全。



##  什么是原子操作？

原子操作（**atomic operation**），指不会被线程调度机制打断的操作，这种操作一旦开始，就一直运行到结束，中间不会切换到其他线程。

它有点类似数据库中的 **事务**。

在 Python 的[官方文档](https://docs.python.org/3.5/faq/library.html#what-kinds-of-global-value-mutation-are-thread-safe)上，列出了一些常见原子操作

```python
L.append(x)
L1.extend(L2)
x = L[i]
x = L.pop()
L1[i:j] = L2
L.sort()
x = y
x.field = y
D[x] = y
D1.update(D2)
D.keys()
```

而下面这些就不是原子操作

```python
i = i+1
L.append(L[-1])
L[i] = L[j]
D[x] = D[x] + 1
```

像上面的我使用自增操作 `number += 1`，其实等价于 `number = number + 1`，可以看到这种可以拆分成多个步骤（先读取相加再赋值），并不属于原子操作。

这样就导致多个线程同时读取时，有可能读取到同一个 number 值，读取两次，却只加了一次，最终导致自增的次数小于预期。

当我们还是无法确定我们的代码是否具有原子性的时候，可以尝试通过 `dis` 模块里的 dis 函数来查看

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-072254.png)

当我们执行这段代码时，可以看到 `number += 1` 这一行代码，由两条字节码实现。

- `BINARY_ADD` ：将两个值相加
- `STORE_GLOBAL`： 将相加后的值重新赋值

每一条字节码指令都是一个整体，无法分割，他实现的效果也就是我们所说的原子操作。

当一行代码被分成多条字节码指令的时候，就代表在线程线程切换时，有可能只执行了一条字节码指令，此时若这行代码里有被多个线程共享的变量或资源时，并且拆分的多条指令里有对于这个共享变量的写操作，就会发生数据的冲突，导致数据的不准确。

为了对比，我们从上面列表的原子操作拿一个出来也来试试，是不是真如官网所说的原子操作。

这里我拿字典的 update 操作举例，代码和执行过程如下图

![](https://gitee.com/liuhuihe/Ehe/raw/master/images/Python中文指南-20211128-171029-086263.png)

从截图里可以看到，`info.update(new)` 虽然也分为好几个操作

- `LOAD_GLOBAL`：加载全局变量
- `LOAD_ATTR`： 加载属性，获取 update 方法
- `LOAD_FAST`：加载 new 变量
- `CALL_FUNCTION`：调用函数
- `POP_TOP`：执行更新操作

但我们要知道真正会引导数据冲突的，其实不是读操作，而是写操作。

上面这么多字节码指令，写操作都只有一个（**POP_TOP**），因此字典的 update 方法是原子操作。

## 实现人工原子操作

在多线程下，我们并不能保证我们的代码都具有原子性，因此如何让我们的代码变得具有 “原子性” ，就是一件很重要的事。

方法也很简单，就是当你在访问一个多线程间共享的资源时，加锁可以实现类似原子操作的效果，一个代码要嘛不执行，执行了的话就要执行完毕，才能接受线程的调度。

因此，我们使用加锁的方法，对例子一进行一些修改，使其具备原子性。

```python
from threading import Thread, Lock

number = 0
lock = Lock()

def target():
    global number
    for _ in range(1000000):
        with lock:
            number += 1

thread_01 = Thread(target=target)
thread_02 = Thread(target=target)
thread_01.start()
thread_02.start()

thread_01.join()
thread_02.join()

print(number)
```

此时，不管你执行多少遍，输出都是 2000000.



## 为什么 Queue 是线程安全的？

Python 的 threading 模块里的消息通信机制主要有如下三种：

1. Event
2. Condition
3. Queue

使用最多的是 Queue，而我们都知道它是线程安全的。当我们对它进行写入和提取的操作不会被中断而导致错误，这也是我们在使用队列时，不需要额外加锁的原因。

他是如何做到的呢？

其根本原因就是 Queue 实现了锁原语，因此他能像第三节那样实现人工原子操作。

原语指由若干个机器指令构成的完成某种特定功能的一段程序，具有不可分割性；即原语的执行必须是连续的，在执行过程中不允许被中断。



## 参考文章：

https://zhuanlan.zhihu.com/p/34150765

https://juejin.im/post/5b129a1be51d45068a6c91d4#comment