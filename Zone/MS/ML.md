# **请简要说说一个完整机器学习项目的流程**

1. 抽象成数学问题
	明确问题是进行机器学习的第一步。机器学习的训练过程通常都是一件非常耗时的事情，胡乱尝试时间成本是非常高的。
	这里的抽象成数学问题，指的我们明确我们可以获得什么样的数据，目标是一个分类还是回归或者是聚类的问题，如果都不是的话，如果划归为其中的某类问题。
2. 获取数据
	数据决定了机器学习结果的上限，而算法只是尽可能逼近这个上限。
	数据要有代表性，否则必然会过拟合。
	而且对于分类问题，数据偏斜不能过于严重，不同类别的数据数量不要有数个数量级的差距。
	而且还要对数据的量级有一个评估，多少个样本，多少个特征，可以估算出其对内存的消耗程度，判断训练过程中内存是否能够放得下。如果放不下就得考虑改进算法或者使用一些降维的技巧了。如果数据量实在太大，那就要考虑分布式了。
3. 特征预处理与特征选择
	良好的数据要能够提取出良好的特征才能真正发挥效力。
	特征预处理、数据清洗是很关键的步骤，往往能够使得算法的效果和性能得到显著提高。归一化、离散化、因子化、缺失值处理、去除共线性等，数据挖掘过程中很多时间就花在它们上面。这些工作简单可复制，收益稳定可预期，是机器学习的基础必备步骤。
	筛选出显著特征、摒弃非显著特征，需要机器学习工程师反复理解业务。这对很多结果有决定性的影响。特征选择好了，非常简单的算法也能得出良好、稳定的结果。这需要运用特征有效性分析的相关技术，如相关系数、卡方检验、平均互信息、条件熵、后验概率、逻辑回归权重等方法。
4. 训练模型与调优
	直到这一步才用到我们上面说的算法进行训练。现在很多算法都能够封装成黑盒供人使用。但是真正考验水平的是调整这些算法的（超）参数，使得结果变得更加优良。这需要我们对算法的原理有深入的理解。理解越深入，就越能发现问题的症结，提出良好的调优方案。
5. 模型诊断
	如何确定模型调优的方向与思路呢？这就需要对模型进行诊断的技术。
	过拟合、欠拟合 判断是模型诊断中至关重要的一步。常见的方法如交叉验证，绘制学习曲线等。过拟合的基本调优思路是增加数据量，降低模型复杂度。欠拟合的基本调优思路是提高特征数量和质量，增加模型复杂度。
	误差分析 也是机器学习至关重要的步骤。通过观察误差样本，全面分析误差产生误差的原因:是参数的问题还是算法选择的问题，是特征的问题还是数据本身的问题……
	诊断后的模型需要进行调优，调优后的新模型需要重新进行诊断，这是一个反复迭代不断逼近的过程，需要不断地尝试， 进而达到最优状态。
6. 模型融合
	一般来说，模型融合后都能使得效果有一定提升。而且效果很好。
	工程上，主要提升算法准确度的方法是分别在模型的前端（特征清洗和预处理，不同的采样模式）与后端（模型融合）上下功夫。因为他们比较标准可复制，效果比较稳定。而直接调参的工作不会很多，毕竟大量数据训练起来太慢了，而且效果难以保证。
7. 上线运行
	这一部分内容主要跟工程实现的相关性比较大。工程上是结果导向，模型在线上运行的效果直接决定模型的成败。 不单纯包括其准确程度、误差等情况，还包括其运行的速度(时间复杂度)、资源消耗程度（空间复杂度）、稳定性是否可接受。
	这些工作流程主要是工程实践上总结出的一些经验。并不是每个项目都包含完整的一个流程。这里的部分只是一个指导性的说明，只有大家自己多实践，多积累项目经验，才会有自己更深刻的认识。
	故，基于此，七月在线每一期ML算法班都特此增加特征工程、模型调优等相关课。比如，这里有个公开课视频《特征处理与特征选择》。





# **简单说下有监督学习和无监督学习的区别？**

有监督学习：对具有标记的训练样本进行学习，以尽可能对训练样本集外的数据进行分类预测。（LR,SVM,BP,RF,GBDT）
无监督学习：对未标记的样本进行训练学习，比发现这些样本中的结构知识。(KMeans,DL)





# 协方差和相关性有什么区别？

相关性是协方差的标准化格式。协方差本身很难做比较。例如：如果我们计算工资（$）和年龄（岁）的协方差，因为这两个变量有不同的度量，所以我们会得到不能做比较的不同的协方差。为了解决这个问题，我们计算相关性来得到一个介于-1和1之间的值，就可以忽略它们各自不同的度量。




# 线性分类器与非线性分类器的区别以及优劣。

如果模型是参数的线性函数，并且存在线性分类面，那么就是线性分类器，否则不是。
常见的线性分类器有：LR,贝叶斯分类，单层感知机、线性回归。
常见的非线性分类器：决策树、RF、GBDT、多层感知机。
SVM两种都有(看线性核还是高斯核)。
线性分类器速度快、编程方便，但是可能拟合效果不会很好。
非线性分类器编程复杂，但是效果拟合能力强。



# 优化算法及其优缺点？

温馨提示：在回答面试官的问题的时候，往往将问题往大的方面去回答，这样不会陷于小的技术上死磕，最后很容易把自己嗑死了。
1）随机梯度下降
优点：可以一定程度上解决局部最优解的问题
缺点：收敛速度较慢
2）批量梯度下降
优点：容易陷入局部最优解
缺点：收敛速度较快
3）mini_batch梯度下降
综合随机梯度下降和批量梯度下降的优缺点，提取的一个中和的方法。
4）牛顿法
牛顿法在迭代的时候，需要计算Hessian矩阵，当维度较高的时候，计算 Hessian矩阵比较困难。
5）拟牛顿法
拟牛顿法是为了改进牛顿法在迭代过程中，计算Hessian矩阵而提取的算法，它采用的方式是通过逼近Hessian的方式来进行求解。



# 相似度的计算方法了解哪些，各自的优缺点是什么 

1.皮尔逊相关系数

反映的是两个变量之间的线性相关性，它的一个缺点是针对用户之间只有一个共同的评分项不能进行比较，另外没有考虑重叠的评分项数量对相似度的影响

2.欧几里得距离

  描述两个变量之间的直线距离，当两个变量至少有一个相同评分项时可以计算

3.余弦相似度

  余弦值代表的是空间向量上的夹角余弦值，更体现的是空间上的差异性，与欧几里得距离相比，欧更注重的是绝对数值之间的差异，而余弦更注重的趋势上的不同，

4. 曼哈顿距离

  表示绝对轴距总和，只有上下和左右的方向


# 聚类算法有哪些，优缺点是什么

1. 基于层次的聚类

    做法是将每个对象都看做一个类，计算两两之间距离最小的对象归为一类，然后重复这样的操作直至成为一个类，这种方式是采用贪心的方法，一步错步步错，时间复杂度过高，可解释性比较好

2. 基于划分的聚类（k-Means）

  原则是保证簇内的数据距离尽可能小，簇间的距离尽可能大，做法是确定需要划分的k的类别数，然后选择初始点，计算所有点到这些点的距离，将距离最近的点划为一簇，然后计算每一簇的平均值当做新的中心点，重复这样的过程直至最后收敛，优点在于时间空间复杂度都不高，但是对于k比较敏感，容易陷入局部最优解

3. 基于密度的聚类（DBSCAN）

 k-means聚类解决不了不规则形状的聚类，而基于密度的聚类可以解决，并对于噪声点比较有效，能发现任意形状的聚类，但是聚类的结果和参数关系很大

4. 基于网络的聚类

 原理是将数据空间划分成网格，计算每个网格中的数据密度，将相邻的高密度网格划为一簇，优点就是划分速度很快，因为是按照网格划分的，和数据点个数没有关系，所以对数据个数不敏感，但是却是以牺牲精度作为代价来实现的

5. 基于模型的聚类 (SOM)

原理是为每一簇拟定一个概率模型，主要是基于概率模型和神经网络模型的方法，假定随机选择的数据服从某种分布，找到获胜单元，然后调整获胜单元周围的向量向其靠拢，最后形成簇，优点是分成簇没有那么硬，分类比较柔和，是以概率的形式表示的，缺点是执行效率不高，当数据较多较复杂时很慢

6. 基于模糊的聚类（FCM）

原理来自于模糊集合论，使用隶属度来确定每个数据属于哪一类的，不断迭代隶属矩阵直至收敛来确定类别，算法对满足正态分布的数据具有很好的效果，缺点是算法的性能依赖于初始簇心，不能保证收敛于一个最优解
