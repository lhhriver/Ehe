# nn_Sequential

```python
import torch
from torch import nn
import torchkeras
from torchkeras import summary
from collections import OrderedDict

net = nn.Sequential()
net.add_module("conv1", nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3))
net.add_module("pool1", nn.MaxPool2d(kernel_size=2, stride=2))
net.add_module("conv2", nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5))
net.add_module("pool2", nn.MaxPool2d(kernel_size=2, stride=2))
net.add_module("dropout", nn.Dropout2d(p=0.1))
net.add_module("adaptive_pool", nn.AdaptiveMaxPool2d((1, 1)))
net.add_module("flatten", nn.Flatten())
net.add_module("linear1", nn.Linear(64, 32))
net.add_module("relu", nn.ReLU())
net.add_module("linear2", nn.Linear(32, 1))
net.add_module("sigmoid", nn.Sigmoid())

net_2 = nn.Sequential(
    nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
    nn.MaxPool2d(kernel_size=2, stride=2),
    nn.Dropout2d(p=0.1),
    nn.AdaptiveMaxPool2d((1, 1)),
    nn.Flatten(),
    nn.Linear(64, 32),
    nn.ReLU(),
    nn.Linear(32, 1),
    nn.Sigmoid()
)

net_3 = nn.Sequential(
    OrderedDict(
        [("conv1", nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)),
         ("pool1", nn.MaxPool2d(kernel_size=2, stride=2)),
         ("conv2", nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)),
         ("pool2", nn.MaxPool2d(kernel_size=2, stride=2)),
         ("dropout", nn.Dropout2d(p=0.1)),
         ("adaptive_pool", nn.AdaptiveMaxPool2d((1, 1))),
         ("flatten", nn.Flatten()),
         ("linear1", nn.Linear(64, 32)),
         ("relu", nn.ReLU()),
         ("linear2", nn.Linear(32, 1)),
         ("sigmoid", nn.Sigmoid())
         ])
)


class Net_4(nn.Module):
    def __init__(self):
        super(Net_4, self).__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(p=0.1),
            nn.AdaptiveMaxPool2d((1, 1))
        )
        self.dense = nn.Sequential(
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()
        )

    def forward(self, x):
        x = self.conv(x)
        y = self.dense(x)
        return y


class Net_5(nn.Module):
    def __init__(self):
        super(Net_5, self).__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(p=0.1),
            nn.AdaptiveMaxPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 1),
            nn.Sigmoid()]
        )

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


class Net_6(nn.Module):

    def __init__(self):
        super(Net_6, self).__init__()
        self.layers_dict = nn.ModuleDict(
            {"conv1": nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3),
             "pool": nn.MaxPool2d(kernel_size=2, stride=2),
             "conv2": nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
             "dropout": nn.Dropout2d(p=0.1),
             "adaptive": nn.AdaptiveMaxPool2d((1, 1)),
             "flatten": nn.Flatten(),
             "linear1": nn.Linear(64, 32),
             "relu": nn.ReLU(),
             "linear2": nn.Linear(32, 1),
             "sigmoid": nn.Sigmoid()
             })

    def forward(self, x):
        layers = ["conv1", "pool", "conv2", "pool", "dropout", "adaptive",
                  "flatten", "linear1", "relu", "linear2", "sigmoid"]
        for layer in layers:
            x = self.layers_dict[layer](x)
        return x


if __name__ == '__main__':
    print("*" * 100)
    print(net)

    print("*" * 100)
    print(net_2)

    print("*" * 100)
    print(net_3)
    torchkeras.summary(net_3, input_shape=(3, 32, 32))

    print("*" * 100)
    net_4 = Net_4()
    print(net_4)
    torchkeras.summary(net_4, input_shape=(3, 32, 32))

    print("*" * 100)
    net_5 = Net_5()
    print(net_5)
    torchkeras.summary(net_5, input_shape=(3, 32, 32))

    print("*" * 100)
    net_6 = Net_6()
    print(net_6)
    torchkeras.summary(net_6, input_shape=(3, 32, 32))

```



# nn_Module

```python
import torch 
from torch import nn
from torchkeras import summary


class Net(nn.Module):

    def __init__(self):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout = nn.Dropout2d(p=0.1)
        self.adaptive_pool = nn.AdaptiveMaxPool2d((1, 1))
        self.flatten = nn.Flatten()
        self.linear1 = nn.Linear(64, 32)
        self.relu = nn.ReLU()
        self.linear2 = nn.Linear(32, 1)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        x = self.conv1(x)
        x = self.pool1(x)
        x = self.conv2(x)
        x = self.pool2(x)
        x = self.dropout(x)
        x = self.adaptive_pool(x)
        x = self.flatten(x)
        x = self.linear1(x)
        x = self.relu(x)
        x = self.linear2(x)
        y = self.sigmoid(x)
        return y


net = Net()
print(net)

summary(net,input_shape= (3,32,32))
```



# model_script

```python
import torch
from torch import nn
from torchkeras import summary, Model

import torchvision
from torchvision import transforms

import datetime
import pandas as pd
from sklearn.metrics import accuracy_score

transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=True, download=False, transform=transform)
ds_valid = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=False, download=False, transform=transform)

dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=0)
dl_valid = torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=0)

print(len(ds_train))
print(len(ds_valid))


def show_images():
    # %matplotlib inline
    # %config InlineBackend.figure_format = 'svg'

    # 查看部分样本
    from matplotlib import pyplot as plt

    plt.figure(figsize=(8, 8))
    for i in range(9):
        img, label = ds_train[i]
        img = torch.squeeze(img)
        ax = plt.subplot(3, 3, i + 1)
        ax.imshow(img.numpy())
        ax.set_title("label = %d" % label)
        ax.set_xticks([])
        ax.set_yticks([])
    plt.show()


net = nn.Sequential()
net.add_module("conv1", nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3))
net.add_module("pool1", nn.MaxPool2d(kernel_size=2, stride=2))
net.add_module("conv2", nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5))
net.add_module("pool2", nn.MaxPool2d(kernel_size=2, stride=2))
net.add_module("dropout", nn.Dropout2d(p=0.1))
net.add_module("adaptive_pool", nn.AdaptiveMaxPool2d((1, 1)))
net.add_module("flatten", nn.Flatten())
net.add_module("linear1", nn.Linear(64, 32))
net.add_module("relu", nn.ReLU())
net.add_module("linear2", nn.Linear(32, 10))
# print(net)


def accuracy(y_pred, y_true):
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1).data
    return accuracy_score(y_true, y_pred_cls)


if __name__ == '__main__':
    summary(net, input_shape=(1, 32, 32))

    loss_func = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(params=net.parameters(), lr=0.01)
    metric_func = accuracy
    metric_name = "accuracy"

    epochs = 3
    log_step_freq = 100

    dfhistory = pd.DataFrame(columns=["epoch", "loss", metric_name, "val_loss", "val_" + metric_name])
    print("Start Training...")
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("==========" * 8 + "%s" % nowtime)

    for epoch in range(1, epochs + 1):

        # 1，训练循环-------------------------------------------------
        net.train()
        loss_sum = 0.0
        metric_sum = 0.0
        step = 1

        for step, (features, labels) in enumerate(dl_train, 1):

            # 梯度清零
            optimizer.zero_grad()

            # 正向传播求损失
            predictions = net(features)
            loss = loss_func(predictions, labels)
            metric = metric_func(predictions, labels)

            # 反向传播求梯度
            loss.backward()
            optimizer.step()

            # 打印batch级别日志
            loss_sum += loss.item()
            metric_sum += metric.item()
            if step % log_step_freq == 0:
                print(("[step = %d] loss: %.3f, " + metric_name + ": %.3f") % (step, loss_sum / step, metric_sum / step))

        # 2，验证循环-------------------------------------------------
        net.eval()
        val_loss_sum = 0.0
        val_metric_sum = 0.0
        val_step = 1

        for val_step, (features, labels) in enumerate(dl_valid, 1):
            with torch.no_grad():
                predictions = net(features)
                val_loss = loss_func(predictions, labels)
                val_metric = metric_func(predictions, labels)

            val_loss_sum += val_loss.item()
            val_metric_sum += val_metric.item()

        # 3，记录日志-------------------------------------------------
        info = (epoch, loss_sum / step, metric_sum / step, val_loss_sum / val_step, val_metric_sum / val_step)
        dfhistory.loc[epoch - 1] = info

        # 打印epoch级别日志
        print(("\nEPOCH = %d, loss = %.3f," + metric_name + "  = %.3f, val_loss = %.3f, " + "val_" + metric_name + " = %.3f") % info)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n" + "==========" * 8 + "%s" % nowtime)

    print('Finished Training...')

```



# model_func

```python
import torch
from torch import nn
from torchkeras import summary, Model

import torchvision
from torchvision import transforms

import datetime
import pandas as pd
from sklearn.metrics import accuracy_score

transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=True, download=False, transform=transform)
ds_valid = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=False, download=False, transform=transform)

dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=0)
dl_valid = torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=0)

print(len(ds_train))
print(len(ds_valid))


class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(p=0.1),
            nn.AdaptiveMaxPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 10)]
        )

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


def accuracy(y_pred, y_true):
    """

    :param y_pred:
    :param y_true:
    :return:
    """
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1).data
    return accuracy_score(y_true, y_pred_cls)


def train_step(model, features, labels):
    """

    :param model:
    :param features:
    :param labels:
    :return:
    """
    # 训练模式，dropout层发生作用
    model.train()

    # 梯度清零
    model.optimizer.zero_grad()

    # 正向传播求损失
    predictions = model(features)
    loss = model.loss_func(predictions, labels)
    metric = model.metric_func(predictions, labels)

    # 反向传播求梯度
    loss.backward()
    model.optimizer.step()

    return loss.item(), metric.item()


@torch.no_grad()
def valid_step(model, features, labels):
    # 预测模式，dropout层不发生作用
    model.eval()

    predictions = model(features)
    loss = model.loss_func(predictions, labels)
    metric = model.metric_func(predictions, labels)

    return loss.item(), metric.item()


def train_model(model, epochs, dl_train, dl_valid, log_step_freq):
    """

    :param model:
    :param epochs:
    :param dl_train:
    :param dl_valid:
    :param log_step_freq:
    :return:
    """
    metric_name = model.metric_name
    dfhistory = pd.DataFrame(columns=["epoch", "loss", metric_name, "val_loss", "val_" + metric_name])
    print("Start Training...")
    nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
    print("==========" * 8 + "%s" % nowtime)

    for epoch in range(1, epochs + 1):

        # 1，训练循环-------------------------------------------------
        loss_sum = 0.0
        metric_sum = 0.0
        step = 1

        for step, (features, labels) in enumerate(dl_train, 1):
            loss, metric = train_step(model, features, labels)

            # 打印batch级别日志
            loss_sum += loss
            metric_sum += metric
            if step % log_step_freq == 0:
                print(("[step = %d] loss: %.3f, " + metric_name + ": %.3f") % (step, loss_sum / step, metric_sum / step))

        # 2，验证循环-------------------------------------------------
        val_loss_sum = 0.0
        val_metric_sum = 0.0
        val_step = 1

        for val_step, (features, labels) in enumerate(dl_valid, 1):
            val_loss, val_metric = valid_step(model, features, labels)

            val_loss_sum += val_loss
            val_metric_sum += val_metric

        # 3，记录日志-------------------------------------------------
        info = (epoch, loss_sum / step, metric_sum / step, val_loss_sum / val_step, val_metric_sum / val_step)
        dfhistory.loc[epoch - 1] = info

        # 打印epoch级别日志
        print(("\nEPOCH = %d, loss = %.3f," + metric_name + "  = %.3f, val_loss = %.3f, " + "val_" + metric_name + " = %.3f") % info)
        nowtime = datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')
        print("\n" + "==========" * 8 + "%s" % nowtime)

    print('Finished Training...')
    return dfhistory


if __name__ == '__main__':
    net = Net()
    # print(net)

    summary(net, input_shape=(1, 32, 32))

    model = net
    model.optimizer = torch.optim.SGD(model.parameters(), lr=0.01)
    model.loss_func = nn.CrossEntropyLoss()
    model.metric_func = accuracy
    model.metric_name = "accuracy"

    # features, labels = next(iter(dl_train))
    # train_step(model, features, labels)
    epochs = 3
    dfhistory = train_model(model, epochs, dl_train, dl_valid, log_step_freq=100)

```



# model_class

```python
import torch
from torch import nn
from torchkeras import summary, Model
import torchkeras

import torchvision
from torchvision import transforms

import datetime
import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score

transform = transforms.Compose([transforms.ToTensor()])

ds_train = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=True, download=False, transform=transform)
ds_valid = torchvision.datasets.MNIST(root=r"F:\datasets\minist", train=False, download=False, transform=transform)

dl_train = torch.utils.data.DataLoader(ds_train, batch_size=128, shuffle=True, num_workers=0)
dl_valid = torch.utils.data.DataLoader(ds_valid, batch_size=128, shuffle=False, num_workers=0)

print(len(ds_train))
print(len(ds_valid))


class CnnModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.layers = nn.ModuleList([
            nn.Conv2d(in_channels=1, out_channels=32, kernel_size=3),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=5),
            nn.MaxPool2d(kernel_size=2, stride=2),
            nn.Dropout2d(p=0.1),
            nn.AdaptiveMaxPool2d((1, 1)),
            nn.Flatten(),
            nn.Linear(64, 32),
            nn.ReLU(),
            nn.Linear(32, 10)]
        )

    def forward(self, x):
        for layer in self.layers:
            x = layer(x)
        return x


def accuracy(y_pred, y_true):
    """

    :param y_pred:
    :param y_true:
    :return:
    """
    y_pred_cls = torch.argmax(nn.Softmax(dim=1)(y_pred), dim=1).data
    return accuracy_score(y_true.numpy(), y_pred_cls.numpy())


if __name__ == '__main__':
    model = torchkeras.Model(CnnModel())
    # print(model)

    model.summary(input_shape=(1, 32, 32))

    model.compile(loss_func=nn.CrossEntropyLoss(),
                  optimizer=torch.optim.Adam(model.parameters(), lr=0.02),
                  metrics_dict={"accuracy": accuracy})

    dfhistory = model.fit(3, dl_train=dl_train, dl_val=dl_valid, log_step_freq=100)

```

